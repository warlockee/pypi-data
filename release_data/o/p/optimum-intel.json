{
    "1.2.2": {
        "info": {
            "author": "HuggingFace Inc. Special Ops Team",
            "author_email": "hardware@huggingface.co",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 5 - Production/Stable",
                "Intended Audience :: Developers",
                "Intended Audience :: Education",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: Apache Software License",
                "Operating System :: OS Independent",
                "Programming Language :: Python :: 3.7",
                "Programming Language :: Python :: 3.8",
                "Programming Language :: Python :: 3.9",
                "Topic :: Scientific/Engineering :: Artificial Intelligence"
            ],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://www.intel.com",
            "keywords": "transformers,quantization,pruning,knowledge distillation,optimization,training",
            "license": "Apache",
            "maintainer": "",
            "maintainer_email": "",
            "name": "optimum-intel",
            "package_url": "https://pypi.org/project/optimum-intel/",
            "platform": null,
            "project_url": "https://pypi.org/project/optimum-intel/",
            "project_urls": {
                "Homepage": "https://www.intel.com"
            },
            "release_url": "https://pypi.org/project/optimum-intel/1.2.2/",
            "requires_dist": null,
            "requires_python": "",
            "summary": "Optimum Library is an extension of the Hugging Face Transformers library, providing a framework to integrate third-party libraries from Hardware Partners and interface with their specific functionality.",
            "version": "1.2.2",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16211630,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "1f949190d7986864542f53b8f132d163",
                    "sha256": "e4ef989b44808e512fc2e56d0d6882fc2bfba049fa7a7d3bed1a157cb3c59154"
                },
                "downloads": -1,
                "filename": "optimum-intel-1.2.2.tar.gz",
                "has_sig": false,
                "md5_digest": "1f949190d7986864542f53b8f132d163",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 23564,
                "upload_time": "2022-06-07T08:35:51",
                "upload_time_iso_8601": "2022-06-07T08:35:51.331045Z",
                "url": "https://files.pythonhosted.org/packages/2a/44/0ce4577bf8df43e8dff8f1db58caf7d933322ecda9598ee49344cd4e6e81/optimum-intel-1.2.2.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "1.2.3": {
        "info": {
            "author": "HuggingFace Inc. Special Ops Team",
            "author_email": "hardware@huggingface.co",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 5 - Production/Stable",
                "Intended Audience :: Developers",
                "Intended Audience :: Education",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: Apache Software License",
                "Operating System :: OS Independent",
                "Programming Language :: Python :: 3.7",
                "Programming Language :: Python :: 3.8",
                "Programming Language :: Python :: 3.9",
                "Topic :: Scientific/Engineering :: Artificial Intelligence"
            ],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://www.intel.com",
            "keywords": "transformers,quantization,pruning,knowledge distillation,optimization,training",
            "license": "Apache",
            "maintainer": "",
            "maintainer_email": "",
            "name": "optimum-intel",
            "package_url": "https://pypi.org/project/optimum-intel/",
            "platform": null,
            "project_url": "https://pypi.org/project/optimum-intel/",
            "project_urls": {
                "Homepage": "https://www.intel.com"
            },
            "release_url": "https://pypi.org/project/optimum-intel/1.2.3/",
            "requires_dist": null,
            "requires_python": "",
            "summary": "Optimum Library is an extension of the Hugging Face Transformers library, providing a framework to integrate third-party libraries from Hardware Partners and interface with their specific functionality.",
            "version": "1.2.3",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16211630,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "34be315812daae2ae5f24e3fb3b7eab7",
                    "sha256": "e8d137350555741c3c12076b814632e58c651cb9b7fc6bda412748288876c76a"
                },
                "downloads": -1,
                "filename": "optimum-intel-1.2.3.tar.gz",
                "has_sig": false,
                "md5_digest": "34be315812daae2ae5f24e3fb3b7eab7",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 22766,
                "upload_time": "2022-06-13T17:29:33",
                "upload_time_iso_8601": "2022-06-13T17:29:33.084309Z",
                "url": "https://files.pythonhosted.org/packages/da/2b/85ffcfc83b7496b84f77e30d93013cf4207037360c2bbb653c648905ba56/optimum-intel-1.2.3.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "1.3.0": {
        "info": {
            "author": "HuggingFace Inc. Special Ops Team",
            "author_email": "hardware@huggingface.co",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 5 - Production/Stable",
                "Intended Audience :: Developers",
                "Intended Audience :: Education",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: Apache Software License",
                "Operating System :: OS Independent",
                "Programming Language :: Python :: 3.7",
                "Programming Language :: Python :: 3.8",
                "Programming Language :: Python :: 3.9",
                "Topic :: Scientific/Engineering :: Artificial Intelligence"
            ],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://www.intel.com",
            "keywords": "transformers,quantization,pruning,knowledge distillation,optimization,training",
            "license": "Apache",
            "maintainer": "",
            "maintainer_email": "",
            "name": "optimum-intel",
            "package_url": "https://pypi.org/project/optimum-intel/",
            "platform": null,
            "project_url": "https://pypi.org/project/optimum-intel/",
            "project_urls": {
                "Homepage": "https://www.intel.com"
            },
            "release_url": "https://pypi.org/project/optimum-intel/1.3.0/",
            "requires_dist": null,
            "requires_python": "",
            "summary": "Optimum Library is an extension of the Hugging Face Transformers library, providing a framework to integrate third-party libraries from Hardware Partners and interface with their specific functionality.",
            "version": "1.3.0",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16211630,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "95887f2489edc09b0786d2d41c624b9e",
                    "sha256": "44b588dfcd5cc8696a72fc3bc57e494ac4ed88fd0f6736a018c99d0e569faf09"
                },
                "downloads": -1,
                "filename": "optimum-intel-1.3.0.tar.gz",
                "has_sig": false,
                "md5_digest": "95887f2489edc09b0786d2d41c624b9e",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 26632,
                "upload_time": "2022-08-05T15:28:00",
                "upload_time_iso_8601": "2022-08-05T15:28:00.221440Z",
                "url": "https://files.pythonhosted.org/packages/fc/4a/dad54d82a75ebe626bc0c1e56f7b88cfee642f733b6be262b800403745b8/optimum-intel-1.3.0.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "1.3.1": {
        "info": {
            "author": "HuggingFace Inc. Special Ops Team",
            "author_email": "hardware@huggingface.co",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 5 - Production/Stable",
                "Intended Audience :: Developers",
                "Intended Audience :: Education",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: Apache Software License",
                "Operating System :: OS Independent",
                "Programming Language :: Python :: 3.7",
                "Programming Language :: Python :: 3.8",
                "Programming Language :: Python :: 3.9",
                "Topic :: Scientific/Engineering :: Artificial Intelligence"
            ],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://www.intel.com",
            "keywords": "transformers,quantization,pruning,knowledge distillation,optimization,training",
            "license": "Apache",
            "maintainer": "",
            "maintainer_email": "",
            "name": "optimum-intel",
            "package_url": "https://pypi.org/project/optimum-intel/",
            "platform": null,
            "project_url": "https://pypi.org/project/optimum-intel/",
            "project_urls": {
                "Homepage": "https://www.intel.com"
            },
            "release_url": "https://pypi.org/project/optimum-intel/1.3.1/",
            "requires_dist": null,
            "requires_python": "",
            "summary": "Optimum Library is an extension of the Hugging Face Transformers library, providing a framework to integrate third-party libraries from Hardware Partners and interface with their specific functionality.",
            "version": "1.3.1",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16211630,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "6c8d776dae4e5f04644789e0d3c3c1ab",
                    "sha256": "ee4838d9bb2769e28467cc86e404f1d060fd4479524212ce1355b8b8720fe677"
                },
                "downloads": -1,
                "filename": "optimum-intel-1.3.1.tar.gz",
                "has_sig": false,
                "md5_digest": "6c8d776dae4e5f04644789e0d3c3c1ab",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 27017,
                "upload_time": "2022-09-07T17:02:10",
                "upload_time_iso_8601": "2022-09-07T17:02:10.617528Z",
                "url": "https://files.pythonhosted.org/packages/b0/f0/7e6aa3f2ae2cf82864025b22d297cade3f6ea1856a794d5a06a02f5293c0/optimum-intel-1.3.1.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "1.4.0": {
        "info": {
            "author": "HuggingFace Inc. Special Ops Team",
            "author_email": "hardware@huggingface.co",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 5 - Production/Stable",
                "Intended Audience :: Developers",
                "Intended Audience :: Education",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: Apache Software License",
                "Operating System :: OS Independent",
                "Programming Language :: Python :: 3.7",
                "Programming Language :: Python :: 3.8",
                "Programming Language :: Python :: 3.9",
                "Topic :: Scientific/Engineering :: Artificial Intelligence"
            ],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://www.intel.com",
            "keywords": "transformers,quantization,pruning,knowledge distillation,optimization,training",
            "license": "Apache",
            "maintainer": "",
            "maintainer_email": "",
            "name": "optimum-intel",
            "package_url": "https://pypi.org/project/optimum-intel/",
            "platform": null,
            "project_url": "https://pypi.org/project/optimum-intel/",
            "project_urls": {
                "Homepage": "https://www.intel.com"
            },
            "release_url": "https://pypi.org/project/optimum-intel/1.4.0/",
            "requires_dist": null,
            "requires_python": "",
            "summary": "Optimum Library is an extension of the Hugging Face Transformers library, providing a framework to integrate third-party libraries from Hardware Partners and interface with their specific functionality.",
            "version": "1.4.0",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16211630,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "bd36366be6b29140e5327565ef7e11bf",
                    "sha256": "1250ad55be2beb1347aaa5175763e421d6a725f2bbc7c336eb3bc8fa5c999ba5"
                },
                "downloads": -1,
                "filename": "optimum-intel-1.4.0.tar.gz",
                "has_sig": false,
                "md5_digest": "bd36366be6b29140e5327565ef7e11bf",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 38481,
                "upload_time": "2022-09-26T14:49:59",
                "upload_time_iso_8601": "2022-09-26T14:49:59.032922Z",
                "url": "https://files.pythonhosted.org/packages/59/0b/84b09620fb3f109718cd494714da3ecf38f0bdab11e36c08746c7432e236/optimum-intel-1.4.0.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "1.5.0": {
        "info": {
            "author": "HuggingFace Inc. Special Ops Team",
            "author_email": "hardware@huggingface.co",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 5 - Production/Stable",
                "Intended Audience :: Developers",
                "Intended Audience :: Education",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: Apache Software License",
                "Operating System :: OS Independent",
                "Programming Language :: Python :: 3.7",
                "Programming Language :: Python :: 3.8",
                "Programming Language :: Python :: 3.9",
                "Topic :: Scientific/Engineering :: Artificial Intelligence"
            ],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://www.intel.com",
            "keywords": "transformers,quantization,pruning,knowledge distillation,optimization,training",
            "license": "Apache",
            "maintainer": "",
            "maintainer_email": "",
            "name": "optimum-intel",
            "package_url": "https://pypi.org/project/optimum-intel/",
            "platform": null,
            "project_url": "https://pypi.org/project/optimum-intel/",
            "project_urls": {
                "Homepage": "https://www.intel.com"
            },
            "release_url": "https://pypi.org/project/optimum-intel/1.5.0/",
            "requires_dist": null,
            "requires_python": "",
            "summary": "Optimum Library is an extension of the Hugging Face Transformers library, providing a framework to integrate third-party libraries from Hardware Partners and interface with their specific functionality.",
            "version": "1.5.0",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16211630,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "4b277811ff0929f89c504c218b3e9a26",
                    "sha256": "20439af5b1eac39ea64a8fc891e03ee78641ef5bbcabbcd0fecab6a320a3ce6d"
                },
                "downloads": -1,
                "filename": "optimum-intel-1.5.0.tar.gz",
                "has_sig": false,
                "md5_digest": "4b277811ff0929f89c504c218b3e9a26",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 49313,
                "upload_time": "2022-10-21T09:19:29",
                "upload_time_iso_8601": "2022-10-21T09:19:29.146383Z",
                "url": "https://files.pythonhosted.org/packages/2c/42/bcb1b4b567867eca15a40bcff92dce772db629e4cb89bb69caa5a9737d90/optimum-intel-1.5.0.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "1.5.1": {
        "info": {
            "author": "HuggingFace Inc. Special Ops Team",
            "author_email": "hardware@huggingface.co",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 5 - Production/Stable",
                "Intended Audience :: Developers",
                "Intended Audience :: Education",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: Apache Software License",
                "Operating System :: OS Independent",
                "Programming Language :: Python :: 3.7",
                "Programming Language :: Python :: 3.8",
                "Programming Language :: Python :: 3.9",
                "Topic :: Scientific/Engineering :: Artificial Intelligence"
            ],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://www.intel.com",
            "keywords": "transformers,quantization,pruning,knowledge distillation,optimization,training",
            "license": "Apache",
            "maintainer": "",
            "maintainer_email": "",
            "name": "optimum-intel",
            "package_url": "https://pypi.org/project/optimum-intel/",
            "platform": null,
            "project_url": "https://pypi.org/project/optimum-intel/",
            "project_urls": {
                "Homepage": "https://www.intel.com"
            },
            "release_url": "https://pypi.org/project/optimum-intel/1.5.1/",
            "requires_dist": null,
            "requires_python": "",
            "summary": "Optimum Library is an extension of the Hugging Face Transformers library, providing a framework to integrate third-party libraries from Hardware Partners and interface with their specific functionality.",
            "version": "1.5.1",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16211630,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "1ce6923c13116969cabf78135852887a",
                    "sha256": "4e70f1d0a156bce63c6709b12713c224b98c3218f0880de60699aad22dc36c36"
                },
                "downloads": -1,
                "filename": "optimum-intel-1.5.1.tar.gz",
                "has_sig": false,
                "md5_digest": "1ce6923c13116969cabf78135852887a",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 52098,
                "upload_time": "2022-11-14T14:08:12",
                "upload_time_iso_8601": "2022-11-14T14:08:12.895017Z",
                "url": "https://files.pythonhosted.org/packages/7d/f7/5c3e083693c9cbcff6410caa68f9a1b5dc3682fc8d8f3c4c406307453393/optimum-intel-1.5.1.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "1.5.2": {
        "info": {
            "author": "HuggingFace Inc. Special Ops Team",
            "author_email": "hardware@huggingface.co",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 5 - Production/Stable",
                "Intended Audience :: Developers",
                "Intended Audience :: Education",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: Apache Software License",
                "Operating System :: OS Independent",
                "Programming Language :: Python :: 3.7",
                "Programming Language :: Python :: 3.8",
                "Programming Language :: Python :: 3.9",
                "Topic :: Scientific/Engineering :: Artificial Intelligence"
            ],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://www.intel.com",
            "keywords": "transformers,quantization,pruning,knowledge distillation,optimization,training",
            "license": "Apache",
            "maintainer": "",
            "maintainer_email": "",
            "name": "optimum-intel",
            "package_url": "https://pypi.org/project/optimum-intel/",
            "platform": null,
            "project_url": "https://pypi.org/project/optimum-intel/",
            "project_urls": {
                "Homepage": "https://www.intel.com"
            },
            "release_url": "https://pypi.org/project/optimum-intel/1.5.2/",
            "requires_dist": null,
            "requires_python": "",
            "summary": "Optimum Library is an extension of the Hugging Face Transformers library, providing a framework to integrate third-party libraries from Hardware Partners and interface with their specific functionality.",
            "version": "1.5.2",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16211630,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "433630006d36ceb4ee5ffdf098e6b711",
                    "sha256": "c3ce0dfa94b1e7caa0bbc72e8d3ee69f0e7eaca9363291a21072d8c69ccf0c9d"
                },
                "downloads": -1,
                "filename": "optimum-intel-1.5.2.tar.gz",
                "has_sig": false,
                "md5_digest": "433630006d36ceb4ee5ffdf098e6b711",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 52708,
                "upload_time": "2022-11-23T13:27:46",
                "upload_time_iso_8601": "2022-11-23T13:27:46.246223Z",
                "url": "https://files.pythonhosted.org/packages/b5/e8/7f285a60cc89eb83c18dfe7e1c6313b64b4c6688e1b6053617123c9c25ee/optimum-intel-1.5.2.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "1.5.3": {
        "info": {
            "author": "HuggingFace Inc. Special Ops Team",
            "author_email": "hardware@huggingface.co",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 5 - Production/Stable",
                "Intended Audience :: Developers",
                "Intended Audience :: Education",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: Apache Software License",
                "Operating System :: OS Independent",
                "Programming Language :: Python :: 3.7",
                "Programming Language :: Python :: 3.8",
                "Programming Language :: Python :: 3.9",
                "Topic :: Scientific/Engineering :: Artificial Intelligence"
            ],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://www.intel.com",
            "keywords": "transformers,quantization,pruning,knowledge distillation,optimization,training",
            "license": "Apache",
            "maintainer": "",
            "maintainer_email": "",
            "name": "optimum-intel",
            "package_url": "https://pypi.org/project/optimum-intel/",
            "platform": null,
            "project_url": "https://pypi.org/project/optimum-intel/",
            "project_urls": {
                "Homepage": "https://www.intel.com"
            },
            "release_url": "https://pypi.org/project/optimum-intel/1.5.3/",
            "requires_dist": null,
            "requires_python": "",
            "summary": "Optimum Library is an extension of the Hugging Face Transformers library, providing a framework to integrate third-party libraries from Hardware Partners and interface with their specific functionality.",
            "version": "1.5.3",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16211630,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "3c270905ccab692bb75551971ab05709",
                    "sha256": "7af451a058b3ea2b41d51f55c8e56f558c68fe66a242ac5b1ae08ae0b7d60881"
                },
                "downloads": -1,
                "filename": "optimum-intel-1.5.3.tar.gz",
                "has_sig": false,
                "md5_digest": "3c270905ccab692bb75551971ab05709",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 55617,
                "upload_time": "2022-12-05T14:19:17",
                "upload_time_iso_8601": "2022-12-05T14:19:17.927132Z",
                "url": "https://files.pythonhosted.org/packages/ac/59/1e07f5e7cc916168c0eb794aaad7855a218b8411883684a7b87a7d922f3a/optimum-intel-1.5.3.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "1.5.4": {
        "info": {
            "author": "HuggingFace Inc. Special Ops Team",
            "author_email": "hardware@huggingface.co",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 5 - Production/Stable",
                "Intended Audience :: Developers",
                "Intended Audience :: Education",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: Apache Software License",
                "Operating System :: OS Independent",
                "Programming Language :: Python :: 3.7",
                "Programming Language :: Python :: 3.8",
                "Programming Language :: Python :: 3.9",
                "Topic :: Scientific/Engineering :: Artificial Intelligence"
            ],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://www.intel.com",
            "keywords": "transformers,quantization,pruning,knowledge distillation,optimization,training",
            "license": "Apache",
            "maintainer": "",
            "maintainer_email": "",
            "name": "optimum-intel",
            "package_url": "https://pypi.org/project/optimum-intel/",
            "platform": null,
            "project_url": "https://pypi.org/project/optimum-intel/",
            "project_urls": {
                "Homepage": "https://www.intel.com"
            },
            "release_url": "https://pypi.org/project/optimum-intel/1.5.4/",
            "requires_dist": null,
            "requires_python": "",
            "summary": "Optimum Library is an extension of the Hugging Face Transformers library, providing a framework to integrate third-party libraries from Hardware Partners and interface with their specific functionality.",
            "version": "1.5.4",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16211630,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "74ef8f0d9f1f063f58533c6acd886072",
                    "sha256": "b1ff6ff03f86465dcc51887936be0f64b97f60885846810d78c48475369bddd8"
                },
                "downloads": -1,
                "filename": "optimum-intel-1.5.4.tar.gz",
                "has_sig": false,
                "md5_digest": "74ef8f0d9f1f063f58533c6acd886072",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 55659,
                "upload_time": "2022-12-12T17:32:12",
                "upload_time_iso_8601": "2022-12-12T17:32:12.702386Z",
                "url": "https://files.pythonhosted.org/packages/a8/85/55a9d5fcbfc1d88458eb6c48ae213a5deb4e6ecadfeeb7f0eb371134b749/optimum-intel-1.5.4.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "1.5.5": {
        "info": {
            "author": "HuggingFace Inc. Special Ops Team",
            "author_email": "hardware@huggingface.co",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 5 - Production/Stable",
                "Intended Audience :: Developers",
                "Intended Audience :: Education",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: Apache Software License",
                "Operating System :: OS Independent",
                "Programming Language :: Python :: 3.7",
                "Programming Language :: Python :: 3.8",
                "Programming Language :: Python :: 3.9",
                "Topic :: Scientific/Engineering :: Artificial Intelligence"
            ],
            "description": "<p align=\"center\">\n    <img src=\"readme_logo.png\" />\n</p>\n\n# Optimum Intel\n\n\ud83e\udd17 Optimum Intel is the interface between the \ud83e\udd17 Transformers library and the different tools and libraries provided by Intel to accelerate end-to-end pipelines on Intel architectures.\n\nIntel [Neural Compressor](https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html) is an open-source library enabling the usage of the most popular compression techniques such as quantization, pruning and knowledge distillation. It supports automatic accuracy-driven tuning strategies in order for users to easily generate quantized model. The users can easily apply static, dynamic and aware-training quantization approaches while giving an expected accuracy criteria. It also supports different weight pruning techniques enabling the creation of pruned model giving a predefined sparsity target.\n\n[OpenVINO](https://docs.openvino.ai/latest/index.html) is an open-source toolkit that enables high performance inference capabilities for Intel CPUs, GPUs, and special DL inference accelerators. It is supplied with a set of tools to optimize and quantize models. Optimum Intel provides a simple interface to optimize Transformer models, convert them to OpenVINO Intermediate Representation format and to run inference using OpenVINO.\n\n## Installation\n\nTo install the latest release of \ud83e\udd17 Optimum Intel with the corresponding required dependencies, you can use `pip` as follows:\n\n| Accelerator                                                                                                      | Installation                                                        |\n|:-----------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------|\n| [Intel Neural Compressor](https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html) | `python -m pip install optimum[neural-compressor]`                  |\n| [OpenVINO](https://docs.openvino.ai/latest/index.html)                                                           | `python -m pip install optimum[openvino,nncf]`                      |\n\n\nOptimum Intel is a fast-moving project, and you may want to install from source.\n\n```bash\npip install git+https://github.com/huggingface/optimum-intel.git\n```\n\n# Quick tour\n\n## OpenVINO\n\nBelow are the examples of how to use OpenVINO and its [NNCF](https://docs.openvino.ai/latest/tmo_introduction.html) framework to accelerate inference.\n\n#### Inference:\n\nTo load a model and run inference with OpenVINO Runtime, you can just replace your `AutoModelForXxx` class with the corresponding `OVModelForXxx` class.\nIf you want to load a PyTorch checkpoint, set `from_transformers=True` to convert your model to the OpenVINO IR.\n\n```diff\n-from transformers import AutoModelForSequenceClassification\n+from optimum.intel.openvino import OVModelForSequenceClassification\nfrom transformers import AutoTokenizer, pipeline\n\nmodel_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n-model = AutoModelForSequenceClassification.from_pretrained(model_id)\n+model = OVModelForSequenceClassification.from_pretrained(model_id, from_transformers=True)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\npipe_cls = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\ntext = \"He's a dreadful magician.\"\noutputs = pipe_cls(text)\n```\n\n#### Post-training static quantization:\n\nPost-training static quantization introduces an additional calibration step where data is fed through the network in order to compute the activations quantization parameters. Here is an example on how to apply static quantization on a fine-tuned DistilBERT.\n\n```python\nfrom functools import partial\nfrom optimum.intel.openvino import OVQuantizer, OVModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nmodel_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\nmodel = AutoModelForSequenceClassification.from_pretrained(model_id)    \ntokenizer = AutoTokenizer.from_pretrained(model_id)\ndef preprocess_fn(examples, tokenizer):\n    return tokenizer(\n        examples[\"sentence\"], padding=True, truncation=True, max_length=128\n    )\n\nquantizer = OVQuantizer.from_pretrained(model)\ncalibration_dataset = quantizer.get_calibration_dataset(\n    \"glue\",\n    dataset_config_name=\"sst2\",\n    preprocess_function=partial(preprocess_fn, tokenizer=tokenizer),\n    num_samples=100,\n    dataset_split=\"train\",\n    preprocess_batch=True,\n)\n# The directory where the quantized model will be saved\nsave_dir = \"nncf_results\"\n# Apply static quantization and save the resulting model in the OpenVINO IR format\nquantizer.quantize(calibration_dataset=calibration_dataset, save_directory=save_dir)\n# Load the quantized model\noptimized_model = OVModelForSequenceClassification.from_pretrained(save_dir)\n```\n\n#### Quantization-aware training:\n\nQuantization aware training (QAT) is applied in order to simulate the effects of quantization during training, to alleviate its effects on the model\u2019s accuracy. Here is an example on how to fine-tune a DistilBERT model on the sst-2 task while applying quantization aware training (QAT).\n\n```diff\nimport evaluate\nimport numpy as np\nfrom datasets import load_dataset\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, default_data_collator\n-from transformers import Trainer\n+from optimum.intel.openvino import OVConfig, OVModelForSequenceClassification, OVTrainer\n\nmodel_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\nmodel = AutoModelForSequenceClassification.from_pretrained(model_id)    \ntokenizer = AutoTokenizer.from_pretrained(model_id)\ndataset = load_dataset(\"glue\", \"sst2\")\ndataset = dataset.map(\n    lambda examples: tokenizer(examples[\"sentence\"], padding=True, truncation=True, max_length=128), batched=True\n)\nmetric = evaluate.load(\"glue\", \"sst2\")\ncompute_metrics = lambda p: metric.compute(\n    predictions=np.argmax(p.predictions, axis=1), references=p.label_ids\n)\n\n# The directory where the quantized model will be saved\nsave_dir = \"nncf_results\"\n\n# Load the default quantization configuration detailing the quantization we wish to apply\n+ov_config = OVConfig()\n\n-trainer = Trainer(\n+trainer = OVTrainer(\n    model=model,\n    args=TrainingArguments(save_dir, num_train_epochs=1.0, do_train=True, do_eval=True),\n    train_dataset=dataset[\"train\"].select(range(300)),\n    eval_dataset=dataset[\"validation\"],\n    compute_metrics=compute_metrics,\n    tokenizer=tokenizer,\n    data_collator=default_data_collator,\n+   ov_config=ov_config,\n+   feature=\"sequence-classification\",\n)\ntrain_result = trainer.train()\nmetrics = trainer.evaluate()\ntrainer.save_model()\n\n+optimized_model = OVModelForSequenceClassification.from_pretrained(save_dir)\n```\n\n## Neural Compressor\n\n#### Dynamic quantization:\n\nHere is an example on how to apply dynamic quantization on a DistilBERT fine-tuned on the SQuAD1.0 dataset.\nNote that quantization is currently only supported for CPUs (only CPU backends are available), so we will not be utilizing GPUs / CUDA in this example.\n\n```python\nfrom datasets import load_dataset\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\nfrom evaluate import evaluator\nfrom optimum.intel.neural_compressor import IncOptimizer, IncQuantizationConfig, IncQuantizer\n\nmodel_id = \"distilbert-base-cased-distilled-squad\"\nmax_eval_samples = 100\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_id)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\neval_dataset = load_dataset(\"squad\", split=\"validation\").select(range(max_eval_samples))\ntask_evaluator = evaluator(\"question-answering\")\nqa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n\ndef eval_func(model):\n    qa_pipeline.model = model\n    metrics = task_evaluator.compute(model_or_pipeline=qa_pipeline, data=eval_dataset, metric=\"squad\")\n    return metrics[\"f1\"]\n\n# Load the quantization configuration detailing the quantization we wish to apply\nconfig_path = \"echarlaix/distilbert-base-uncased-finetuned-sst-2-english-int8-dynamic\"\nquantization_config = IncQuantizationConfig.from_pretrained(config_path)\n\n# Instantiate our IncQuantizer using the desired configuration and the evaluation function used\n# for the INC accuracy-driven tuning strategy\nquantizer = IncQuantizer(quantization_config, eval_func=eval_func)\noptimizer = IncOptimizer(model, quantizer=quantizer)\n\n# Apply dynamic quantization\nquantized_model = optimizer.fit()\n\n# Save the resulting model and its corresponding configuration in the given directory\noptimizer.save_pretrained(\"./quantized_model\")\n```\n\nTo load a quantized model hosted locally or on the \ud83e\udd17 hub, you can do as follows :\n```python\nfrom optimum.intel.neural_compressor.quantization import IncQuantizedModelForSequenceClassification\n\nloaded_model_from_hub = IncQuantizedModelForSequenceClassification.from_pretrained(\n    \"Intel/distilbert-base-uncased-finetuned-sst-2-english-int8-dynamic\"\n)\n```\n\nYou can load many more quantized models hosted on the hub under the Intel organization [`here`](https://huggingface.co/Intel).\n\nYou can find more examples in the [documentation](https://huggingface.co/docs/optimum/intel/index).\n\n\n## Running the examples\n\nCheck out the [`examples`](https://github.com/huggingface/optimum-intel/tree/main/examples) directory to see how \ud83e\udd17 Optimum Intel can be used to accelerate inference.\n\nDo not forget to install requirements for every example:\n\n```\ncd <example-folder>\npip install -r requirements.txt\n```\n",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://www.intel.com",
            "keywords": "transformers,quantization,pruning,knowledge distillation,optimization,training",
            "license": "Apache",
            "maintainer": "",
            "maintainer_email": "",
            "name": "optimum-intel",
            "package_url": "https://pypi.org/project/optimum-intel/",
            "platform": null,
            "project_url": "https://pypi.org/project/optimum-intel/",
            "project_urls": {
                "Homepage": "https://www.intel.com"
            },
            "release_url": "https://pypi.org/project/optimum-intel/1.5.5/",
            "requires_dist": null,
            "requires_python": "",
            "summary": "Optimum Library is an extension of the Hugging Face Transformers library, providing a framework to integrate third-party libraries from Hardware Partners and interface with their specific functionality.",
            "version": "1.5.5",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16211630,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "f07ec9c4369bec5b0e1cea763d7e7908",
                    "sha256": "25733a6a66facbf9d4e34c7ef565abcf870a7dc5308025e3d33797c9d780de85"
                },
                "downloads": -1,
                "filename": "optimum-intel-1.5.5.tar.gz",
                "has_sig": false,
                "md5_digest": "f07ec9c4369bec5b0e1cea763d7e7908",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 55914,
                "upload_time": "2022-12-25T23:34:38",
                "upload_time_iso_8601": "2022-12-25T23:34:38.451027Z",
                "url": "https://files.pythonhosted.org/packages/2c/69/97d3243572d7ec8c6159b60d93fdeac040f35b71be21ef18ab31dd89976e/optimum-intel-1.5.5.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}