{
    "0.0.0": {
        "info": {
            "author": "laohur",
            "author_email": "laohur@gmail.com",
            "bugtrack_url": null,
            "classifiers": [],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/laohur/UnicodeTokenizer",
            "keywords": "UnicodeTokenizer,Tokenizer,Unicode,laohur",
            "license": "[Anti-996 License](https: // github.com/996icu/996.ICU/blob/master/LICENSE)",
            "maintainer": "",
            "maintainer_email": "",
            "name": "UnicodeTokenizer",
            "package_url": "https://pypi.org/project/UnicodeTokenizer/",
            "platform": null,
            "project_url": "https://pypi.org/project/UnicodeTokenizer/",
            "project_urls": {
                "Homepage": "https://github.com/laohur/UnicodeTokenizer"
            },
            "release_url": "https://pypi.org/project/UnicodeTokenizer/0.0.0/",
            "requires_dist": null,
            "requires_python": ">=3.0",
            "summary": "UnicodeTokenizer: tokenize all Unicode text",
            "version": "0.0.0",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16269973,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "76a084e3ca3d987a892eed0f7ba16e7f",
                    "sha256": "9af978b00e80079b336b25531119fbb2ad05beb0cabb87b796ba0b0ac412fbc3"
                },
                "downloads": -1,
                "filename": "UnicodeTokenizer-0.0.0-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "76a084e3ca3d987a892eed0f7ba16e7f",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.0",
                "size": 5453,
                "upload_time": "2022-06-25T12:26:44",
                "upload_time_iso_8601": "2022-06-25T12:26:44.146089Z",
                "url": "https://files.pythonhosted.org/packages/05/f7/320fe71db95a096c6a6aea800053db81f686e6402ee5e2710ce75020a53f/UnicodeTokenizer-0.0.0-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "ccd9d0f850bf3c0bc7cca1161af622cf",
                    "sha256": "285afd28905e9273d44bf4af77146e0be5c934bf45a6b785e2f3f8fdf71e4eb1"
                },
                "downloads": -1,
                "filename": "UnicodeTokenizer-0.0.0.tar.gz",
                "has_sig": false,
                "md5_digest": "ccd9d0f850bf3c0bc7cca1161af622cf",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.0",
                "size": 5663,
                "upload_time": "2022-06-25T12:26:46",
                "upload_time_iso_8601": "2022-06-25T12:26:46.239584Z",
                "url": "https://files.pythonhosted.org/packages/99/c0/2f0c9e1644c2626e863848c527624bb74d637834dd8fcfe9ffb34d5667e5/UnicodeTokenizer-0.0.0.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "0.0.1": {
        "info": {
            "author": "laohur",
            "author_email": "laohur@gmail.com",
            "bugtrack_url": null,
            "classifiers": [],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/laohur/UnicodeTokenizer",
            "keywords": "UnicodeTokenizer,Tokenizer,Unicode,laohur",
            "license": "[Anti-996 License](https: // github.com/996icu/996.ICU/blob/master/LICENSE)",
            "maintainer": "",
            "maintainer_email": "",
            "name": "UnicodeTokenizer",
            "package_url": "https://pypi.org/project/UnicodeTokenizer/",
            "platform": null,
            "project_url": "https://pypi.org/project/UnicodeTokenizer/",
            "project_urls": {
                "Homepage": "https://github.com/laohur/UnicodeTokenizer"
            },
            "release_url": "https://pypi.org/project/UnicodeTokenizer/0.0.1/",
            "requires_dist": null,
            "requires_python": ">=3.0",
            "summary": "UnicodeTokenizer: tokenize all Unicode text",
            "version": "0.0.1",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16269973,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "338cb4673244f8b9f6ee4ad1723195fc",
                    "sha256": "d319469ee74add7311bcd5bff441b5130879f53d37b126db51b9264da370e7e4"
                },
                "downloads": -1,
                "filename": "UnicodeTokenizer-0.0.1-py2.py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "338cb4673244f8b9f6ee4ad1723195fc",
                "packagetype": "bdist_wheel",
                "python_version": "py2.py3",
                "requires_python": ">=3.0",
                "size": 30503,
                "upload_time": "2022-06-26T14:14:54",
                "upload_time_iso_8601": "2022-06-26T14:14:54.659198Z",
                "url": "https://files.pythonhosted.org/packages/ed/ff/c580a30f96537b9103008479f77e68c3dc4dae05fb6649d7982e3195f9b4/UnicodeTokenizer-0.0.1-py2.py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "122b366104bc4e54088c697ef610e9ee",
                    "sha256": "8b6a22222143c81149b8e6276d93df9df9c149312c1e1df1d0742289e3837768"
                },
                "downloads": -1,
                "filename": "UnicodeTokenizer-0.0.1-py3.9.egg",
                "has_sig": false,
                "md5_digest": "122b366104bc4e54088c697ef610e9ee",
                "packagetype": "bdist_egg",
                "python_version": "0.0.1",
                "requires_python": ">=3.0",
                "size": 60527,
                "upload_time": "2022-06-26T14:14:58",
                "upload_time_iso_8601": "2022-06-26T14:14:58.632123Z",
                "url": "https://files.pythonhosted.org/packages/04/03/1f6bd90979767d2e28786554df5d1db406b63c82a278b5acf49b674de026/UnicodeTokenizer-0.0.1-py3.9.egg",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "927fda1507fae7752a281ae544c8a26c",
                    "sha256": "549f1469c4e056f0a107144069b5fe556496d8ecb5d7aa4c7b80d70a7918f7b1"
                },
                "downloads": -1,
                "filename": "UnicodeTokenizer-0.0.1-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "927fda1507fae7752a281ae544c8a26c",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.0",
                "size": 5467,
                "upload_time": "2022-06-25T12:32:38",
                "upload_time_iso_8601": "2022-06-25T12:32:38.201542Z",
                "url": "https://files.pythonhosted.org/packages/e3/45/863b9144a41c1ac2d2de61473be2c9940c676cb8c91fe9efc938550d2075/UnicodeTokenizer-0.0.1-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "5199742cf48a8f84c872c75a3980cff8",
                    "sha256": "85ad3d53ce1370da494e69d6cbebaea13847418f399c120bcef206794e31ca37"
                },
                "downloads": -1,
                "filename": "UnicodeTokenizer-0.0.1.tar.gz",
                "has_sig": false,
                "md5_digest": "5199742cf48a8f84c872c75a3980cff8",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.0",
                "size": 5684,
                "upload_time": "2022-06-25T12:32:41",
                "upload_time_iso_8601": "2022-06-25T12:32:41.448794Z",
                "url": "https://files.pythonhosted.org/packages/d8/fb/fc09c4a8fb5b638ec7cff884b5e3e62d004d3890f9c8a16a2ccf106ad792/UnicodeTokenizer-0.0.1.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "0.0.2": {
        "info": {
            "author": "laohur",
            "author_email": "laohur@gmail.com",
            "bugtrack_url": null,
            "classifiers": [],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/laohur/UnicodeTokenizer",
            "keywords": "UnicodeTokenizer,Tokenizer,Unicode,laohur",
            "license": "[Anti-996 License](https: // github.com/996icu/996.ICU/blob/master/LICENSE)",
            "maintainer": "",
            "maintainer_email": "",
            "name": "UnicodeTokenizer",
            "package_url": "https://pypi.org/project/UnicodeTokenizer/",
            "platform": null,
            "project_url": "https://pypi.org/project/UnicodeTokenizer/",
            "project_urls": {
                "Homepage": "https://github.com/laohur/UnicodeTokenizer"
            },
            "release_url": "https://pypi.org/project/UnicodeTokenizer/0.0.2/",
            "requires_dist": null,
            "requires_python": ">=3.0",
            "summary": "UnicodeTokenizer: tokenize all Unicode text",
            "version": "0.0.2",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16269973,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "52e810248f20b7ea5c272276b0632270",
                    "sha256": "f0b921dddcf36e180e17d98523c876d9c38ec9fad23939941f4968e15991a026"
                },
                "downloads": -1,
                "filename": "UnicodeTokenizer-0.0.2-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "52e810248f20b7ea5c272276b0632270",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.0",
                "size": 30543,
                "upload_time": "2022-06-26T14:14:56",
                "upload_time_iso_8601": "2022-06-26T14:14:56.719528Z",
                "url": "https://files.pythonhosted.org/packages/a8/82/36c5b0834c35c1bb773ac763fc945c0aaeb37bc5a6f8a669037298a6ad54/UnicodeTokenizer-0.0.2-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "d4bd5caadf865192d33b3909ba21d854",
                    "sha256": "949a7f8884a851aba68e35253d310e13716be60d2d9a0a89a43be9dd24779f91"
                },
                "downloads": -1,
                "filename": "UnicodeTokenizer-0.0.2.tar.gz",
                "has_sig": false,
                "md5_digest": "d4bd5caadf865192d33b3909ba21d854",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.0",
                "size": 17661,
                "upload_time": "2022-06-26T14:15:00",
                "upload_time_iso_8601": "2022-06-26T14:15:00.644884Z",
                "url": "https://files.pythonhosted.org/packages/31/8b/3520d5208c9fcb960d2689e5d59062bfe7391fadcbad0dd425e8af81369e/UnicodeTokenizer-0.0.2.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "0.0.3": {
        "info": {
            "author": "laohur",
            "author_email": "laohur@gmail.com",
            "bugtrack_url": null,
            "classifiers": [],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/laohur/UnicodeTokenizer",
            "keywords": "UnicodeTokenizer,Tokenizer,Unicode,laohur",
            "license": "[Anti-996 License](https: // github.com/996icu/996.ICU/blob/master/LICENSE)",
            "maintainer": "",
            "maintainer_email": "",
            "name": "UnicodeTokenizer",
            "package_url": "https://pypi.org/project/UnicodeTokenizer/",
            "platform": null,
            "project_url": "https://pypi.org/project/UnicodeTokenizer/",
            "project_urls": {
                "Homepage": "https://github.com/laohur/UnicodeTokenizer"
            },
            "release_url": "https://pypi.org/project/UnicodeTokenizer/0.0.3/",
            "requires_dist": null,
            "requires_python": ">=3.0",
            "summary": "UnicodeTokenizer: tokenize all Unicode text",
            "version": "0.0.3",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16269973,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "39c9d195dd7d4bed68784f8b432624cd",
                    "sha256": "3211279872e7c7bd1318d414ce5ada99383cb3b94a6598014f0ff6d8be89c9db"
                },
                "downloads": -1,
                "filename": "UnicodeTokenizer-0.0.3-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "39c9d195dd7d4bed68784f8b432624cd",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.0",
                "size": 13986,
                "upload_time": "2022-06-30T16:33:40",
                "upload_time_iso_8601": "2022-06-30T16:33:40.218800Z",
                "url": "https://files.pythonhosted.org/packages/30/d3/49ae7e2e1249d45666bdcbf9a8408f5794cc5b42ea2575360e1d89c05308/UnicodeTokenizer-0.0.3-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "be6e0409e1cdb7ae85dac1f4a39265ff",
                    "sha256": "ebb620bd3480f4fb47f8d4b3b021dd04b4b8198f111141c0237758740723d853"
                },
                "downloads": -1,
                "filename": "UnicodeTokenizer-0.0.3.tar.gz",
                "has_sig": false,
                "md5_digest": "be6e0409e1cdb7ae85dac1f4a39265ff",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.0",
                "size": 17672,
                "upload_time": "2022-06-30T16:33:42",
                "upload_time_iso_8601": "2022-06-30T16:33:42.507995Z",
                "url": "https://files.pythonhosted.org/packages/be/47/ff960f3d6c2308b1724e262606a8e9375551d9cd27190ab22a566dcc50bc/UnicodeTokenizer-0.0.3.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "0.0.4": {
        "info": {
            "author": "laohur",
            "author_email": "laohur@gmail.com",
            "bugtrack_url": null,
            "classifiers": [],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/laohur/UnicodeTokenizer",
            "keywords": "UnicodeTokenizer,Tokenizer,Unicode,laohur",
            "license": "[Anti-996 License](https: // github.com/996icu/996.ICU/blob/master/LICENSE)",
            "maintainer": "",
            "maintainer_email": "",
            "name": "UnicodeTokenizer",
            "package_url": "https://pypi.org/project/UnicodeTokenizer/",
            "platform": null,
            "project_url": "https://pypi.org/project/UnicodeTokenizer/",
            "project_urls": {
                "Homepage": "https://github.com/laohur/UnicodeTokenizer"
            },
            "release_url": "https://pypi.org/project/UnicodeTokenizer/0.0.4/",
            "requires_dist": null,
            "requires_python": ">=3.0",
            "summary": "UnicodeTokenizer: tokenize all Unicode text",
            "version": "0.0.4",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16269973,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "ec2eee017631ea7b8a1d5d085ef2c906",
                    "sha256": "c63b6d86a3440e3e8036851e89f8df6bbb32a363418f2cade04fdbbca4eeca77"
                },
                "downloads": -1,
                "filename": "UnicodeTokenizer-0.0.4-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "ec2eee017631ea7b8a1d5d085ef2c906",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.0",
                "size": 13982,
                "upload_time": "2022-07-05T14:06:42",
                "upload_time_iso_8601": "2022-07-05T14:06:42.066998Z",
                "url": "https://files.pythonhosted.org/packages/89/7e/d3533fd9cf31474bcaf612269363595e52023735cc4892717fcb2752c3bb/UnicodeTokenizer-0.0.4-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "2d28b169af38dd591ac51da69e25522c",
                    "sha256": "fc8142ea4ae92a89275cd62d29aae0f1e036eeed8a22525b418e13a998b0c194"
                },
                "downloads": -1,
                "filename": "UnicodeTokenizer-0.0.4.tar.gz",
                "has_sig": false,
                "md5_digest": "2d28b169af38dd591ac51da69e25522c",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.0",
                "size": 17666,
                "upload_time": "2022-07-05T14:06:44",
                "upload_time_iso_8601": "2022-07-05T14:06:44.450949Z",
                "url": "https://files.pythonhosted.org/packages/88/de/463058fd51486082fca99ee4d23f0b8d8b59bed41cbfdb83d12730f93b22/UnicodeTokenizer-0.0.4.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "0.0.5": {
        "info": {
            "author": "laohur",
            "author_email": "laohur@gmail.com",
            "bugtrack_url": null,
            "classifiers": [],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/laohur/UnicodeTokenizer",
            "keywords": "UnicodeTokenizer,Tokenizer,Unicode,laohur",
            "license": "[Anti-996 License](https: // github.com/996icu/996.ICU/blob/master/LICENSE)",
            "maintainer": "",
            "maintainer_email": "",
            "name": "UnicodeTokenizer",
            "package_url": "https://pypi.org/project/UnicodeTokenizer/",
            "platform": null,
            "project_url": "https://pypi.org/project/UnicodeTokenizer/",
            "project_urls": {
                "Homepage": "https://github.com/laohur/UnicodeTokenizer"
            },
            "release_url": "https://pypi.org/project/UnicodeTokenizer/0.0.5/",
            "requires_dist": null,
            "requires_python": ">=3.0",
            "summary": "UnicodeTokenizer: tokenize all Unicode text",
            "version": "0.0.5",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16269973,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "1ee6859a2b198b4659299a3441a7ffe9",
                    "sha256": "75bcde58eb6e1b32f24945dc2d636ab6f70936e4e14dc5f67cfc44151e2623b7"
                },
                "downloads": -1,
                "filename": "UnicodeTokenizer-0.0.5-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "1ee6859a2b198b4659299a3441a7ffe9",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.0",
                "size": 13938,
                "upload_time": "2022-07-11T12:55:38",
                "upload_time_iso_8601": "2022-07-11T12:55:38.638940Z",
                "url": "https://files.pythonhosted.org/packages/f5/32/4cd36c68195ce700b9c48cff0d6dc4700c12b1c7741c5bbff296e2003113/UnicodeTokenizer-0.0.5-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "0.0.6": {
        "info": {
            "author": "laohur",
            "author_email": "laohur@gmail.com",
            "bugtrack_url": null,
            "classifiers": [],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/laohur/UnicodeTokenizer",
            "keywords": "UnicodeTokenizer,Tokenizer,Unicode,laohur",
            "license": "[Anti-996 License](https: // github.com/996icu/996.ICU/blob/master/LICENSE)",
            "maintainer": "",
            "maintainer_email": "",
            "name": "UnicodeTokenizer",
            "package_url": "https://pypi.org/project/UnicodeTokenizer/",
            "platform": null,
            "project_url": "https://pypi.org/project/UnicodeTokenizer/",
            "project_urls": {
                "Homepage": "https://github.com/laohur/UnicodeTokenizer"
            },
            "release_url": "https://pypi.org/project/UnicodeTokenizer/0.0.6/",
            "requires_dist": null,
            "requires_python": ">=3.0",
            "summary": "UnicodeTokenizer: tokenize all Unicode text",
            "version": "0.0.6",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16269973,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "bd2341dc41ff93d6c0b4febacf419961",
                    "sha256": "03ab83ffbe210f76d4f7b8c2d73c3064ff633d7cf75d3958779a963bcb946ae5"
                },
                "downloads": -1,
                "filename": "UnicodeTokenizer-0.0.6-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "bd2341dc41ff93d6c0b4febacf419961",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.0",
                "size": 13896,
                "upload_time": "2022-07-18T13:53:03",
                "upload_time_iso_8601": "2022-07-18T13:53:03.772267Z",
                "url": "https://files.pythonhosted.org/packages/8c/0c/8c71f77311db187003f2741587c817f23dcbe82169ad5648501d01224bdc/UnicodeTokenizer-0.0.6-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "7975ce63f3295c01d6019072676b1df4",
                    "sha256": "093a358bf6334e49f4d1502bfa62fe6aa04d9d78f30d098e60284bdfd0b5a2f9"
                },
                "downloads": -1,
                "filename": "UnicodeTokenizer-0.0.6.tar.gz",
                "has_sig": false,
                "md5_digest": "7975ce63f3295c01d6019072676b1df4",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.0",
                "size": 17633,
                "upload_time": "2022-07-18T13:53:06",
                "upload_time_iso_8601": "2022-07-18T13:53:06.901564Z",
                "url": "https://files.pythonhosted.org/packages/a1/fa/a73908deb93262d5a66c3b8646690fd6fa2cd0ae0ce9fcc706d02552416d/UnicodeTokenizer-0.0.6.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "0.0.7": {
        "info": {
            "author": "laohur",
            "author_email": "laohur@gmail.com",
            "bugtrack_url": null,
            "classifiers": [],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/laohur/UnicodeTokenizer",
            "keywords": "UnicodeTokenizer,Tokenizer,Unicode,ZiTokenizer,ZiCutter,laohur",
            "license": "[Anti-996 License](https: // github.com/996icu/996.ICU/blob/master/LICENSE)",
            "maintainer": "",
            "maintainer_email": "",
            "name": "UnicodeTokenizer",
            "package_url": "https://pypi.org/project/UnicodeTokenizer/",
            "platform": null,
            "project_url": "https://pypi.org/project/UnicodeTokenizer/",
            "project_urls": {
                "Homepage": "https://github.com/laohur/UnicodeTokenizer"
            },
            "release_url": "https://pypi.org/project/UnicodeTokenizer/0.0.7/",
            "requires_dist": null,
            "requires_python": ">=3.0",
            "summary": "UnicodeTokenizer: tokenize all Unicode text",
            "version": "0.0.7",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16269973,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "a7caea91e4b160ea3b4628661e89f59a",
                    "sha256": "482a893a7bc7a4b58596a812a4fd6ec5199552a9ac7fa250f48d5051cae21099"
                },
                "downloads": -1,
                "filename": "UnicodeTokenizer-0.0.7-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "a7caea91e4b160ea3b4628661e89f59a",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.0",
                "size": 14109,
                "upload_time": "2022-07-31T15:40:05",
                "upload_time_iso_8601": "2022-07-31T15:40:05.033226Z",
                "url": "https://files.pythonhosted.org/packages/ff/cd/a776d240041b7bb1ff28cccbdddbe35606caeb8553cbcdeea8c0151ae8d1/UnicodeTokenizer-0.0.7-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "242bfb5274c8acff7b9b2968eea87d14",
                    "sha256": "4d1865e6e45a57097e3f05c0c7ceacd87757cd957169534116ea82eb8461a6d7"
                },
                "downloads": -1,
                "filename": "UnicodeTokenizer-0.0.7.tar.gz",
                "has_sig": false,
                "md5_digest": "242bfb5274c8acff7b9b2968eea87d14",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.0",
                "size": 17840,
                "upload_time": "2022-07-31T15:40:07",
                "upload_time_iso_8601": "2022-07-31T15:40:07.156063Z",
                "url": "https://files.pythonhosted.org/packages/42/ec/147f8409a69e8688b06c7b8b4aaba8fdc3518f6aa351b96ce40e1ded8896/UnicodeTokenizer-0.0.7.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "0.0.8": {
        "info": {
            "author": "laohur",
            "author_email": "laohur@gmail.com",
            "bugtrack_url": null,
            "classifiers": [],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/laohur/UnicodeTokenizer",
            "keywords": "UnicodeTokenizer,Tokenizer,Unicode,ZiTokenizer,ZiCutter,laohur",
            "license": "[Anti-996 License](https: // github.com/996icu/996.ICU/blob/master/LICENSE)",
            "maintainer": "",
            "maintainer_email": "",
            "name": "UnicodeTokenizer",
            "package_url": "https://pypi.org/project/UnicodeTokenizer/",
            "platform": null,
            "project_url": "https://pypi.org/project/UnicodeTokenizer/",
            "project_urls": {
                "Homepage": "https://github.com/laohur/UnicodeTokenizer"
            },
            "release_url": "https://pypi.org/project/UnicodeTokenizer/0.0.8/",
            "requires_dist": null,
            "requires_python": ">=3.0",
            "summary": "UnicodeTokenizer: tokenize all Unicode text",
            "version": "0.0.8",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16269973,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "4a1b259e02b00d66af9bd639492afc81",
                    "sha256": "22e24c6f3b9371123d861e4a0d3a72feefc583bbfe6727227524338397df81ec"
                },
                "downloads": -1,
                "filename": "UnicodeTokenizer-0.0.8-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "4a1b259e02b00d66af9bd639492afc81",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.0",
                "size": 13845,
                "upload_time": "2022-08-15T12:52:49",
                "upload_time_iso_8601": "2022-08-15T12:52:49.606555Z",
                "url": "https://files.pythonhosted.org/packages/d3/ee/7c7417cd619200b89c1a80c15296927215e6320796a03a5e6f9df8760dd0/UnicodeTokenizer-0.0.8-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "0496f7b82ef3280f218f962229ab09bf",
                    "sha256": "c4392e8a47a017f3bb9e8ee07c5da8fb3a83785af6497bc93a24a7d01bc90e3e"
                },
                "downloads": -1,
                "filename": "UnicodeTokenizer-0.0.8.tar.gz",
                "has_sig": false,
                "md5_digest": "0496f7b82ef3280f218f962229ab09bf",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.0",
                "size": 13694,
                "upload_time": "2022-08-15T12:52:51",
                "upload_time_iso_8601": "2022-08-15T12:52:51.574935Z",
                "url": "https://files.pythonhosted.org/packages/4d/90/ab67e33c16f7757156f24d2dd8c7341b9952daf3da71cfb8402bbea2304c/UnicodeTokenizer-0.0.8.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "0.0.9": {
        "info": {
            "author": "laohur",
            "author_email": "laohur@gmail.com",
            "bugtrack_url": null,
            "classifiers": [],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/laohur/UnicodeTokenizer",
            "keywords": "UnicodeTokenizer,Tokenizer,Unicode,ZiTokenizer,ZiCutter,laohur",
            "license": "[Anti-996 License](https: // github.com/996icu/996.ICU/blob/master/LICENSE)",
            "maintainer": "",
            "maintainer_email": "",
            "name": "UnicodeTokenizer",
            "package_url": "https://pypi.org/project/UnicodeTokenizer/",
            "platform": null,
            "project_url": "https://pypi.org/project/UnicodeTokenizer/",
            "project_urls": {
                "Homepage": "https://github.com/laohur/UnicodeTokenizer"
            },
            "release_url": "https://pypi.org/project/UnicodeTokenizer/0.0.9/",
            "requires_dist": null,
            "requires_python": ">=3.0",
            "summary": "UnicodeTokenizer: tokenize all Unicode text",
            "version": "0.0.9",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16269973,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "fd5a111d1505aa2cdeb49b94b08c2cbe",
                    "sha256": "639d1d47697c37e3640cdfb68dcfcaf230ef3ec0fa26c1cfe799b639713a5609"
                },
                "downloads": -1,
                "filename": "UnicodeTokenizer-0.0.9-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "fd5a111d1505aa2cdeb49b94b08c2cbe",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.0",
                "size": 6823,
                "upload_time": "2022-12-22T15:24:37",
                "upload_time_iso_8601": "2022-12-22T15:24:37.387239Z",
                "url": "https://files.pythonhosted.org/packages/6b/ea/f0aede76e6205a1ded9b9bc09898a9e55c0196bb50139be525675ec1cf32/UnicodeTokenizer-0.0.9-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "f20457d396be2b64f7a41a98391d799c",
                    "sha256": "dbfddfee2345919111464c6e2da462d6790433662a6465f53d698d96a94d747c"
                },
                "downloads": -1,
                "filename": "UnicodeTokenizer-0.0.9.tar.gz",
                "has_sig": false,
                "md5_digest": "f20457d396be2b64f7a41a98391d799c",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.0",
                "size": 6772,
                "upload_time": "2022-12-22T15:24:39",
                "upload_time_iso_8601": "2022-12-22T15:24:39.006172Z",
                "url": "https://files.pythonhosted.org/packages/d7/7f/f55c3baf4c4b616b1dd773e212536a9ae34b70dbd94252e67bbfe809c397/UnicodeTokenizer-0.0.9.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "0.1.0": {
        "info": {
            "author": "laohur",
            "author_email": "laohur@gmail.com",
            "bugtrack_url": null,
            "classifiers": [],
            "description": "# UnicodeTokenizer\n\nUnicodeTokenizer: tokenize all Unicode text, tokenize blank char as a token as default\n\n## \u5207\u8bcd\u89c4\u5219 Tokenize Rules\n* \u7a7a\u767d\u5207\u5206 split on blank\uff1a '\\n', ' ', '\\t'\n* \u4fdd\u7559\u5173\u952e\u8bcd keep never_splits\n* \u82e5\u5c0f\u5199\uff0c\u5219\u89c4\u8303\u5316\uff1a\u5168\u89d2\u8f6c\u534a\u89d2\uff0c\u5219NFD\u89c4\u8303\u5316\uff0c\u518d\u5b57\u7b26\u5206\u5272  nomalize if lower\uff1afull2half\uff0cnomalize NFD, then chars split \n    - \u7c7b\u522b\u4e3aM\uff0c\u7565\u8fc7 ingore if category M \uff1a category M -> ''\n* \u5b57\u7b26\u5206\u5272 chars split\uff1a \u4ee5\u5b57\u7684\u7b26\u53f7\u7c7b\u522b\u548c\u8bed\u8a00\u5206\u5272 split line by category and languae of characters\n    - \u53ea\u6709\u4e34\u8fd1\u6570\u5b57\u6210\u8bcd only numers joind\n    - \u53ea\u6709\u4e34\u8fd1\u5b57\u6bcd\u6210\u8bcd only letters joind \n    - \u9ad8\u7801\u70b9\u72ec\u5b57  split high UnicodePoint characters\n\n* \u622a\u65ad max_len\n\n\n## use\n> pip install UnicodeTokenizer\n\n```python\nfrom UnicodeTokenizer import UnicodeTokenizer\ntokenizer=UnicodeTokenizer()\n\ndoc0 = \"\"\" \uf87f\n        \u9996\u51488.88\u8bbe\u7f6e st\u3002art_new_word=True \u548c output=[a\u00e7a\u00ed]\uff0coutput \u5c31\u662f\u6700\u7ec8\uf87f\ued30\u0091 no such name\"\n        \u7684\u8f93\u51fa\u0e04\u0e38\u0e13\u0e08\u0e30\u0e08\u0e31\u0e14\u0e1e\u0e34\u0e18\u0e35\u0e41\u0e15\u0e48\u0e07\u0e07\u0e32\u0e19\u0e40\u0e21\u0e37\u0e48\u0e2d\u0e44\u0e23\u0e04\u0e30\ud0d1\uc2b9 \uc218\uc18d\ud574\uc57cpneumonoultramicroscopicsilicovolcanoconiosis\"\n        \ud558\ub294\ub370 \uce74\uc6b4\ud130\uac00 \uc5b4\ub514\uc5d0 \uc788\uc5b4\uc694\ua183\ua3ad\ua188\ua320\ua2a8\ua3e6\ua3f2\ua149\ua185\ua25a\ua149\ua2cd\ua0b7\ua0b6\ua320\u0644\u0623\u062d\u064a\u0627\u0621 \u062a\u0645\u0627\u0631\u064a\u0646 \u062a\u062a\u0637\u0644\u0628 \u0645\u0646 [MASK] [PAD] [CLS][SEP]\n        est \ud81f\udd02\ud81f\ude6d\ud821\udf36\ud81f\udd32\ud81c\udca7, ou \"phiow-bjij-lhjij-lhjij\", ce que l'on peut traduire par \u00ab pays-grand-blanc-\u00e9lev\u00e9 \u00bb (\u767d\u9ad8\u5927\u590f\u570b). \n    \"\"\"\nprint(tokenizer.tokenize(doc0))\n```\n\n## result \n\n| sentence                                                                                                                                                                                                                                                                                                          | UnicodeTokenizer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | Unicode Tokens Length | BertBasicTokenizer                                                                                                                                                                                                                                                                                                 | Bert Tokens length |\n|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------|\n| \u2167\u9996\u51488.88\u8bbe\u7f6e st\u3002art_new_word=True \u548c output=[a\u00e7a\u00ed]\uff0coutput \u5c31\u662f\u6700\u7ec8\uf87f\ued30\u0091 no such name                                                                                                                                                                                                                              | \u2177 \u9996 \u5148 8 . 88 \u8bbe \u7f6e st \u3002 art _ new _ word = true \u548c output = [ ac \u0327 ai \u0301 ] \uff0c output \u5c31 \u662f \u6700 \u7ec8 \uf87f \ued30 \u0091 no such name                                                                                                                                                                                                                                                                                                                                                                               | 38                    | \u2177 \u9996 \u5148 8 . 88 \u8bbe \u7f6e st \u3002 art _ new _ word = true \u548c output = [ acai ] \uff0c output \u5c31 \u662f \u6700 \u7ec8 no such name                                                                                                                                                                                                         | 32                 |\n| \u7684\u8f93\u51fa\u0e04\u0e38\u0e13\u0e08\u0e30\u0e08\u0e31\u0e14\u0e1e\u0e34\u0e18\u0e35\u0e41\u0e15\u0e48\u0e07\u0e07\u0e32\u0e19\u0e40\u0e21\u0e37\u0e48\u0e2d\u0e44\u0e23\u0e04\u0e30\ud0d1\uc2b9 \uc218\uc18d\ud574\uc57cpneumonoultramicroscopicsilicovolcanoconiosis                                                                                                                                                                                                                             | \u7684 \u8f93 \u51fa \u0e04 \u0e38 \u0e13\u0e08\u0e30\u0e08 \u0e31 \u0e14\u0e1e \u0e34 \u0e18 \u0e35 \u0e41\u0e15 \u0e48 \u0e07\u0e07\u0e32\u0e19\u0e40\u0e21 \u0e37 \u0e48 \u0e2d\u0e44\u0e23\u0e04\u0e30 \u1110\u1161\u11b8\u1109\u1173\u11bc \u1109\u116e\u1109\u1169\u11a8\u1112\u1162\u110b\u1163 pneumonoultramicroscopicsilicovolcanoconiosis                                                                                                                                                                                                                                                                                                                                                                                          | 20                    | \u7684 \u8f93 \u51fa \u0e04\u0e13\u0e08\u0e30\u0e08\u0e14\u0e1e\u0e18\u0e41\u0e15\u0e07\u0e07\u0e32\u0e19\u0e40\u0e21\u0e2d\u0e44\u0e23\u0e04\u0e30\u1110\u1161\u11b8\u1109\u1173\u11bc \u1109\u116e\u1109\u1169\u11a8\u1112\u1162\u110b\u1163pneumonoultramicroscopicsilicovolcanoconiosis                                                                                                                                                                                                                           | 5                  |\n| \ud558\ub294\ub370 \uce74\uc6b4\ud130\uac00 \uc5b4\ub514\uc5d0 \uc788\uc5b4\uc694\ua183\ua3ad\ua188\ua320\ua2a8\ua3e6\ua3f2\ua149\ua185\ua25a\ua149\ua2cd\ua0b7\ua0b6\ua320\u0644\u0623\u062d\u064a\u0627\u0621 \u062a\u0645\u0627\u0631\u064a\u0646 \u062a\u062a\u0637\u0644\u0628 \u0645\u0646 [MASK] [PAD] [CLS][SEP]                                                                                                                                                                                                         | \u1112\u1161\u1102\u1173\u11ab\u1103\u1166 \u110f\u1161\u110b\u116e\u11ab\u1110\u1165\u1100\u1161 \u110b\u1165\u1103\u1175\u110b\u1166 \u110b\u1175\u11bb\u110b\u1165\u110b\u116d \ua183 \ua3ad \ua188 \ua320 \ua2a8 \ua3e6 \ua3f2 \ua149 \ua185 \ua25a \ua149 \ua2cd \ua0b7 \ua0b6 \ua320 \u0644\u0627 \u0654 \u062d\u064a\u0627\u0621 \u062a\u0645\u0627\u0631\u064a\u0646 \u062a\u062a\u0637\u0644\u0628 \u0645\u0646 [MASK] [PAD] [ cls ] [ sep ]                                                                                                                                                                                                                                                                                                                                                                 | 33                    | \u1112\u1161\u1102\u1173\u11ab\u1103\u1166 \u110f\u1161\u110b\u116e\u11ab\u1110\u1165\u1100\u1161 \u110b\u1165\u1103\u1175\u110b\u1166 \u110b\u1175\u11bb\u110b\u1165\u110b\u116d\ua183\ua3ad\ua188\ua320\ua2a8\ua3e6\ua3f2\ua149\ua185\ua25a\ua149\ua2cd\ua0b7\ua0b6\ua320\u0644\u0627\u062d\u064a\u0627\u0621 \u062a\u0645\u0627\u0631\u064a\u0646 \u062a\u062a\u0637\u0644\u0628 \u0645\u0646 [MASK] [PAD] [ cls ] [ sep ]                                                                                                                                                                                                     | 15                 |\n| est \ud81f\udd02\ud81f\ude6d\ud821\udf36\ud81f\udd32\ud81c\udca7, ou \"phiow-bjij-lhjij-lhjij\", ce que l'on peut traduire par \u00ab pays-grand-blanc-\u00e9lev\u00e9 \u00bb (\u767d\u9ad8\u5927\u590f\u570b).                                                                                                                                                                                                    | est \ud81f\udd02 \ud81f\ude6d \ud821\udf36 \ud81f\udd32 \ud81c\udca7 , ou \" phiow - bjij - lhjij - lhjij \" , ce que l ' on peut traduire par \u00ab pays - grand - blanc - e \u0301 leve \u0301 \u00bb ( \u767d \u9ad8 \u5927 \u590f \u570b ) .                                                                                                                                                                                                                                                                                                                                                   | 46                    | est \ud81f\udd02\ud81f\ude6d\ud821\udf36\ud81f\udd32\ud81c\udca7 , ou \" phiow - bjij - lhjij - lhjij \" , ce que l ' on peut traduire par \u00ab pays - grand - blanc - eleve \u00bb ( \u767d \u9ad8 \u5927 \u590f \u570b ) .                                                                                                                                                                            | 39                 |\n| \u0e27\u0e23\u0e23\u0e13\u0e1e\u0e07\u0e29\u0e4c\u0e40\u0e1b\u0e47\u0e19\u0e19\u0e31\u0e01\u0e28\u0e36\u0e01\u0e29\u0e32\u0e0a\u0e31\u0e49\u0e19\u0e1b\u0e35\u0e17\u0e35\u0e48\u0e2b\u0e19\u0e36\u0e48\u0e07 \u0e40\u0e23\u0e35\u0e22\u0e19\u0e2a\u0e32\u0e02\u0e32\u0e27\u0e34\u0e17\u0e22\u0e32\u0e01\u0e32\u0e23\u0e04\u0e2d\u0e21\u0e1e\u0e34\u0e27\u0e40\u0e15\u0e2d\u0e23\u0e4c\u0e41\u0e25\u0e30\u0e2a\u0e32\u0e23\u0e2a\u0e19\u0e40\u0e17\u0e28\u0e04\u0e13\u0e30\u0e27\u0e34\u0e17\u0e22\u0e32\u0e28\u0e32\u0e2a\u0e15\u0e23\u0e4c\u0e1b\u0e23\u0e30\u0e22\u0e38\u0e01\u0e15\u0e4c\u0e41\u0e25\u0e30\u0e27\u0e34\u0e28\u0e27\u0e01\u0e23\u0e23\u0e21\u0e28\u0e32\u0e2a\u0e15\u0e23\u0e4c\u0e2d\u0e22\u0e39\u0e48\u0e17\u0e35\u0e48\u0e21\u0e2b\u0e32\u0e27\u0e34\u0e17\u0e22\u0e32\u0e25\u0e31\u0e22\u0e02\u0e2d\u0e19\u0e41\u0e01\u0e48\u0e19\u0e27\u0e34\u0e17\u0e22\u0e32\u0e40\u0e02\u0e15\u0e2b\u0e19\u0e2d\u0e07\u0e04\u0e32\u0e22\u0e22\u0e37\u0e21\u0e04\u0e37\u0e19\u0e17\u0e23\u0e31\u0e1e\u0e22\u0e32\u0e01\u0e23\u0e2b\u0e49\u0e2d\u0e07\u0e2a\u0e21\u0e38\u0e14\u0e40\u0e2d\u0e01\u0e2a\u0e32\u0e23\u0e2a\u0e31\u0e21\u0e21\u0e19\u0e32\u0e04\u0e2d\u0e21\u0e1e\u0e34\u0e27\u0e40\u0e15\u0e2d\u0e23\u0e4c\u0e1b\u0e31\u0e0d\u0e0d\u0e32\u0e1b\u0e23\u0e30\u0e14\u0e34\u0e29\u0e10\u0e4c\u0e01\u0e31\u0e1a\u0e01\u0e32\u0e23\u0e1e\u0e31\u0e12\u0e19\u0e32\u0e40\u0e01\u0e21\u0e41\u0e21\u0e27\u0e01\u0e34\u0e19\u0e1b\u0e25\u0e32\u0e2b\u0e34\u0e27\u0e27\u0e27\u0e44\u0e2b\u0e21\u0e2b\u0e25\u0e31\u0e01\u0e2a\u0e39\u0e15\u0e23\u0e43\u0e2b\u0e21\u0e48\u0e2a\u0e14\u0e2a\u0e14\u0e17\u0e19\u0e44\u0e14\u0e49                                                                                           | \u0e27\u0e23\u0e23\u0e13\u0e1e\u0e07\u0e29 \u0e4c \u0e40\u0e1b \u0e47 \u0e19\u0e19 \u0e31 \u0e01\u0e28 \u0e36 \u0e01\u0e29\u0e32\u0e0a \u0e31 \u0e49 \u0e19\u0e1b \u0e35 \u0e17 \u0e35 \u0e48 \u0e2b\u0e19 \u0e36 \u0e48 \u0e07 \u0e40\u0e23 \u0e35 \u0e22\u0e19\u0e2a\u0e32\u0e02\u0e32\u0e27 \u0e34 \u0e17\u0e22\u0e32\u0e01\u0e32\u0e23\u0e04\u0e2d\u0e21\u0e1e \u0e34 \u0e27\u0e40\u0e15\u0e2d\u0e23 \u0e4c \u0e41\u0e25\u0e30\u0e2a\u0e32\u0e23\u0e2a\u0e19\u0e40\u0e17\u0e28\u0e04\u0e13\u0e30\u0e27 \u0e34 \u0e17\u0e22\u0e32\u0e28\u0e32\u0e2a\u0e15\u0e23 \u0e4c \u0e1b\u0e23\u0e30\u0e22 \u0e38 \u0e01\u0e15 \u0e4c \u0e41\u0e25\u0e30\u0e27 \u0e34 \u0e28\u0e27\u0e01\u0e23\u0e23\u0e21\u0e28\u0e32\u0e2a\u0e15\u0e23 \u0e4c \u0e2d\u0e22 \u0e39 \u0e48 \u0e17 \u0e35 \u0e48 \u0e21\u0e2b\u0e32\u0e27 \u0e34 \u0e17\u0e22\u0e32\u0e25 \u0e31 \u0e22\u0e02\u0e2d\u0e19\u0e41\u0e01 \u0e48 \u0e19\u0e27 \u0e34 \u0e17\u0e22\u0e32\u0e40\u0e02\u0e15\u0e2b\u0e19\u0e2d\u0e07\u0e04\u0e32\u0e22\u0e22 \u0e37 \u0e21\u0e04 \u0e37 \u0e19\u0e17\u0e23 \u0e31 \u0e1e\u0e22\u0e32\u0e01\u0e23\u0e2b \u0e49 \u0e2d\u0e07\u0e2a\u0e21 \u0e38 \u0e14\u0e40\u0e2d\u0e01\u0e2a\u0e32\u0e23\u0e2a \u0e31 \u0e21\u0e21\u0e19\u0e32\u0e04\u0e2d\u0e21\u0e1e \u0e34 \u0e27\u0e40\u0e15\u0e2d\u0e23 \u0e4c \u0e1b \u0e31 \u0e0d\u0e0d\u0e32\u0e1b\u0e23\u0e30\u0e14 \u0e34 \u0e29\u0e10 \u0e4c \u0e01 \u0e31 \u0e1a\u0e01\u0e32\u0e23\u0e1e \u0e31 \u0e12\u0e19\u0e32\u0e40\u0e01\u0e21\u0e41\u0e21\u0e27\u0e01 \u0e34 \u0e19\u0e1b\u0e25\u0e32\u0e2b \u0e34 \u0e27\u0e27\u0e27\u0e44\u0e2b\u0e21\u0e2b\u0e25 \u0e31 \u0e01\u0e2a \u0e39 \u0e15\u0e23\u0e43\u0e2b\u0e21 \u0e48 \u0e2a\u0e14\u0e2a\u0e14\u0e17\u0e19\u0e44\u0e14 \u0e49                                                                                                                                                                                | 92                    | \u0e27\u0e23\u0e23\u0e13\u0e1e\u0e07\u0e29\u0e40\u0e1b\u0e19\u0e19\u0e01\u0e28\u0e01\u0e29\u0e32\u0e0a\u0e19\u0e1b\u0e17\u0e2b\u0e19\u0e07 \u0e40\u0e23\u0e22\u0e19\u0e2a\u0e32\u0e02\u0e32\u0e27\u0e17\u0e22\u0e32\u0e01\u0e32\u0e23\u0e04\u0e2d\u0e21\u0e1e\u0e27\u0e40\u0e15\u0e2d\u0e23\u0e41\u0e25\u0e30\u0e2a\u0e32\u0e23\u0e2a\u0e19\u0e40\u0e17\u0e28\u0e04\u0e13\u0e30\u0e27\u0e17\u0e22\u0e32\u0e28\u0e32\u0e2a\u0e15\u0e23\u0e1b\u0e23\u0e30\u0e22\u0e01\u0e15\u0e41\u0e25\u0e30\u0e27\u0e28\u0e27\u0e01\u0e23\u0e23\u0e21\u0e28\u0e32\u0e2a\u0e15\u0e23\u0e2d\u0e22\u0e17\u0e21\u0e2b\u0e32\u0e27\u0e17\u0e22\u0e32\u0e25\u0e22\u0e02\u0e2d\u0e19\u0e41\u0e01\u0e19\u0e27\u0e17\u0e22\u0e32\u0e40\u0e02\u0e15\u0e2b\u0e19\u0e2d\u0e07\u0e04\u0e32\u0e22\u0e22\u0e21\u0e04\u0e19\u0e17\u0e23\u0e1e\u0e22\u0e32\u0e01\u0e23\u0e2b\u0e2d\u0e07\u0e2a\u0e21\u0e14\u0e40\u0e2d\u0e01\u0e2a\u0e32\u0e23\u0e2a\u0e21\u0e21\u0e19\u0e32\u0e04\u0e2d\u0e21\u0e1e\u0e27\u0e40\u0e15\u0e2d\u0e23\u0e1b\u0e0d\u0e0d\u0e32\u0e1b\u0e23\u0e30\u0e14\u0e29\u0e10\u0e01\u0e1a\u0e01\u0e32\u0e23\u0e1e\u0e12\u0e19\u0e32\u0e40\u0e01\u0e21\u0e41\u0e21\u0e27\u0e01\u0e19\u0e1b\u0e25\u0e32\u0e2b\u0e27\u0e27\u0e27\u0e44\u0e2b\u0e21\u0e2b\u0e25\u0e01\u0e2a\u0e15\u0e23\u0e43\u0e2b\u0e21\u0e2a\u0e14\u0e2a\u0e14\u0e17\u0e19\u0e44\u0e14                                                                                            | 2                  |\n| \u0eaa\u0ebb\u0ea1\u0ec0\u0e94\u0eb1\u0e94\u0e9e\u0eb0\u0ec0\u0e88\u0ebb\u0ec9\u0eb2\u0ea2\u0eb9\u0ec8\u0eab\u0ebb\u0ea7\u0e9a\u0ecd\u0ea3\u0ebb\u0ea1\u0ec2\u0e81\u0e94\u0e8a\u0ebb\u0e87\u0e97\u0eb3\u0e99\u0eb8\u0e9a\u0eb3\u0ea5\u0eb8\u0e87\u0e9a\u0ec9\u0eb2\u0e99\u0ec0\u0ea1\u0eb7\u0ead\u0e87\u0ec1\u0ea5\u0eb0\u0e9e\u0eb0\u0eaa\u0eb2\u0e94\u0eaa\u0eb0\u0edc\u0eb2\u0e88\u0ebb\u0e99\u0e81\u0ec8\u0eb2\u0ea7\u0ec4\u0e94\u0ec9\u0ea7\u0ec8\u0eb2\u0e81\u0eb8\u0e87\u0eaa\u0eb5\u0ead\u0eb0\u0e8d\u0eb8\u0e97\u0eb0\u0ea2\u0eb2\u0ec3\u0e99\u0eaa\u0eb0\u0ec4\u0edd\u0e9e\u0eb0\u0ead\u0ebb\u0e87\u0e99\u0eb1\u0ec9\u0e99\u0ec0\u0e9b\u0eb1\u0e99\u0e8d\u0eb8\u0e81\u0e97\u0eb5\u0ec8\u0e9a\u0ec9\u0eb2\u0e99\u0ec0\u0ea1\u0eb7\u0ead\u0e87\u0e94\u0eb5 \u0ea1\u0eb5\u0e82\u0eb8\u0e99\u0e99\u0eb2\u0e87\u0e84\u0ebb\u0e99\u0eaa\u0eb3\u0e84\u0eb1\u0e99\u0e97\u0eb5\u0ec8\u0ec0\u0e95\u0eb5\u0e9a\u0ec2\u0e95\u0ec3\u0e99\u0ec0\u0ea7\u0ea5\u0eb2\u0e95\u0ecd\u0ec8\u0ea1\u0eb2 \u0ec3\u0e99\u0ea5\u0eb2\u0e8a\u0eb0\u0e81\u0eb2\u0e99\u0e82\u0ead\u0e87\u0e9e\u0eb0\u0ead\u0ebb\u0e87\u0eab\u0ebc\u0eb2\u0e8d\u0e84\u0ebb\u0e99 \u0ec0\u0e8a\u0eb1\u0ec8\u0e99 \u0eaa\u0ebb\u0ea1\u0ec0\u0e94\u0eb1\u0e94\u0e9e\u0eb0\u0ec0\u0e88\u0ebb\u0ec9\u0eb2\u0e81\u0eb8\u0e87\u0e97\u0ebb\u0e99\u0e9a\u0eb8\u0ea5\u0eb5, \u0e9e\u0eb0\u0e9a\u0eb2\u0e94\u0eaa\u0ebb\u0ea1\u0ec0\u0e94\u0eb1\u0e94\u0e9e\u0eb0\u0e9e\u0eb8\u0e94\u0e97\u0eb0\u0e8d\u0ead\u0e94\u0e9f\u0ec9\u0eb2\u0e88\u0eb8\u0ea5\u0eb2\u0ec2\u0ea5\u0e81\u0ea1\u0eb0\u0eab\u0eb2\u0ea5\u0eb2\u0e94 \u0ec0\u0e9b\u0eb1\u0e99\u0e95\u0ebb\u0ec9\u0e99 \u0ec3\u0e99\u0e97\u0eb2\u0e87\u0e94\u0ec9\u0eb2\u0e99\u0ea7\u0eb1\u0e99\u0e99\u0eb0\u0e84\u0eb0\u0e94\u0eb5\u0e81\u0ecd\u0ea1\u0eb5\u0e81\u0eb0\u0ea7\u0eb5\u0e84\u0ebb\u0e99\u0eaa\u0eb3\u0e84\u0eb1\u0e99 \u0ec0\u0e8a\u0eb1\u0ec8\u0e99 \u0ec0\u0e88\u0ebb\u0ec9\u0eb2\u0e9f\u0ec9\u0eb2\u0e97\u0eb3\u0ea1\u0eb2\u0e97\u0eb4\u0ec0\u0e9a\u0e94\u0ec4\u0e8a\u0e8d\u0eb0\u0ec0\u0e8a\u0e94\u0eaa\u0eb8\u0ea5\u0eb4\u0e8d\u0eb0\u0ea7\u0ebb\u0e87 \u0e81\u0ebb\u0ea1\u0ea1\u0eb0\u0e82\u0eb8\u0e99\u0ec0\u0eaa\u0e99\u0eb2\u0e9e\u0eb4\u0e97\u0eb1\u0e81 \u0eab\u0ebc\u0eb7\u0ec0\u0e88\u0ebb\u0ec9\u0eb2\u0e9f\u0ec9\u0eb2\u0e81\u0eb8\u0ec9\u0e87 \u0ec0\u0e8a\u0eb4\u0ec8\u0e87\u0ec0\u0e9b\u0eb1\u0e99\u0e9e\u0eb0\u0ec2\u0ead\u0ea5\u0ebb\u0e94 \u0ec0\u0e9b\u0eb1\u0e99\u0e95\u0ebb\u0ec9\u0e99 | \u0eaa \u0ebb \u0ea1\u0ec0\u0e94 \u0eb1 \u0e94\u0e9e\u0eb0\u0ec0\u0e88 \u0ebb \u0ec9 \u0eb2\u0ea2 \u0eb9 \u0ec8 \u0eab \u0ebb \u0ea7\u0e9a \u0ecd \u0ea3 \u0ebb \u0ea1\u0ec2\u0e81\u0e94\u0e8a \u0ebb \u0e87\u0e97\u0eb3\u0e99 \u0eb8 \u0e9a\u0eb3\u0ea5 \u0eb8 \u0e87\u0e9a \u0ec9 \u0eb2\u0e99\u0ec0\u0ea1 \u0eb7 \u0ead\u0e87\u0ec1\u0ea5\u0eb0\u0e9e\u0eb0\u0eaa\u0eb2\u0e94\u0eaa\u0eb0\u0edc\u0eb2\u0e88 \u0ebb \u0e99\u0e81 \u0ec8 \u0eb2\u0ea7\u0ec4\u0e94 \u0ec9 \u0ea7 \u0ec8 \u0eb2\u0e81 \u0eb8 \u0e87\u0eaa \u0eb5 \u0ead\u0eb0\u0e8d \u0eb8 \u0e97\u0eb0\u0ea2\u0eb2\u0ec3\u0e99\u0eaa\u0eb0\u0ec4\u0edd\u0e9e\u0eb0\u0ead \u0ebb \u0e87\u0e99 \u0eb1 \u0ec9 \u0e99\u0ec0\u0e9b \u0eb1 \u0e99\u0e8d \u0eb8 \u0e81\u0e97 \u0eb5 \u0ec8 \u0e9a \u0ec9 \u0eb2\u0e99\u0ec0\u0ea1 \u0eb7 \u0ead\u0e87\u0e94 \u0eb5 \u0ea1 \u0eb5 \u0e82 \u0eb8 \u0e99\u0e99\u0eb2\u0e87\u0e84 \u0ebb \u0e99\u0eaa\u0eb3\u0e84 \u0eb1 \u0e99\u0e97 \u0eb5 \u0ec8 \u0ec0\u0e95 \u0eb5 \u0e9a\u0ec2\u0e95\u0ec3\u0e99\u0ec0\u0ea7\u0ea5\u0eb2\u0e95 \u0ecd \u0ec8 \u0ea1\u0eb2 \u0ec3\u0e99\u0ea5\u0eb2\u0e8a\u0eb0\u0e81\u0eb2\u0e99\u0e82\u0ead\u0e87\u0e9e\u0eb0\u0ead \u0ebb \u0e87\u0eab \u0ebc \u0eb2\u0e8d\u0e84 \u0ebb \u0e99 \u0ec0\u0e8a \u0eb1 \u0ec8 \u0e99 \u0eaa \u0ebb \u0ea1\u0ec0\u0e94 \u0eb1 \u0e94\u0e9e\u0eb0\u0ec0\u0e88 \u0ebb \u0ec9 \u0eb2\u0e81 \u0eb8 \u0e87\u0e97 \u0ebb \u0e99\u0e9a \u0eb8 \u0ea5 \u0eb5 , \u0e9e\u0eb0\u0e9a\u0eb2\u0e94\u0eaa \u0ebb \u0ea1\u0ec0\u0e94 \u0eb1 \u0e94\u0e9e\u0eb0\u0e9e \u0eb8 \u0e94\u0e97\u0eb0\u0e8d\u0ead\u0e94\u0e9f \u0ec9 \u0eb2\u0e88 \u0eb8 \u0ea5\u0eb2\u0ec2\u0ea5\u0e81\u0ea1\u0eb0\u0eab\u0eb2\u0ea5\u0eb2\u0e94 \u0ec0\u0e9b \u0eb1 \u0e99\u0e95 \u0ebb \u0ec9 \u0e99 \u0ec3\u0e99\u0e97\u0eb2\u0e87\u0e94 \u0ec9 \u0eb2\u0e99\u0ea7 \u0eb1 \u0e99\u0e99\u0eb0\u0e84\u0eb0\u0e94 \u0eb5 \u0e81 \u0ecd \u0ea1 \u0eb5 \u0e81\u0eb0\u0ea7 \u0eb5 \u0e84 \u0ebb \u0e99\u0eaa\u0eb3\u0e84 \u0eb1 \u0e99 \u0ec0\u0e8a \u0eb1 \u0ec8 \u0e99 \u0ec0\u0e88 \u0ebb \u0ec9 \u0eb2\u0e9f \u0ec9 \u0eb2\u0e97\u0eb3\u0ea1\u0eb2\u0e97 \u0eb4 \u0ec0\u0e9a\u0e94\u0ec4\u0e8a\u0e8d\u0eb0\u0ec0\u0e8a\u0e94\u0eaa \u0eb8 \u0ea5 \u0eb4 \u0e8d\u0eb0\u0ea7 \u0ebb \u0e87 \u0e81 \u0ebb \u0ea1\u0ea1\u0eb0\u0e82 \u0eb8 \u0e99\u0ec0\u0eaa\u0e99\u0eb2\u0e9e \u0eb4 \u0e97 \u0eb1 \u0e81 \u0eab \u0ebc \u0eb7 \u0ec0\u0e88 \u0ebb \u0ec9 \u0eb2\u0e9f \u0ec9 \u0eb2\u0e81 \u0eb8 \u0ec9 \u0e87 \u0ec0\u0e8a \u0eb4 \u0ec8 \u0e87\u0ec0\u0e9b \u0eb1 \u0e99\u0e9e\u0eb0\u0ec2\u0ead\u0ea5 \u0ebb \u0e94 \u0ec0\u0e9b \u0eb1 \u0e99\u0e95 \u0ebb \u0ec9 \u0e99 | 189                   | \u0eaa\u0ea1\u0ec0\u0e94\u0e94\u0e9e\u0eb0\u0ec0\u0e88\u0eb2\u0ea2\u0eab\u0ea7\u0e9a\u0ea3\u0ea1\u0ec2\u0e81\u0e94\u0e8a\u0e87\u0e97\u0eb3\u0e99\u0e9a\u0eb3\u0ea5\u0e87\u0e9a\u0eb2\u0e99\u0ec0\u0ea1\u0ead\u0e87\u0ec1\u0ea5\u0eb0\u0e9e\u0eb0\u0eaa\u0eb2\u0e94\u0eaa\u0eb0\u0edc\u0eb2\u0e88\u0e99\u0e81\u0eb2\u0ea7\u0ec4\u0e94\u0ea7\u0eb2\u0e81\u0e87\u0eaa\u0ead\u0eb0\u0e8d\u0e97\u0eb0\u0ea2\u0eb2\u0ec3\u0e99\u0eaa\u0eb0\u0ec4\u0edd\u0e9e\u0eb0\u0ead\u0e87\u0e99\u0e99\u0ec0\u0e9b\u0e99\u0e8d\u0e81\u0e97\u0e9a\u0eb2\u0e99\u0ec0\u0ea1\u0ead\u0e87\u0e94 \u0ea1\u0e82\u0e99\u0e99\u0eb2\u0e87\u0e84\u0e99\u0eaa\u0eb3\u0e84\u0e99\u0e97\u0ec0\u0e95\u0e9a\u0ec2\u0e95\u0ec3\u0e99\u0ec0\u0ea7\u0ea5\u0eb2\u0e95\u0ea1\u0eb2 \u0ec3\u0e99\u0ea5\u0eb2\u0e8a\u0eb0\u0e81\u0eb2\u0e99\u0e82\u0ead\u0e87\u0e9e\u0eb0\u0ead\u0e87\u0eab\u0eb2\u0e8d\u0e84\u0e99 \u0ec0\u0e8a\u0e99 \u0eaa\u0ea1\u0ec0\u0e94\u0e94\u0e9e\u0eb0\u0ec0\u0e88\u0eb2\u0e81\u0e87\u0e97\u0e99\u0e9a\u0ea5 , \u0e9e\u0eb0\u0e9a\u0eb2\u0e94\u0eaa\u0ea1\u0ec0\u0e94\u0e94\u0e9e\u0eb0\u0e9e\u0e94\u0e97\u0eb0\u0e8d\u0ead\u0e94\u0e9f\u0eb2\u0e88\u0ea5\u0eb2\u0ec2\u0ea5\u0e81\u0ea1\u0eb0\u0eab\u0eb2\u0ea5\u0eb2\u0e94 \u0ec0\u0e9b\u0e99\u0e95\u0e99 \u0ec3\u0e99\u0e97\u0eb2\u0e87\u0e94\u0eb2\u0e99\u0ea7\u0e99\u0e99\u0eb0\u0e84\u0eb0\u0e94\u0e81\u0ea1\u0e81\u0eb0\u0ea7\u0e84\u0e99\u0eaa\u0eb3\u0e84\u0e99 \u0ec0\u0e8a\u0e99 \u0ec0\u0e88\u0eb2\u0e9f\u0eb2\u0e97\u0eb3\u0ea1\u0eb2\u0e97\u0ec0\u0e9a\u0e94\u0ec4\u0e8a\u0e8d\u0eb0\u0ec0\u0e8a\u0e94\u0eaa\u0ea5\u0e8d\u0eb0\u0ea7\u0e87 \u0e81\u0ea1\u0ea1\u0eb0\u0e82\u0e99\u0ec0\u0eaa\u0e99\u0eb2\u0e9e\u0e97\u0e81 \u0eab\u0ec0\u0e88\u0eb2\u0e9f\u0eb2\u0e81\u0e87 \u0ec0\u0e8a\u0e87\u0ec0\u0e9b\u0e99\u0e9e\u0eb0\u0ec2\u0ead\u0ea5\u0e94 \u0ec0\u0e9b\u0e99\u0e95\u0e99 | 15                 |\n\n## reference\n* Unicode Blocks  https://www.unicode.org/Public/UCD/latest/ucd/Blocks.txt\n* unicodedata.category https://www.unicode.org/reports/tr44/  #Table 12. General_Category Values\n* \u6c49\u5b57\u533a\u95f4 http://yedict.com/zsts.htm\n\n\n## License\n[Anti-996 License](https://github.com/996icu/996.ICU/blob/master/LICENSE)\n\n\n",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/laohur/UnicodeTokenizer",
            "keywords": "UnicodeTokenizer,Tokenizer,Unicode,ZiTokenizer,ZiCutter,laohur",
            "license": "[Anti-996 License](https: // github.com/996icu/996.ICU/blob/master/LICENSE)",
            "maintainer": "",
            "maintainer_email": "",
            "name": "UnicodeTokenizer",
            "package_url": "https://pypi.org/project/UnicodeTokenizer/",
            "platform": null,
            "project_url": "https://pypi.org/project/UnicodeTokenizer/",
            "project_urls": {
                "Homepage": "https://github.com/laohur/UnicodeTokenizer"
            },
            "release_url": "https://pypi.org/project/UnicodeTokenizer/0.1.0/",
            "requires_dist": null,
            "requires_python": ">=3.0",
            "summary": "UnicodeTokenizer: tokenize all Unicode text",
            "version": "0.1.0",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16269973,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "4dced36859a8fcf9a9d072db0a9d9234",
                    "sha256": "23f333c8f5837bc415717069823e10bd982035171138d232f59a5b84d8e4e9a1"
                },
                "downloads": -1,
                "filename": "UnicodeTokenizer-0.1.0-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "4dced36859a8fcf9a9d072db0a9d9234",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.0",
                "size": 5950,
                "upload_time": "2023-01-01T02:26:50",
                "upload_time_iso_8601": "2023-01-01T02:26:50.439418Z",
                "url": "https://files.pythonhosted.org/packages/a3/6c/b2e7d0c2925061a3bc53fdb632a6b10dce30a1b220af942d2d502449204f/UnicodeTokenizer-0.1.0-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "8a41a3af17b4dbc6d81578ed8239d4ef",
                    "sha256": "734b7a0f82f017b725686d880542bb5fbe86cc234d2e52982d4edda7871528ef"
                },
                "downloads": -1,
                "filename": "UnicodeTokenizer-0.1.0.tar.gz",
                "has_sig": false,
                "md5_digest": "8a41a3af17b4dbc6d81578ed8239d4ef",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.0",
                "size": 5908,
                "upload_time": "2023-01-01T02:26:52",
                "upload_time_iso_8601": "2023-01-01T02:26:52.467733Z",
                "url": "https://files.pythonhosted.org/packages/b8/3a/6275eb13cbc58ce7e610406da706f1697a8f9bcb127ede8c2b3dc6b40252/UnicodeTokenizer-0.1.0.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}