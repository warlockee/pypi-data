{
    "0.7.0": {
        "info": {
            "author": "Eren G\u00f6lge",
            "author_email": "egolge@coqui.ai",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 3 - Alpha",
                "Intended Audience :: Developers",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: Mozilla Public License 2.0 (MPL 2.0)",
                "Operating System :: POSIX :: Linux",
                "Programming Language :: Python",
                "Programming Language :: Python :: 3",
                "Programming Language :: Python :: 3.10",
                "Programming Language :: Python :: 3.7",
                "Programming Language :: Python :: 3.8",
                "Programming Language :: Python :: 3.9",
                "Topic :: Multimedia",
                "Topic :: Multimedia :: Sound/Audio",
                "Topic :: Multimedia :: Sound/Audio :: Speech",
                "Topic :: Scientific/Engineering :: Artificial Intelligence",
                "Topic :: Software Development",
                "Topic :: Software Development :: Libraries :: Python Modules"
            ],
            "description": "# <img src=\"https://raw.githubusercontent.com/coqui-ai/TTS/main/images/coqui-log-green-TTS.png\" height=\"56\"/>\n\n\ud83d\udc38TTS is a library for advanced Text-to-Speech generation. It's built on the latest research, was designed to achieve the best trade-off among ease-of-training, speed and quality.\n\ud83d\udc38TTS comes with pretrained models, tools for measuring dataset quality and already used in **20+ languages** for products and research projects.\n\n[![Gitter](https://badges.gitter.im/coqui-ai/TTS.svg)](https://gitter.im/coqui-ai/TTS?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)\n[![License](<https://img.shields.io/badge/License-MPL%202.0-brightgreen.svg>)](https://opensource.org/licenses/MPL-2.0)\n[![PyPI version](https://badge.fury.io/py/TTS.svg)](https://badge.fury.io/py/TTS)\n[![Covenant](https://camo.githubusercontent.com/7d620efaa3eac1c5b060ece5d6aacfcc8b81a74a04d05cd0398689c01c4463bb/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6e7472696275746f72253230436f76656e616e742d76322e3025323061646f707465642d6666363962342e737667)](https://github.com/coqui-ai/TTS/blob/master/CODE_OF_CONDUCT.md)\n[![Downloads](https://pepy.tech/badge/tts)](https://pepy.tech/project/tts)\n[![DOI](https://zenodo.org/badge/265612440.svg)](https://zenodo.org/badge/latestdoi/265612440)\n\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/aux_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/data_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/docker.yaml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/inference_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/style_check.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/text_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/tts_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/vocoder_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/zoo_tests.yml/badge.svg)\n[![Docs](<https://readthedocs.org/projects/tts/badge/?version=latest&style=plastic>)](https://tts.readthedocs.io/en/latest/)\n\n\ud83d\udcf0 [**Subscribe to \ud83d\udc38Coqui.ai Newsletter**](https://coqui.ai/?subscription=true)\n\n\ud83d\udce2 [English Voice Samples](https://erogol.github.io/ddc-samples/) and [SoundCloud playlist](https://soundcloud.com/user-565970875/pocket-article-wavernn-and-tacotron2)\n\n\ud83d\udcc4 [Text-to-Speech paper collection](https://github.com/erogol/TTS-papers)\n\n<img src=\"https://static.scarf.sh/a.png?x-pxid=cf317fe7-2188-4721-bc01-124bb5d5dbb2\" />\n\n## \ud83d\udcac Where to ask questions\nPlease use our dedicated channels for questions and discussion. Help is much more valuable if it's shared publicly so that more people can benefit from it.\n\n| Type                            | Platforms                               |\n| ------------------------------- | --------------------------------------- |\n| \ud83d\udea8 **Bug Reports**              | [GitHub Issue Tracker]                  |\n| \ud83c\udf81 **Feature Requests & Ideas** | [GitHub Issue Tracker]                  |\n| \ud83d\udc69\u200d\ud83d\udcbb **Usage Questions**          | [Github Discussions]                    |\n| \ud83d\uddef **General Discussion**       | [Github Discussions] or [Gitter Room]   |\n\n[github issue tracker]: https://github.com/coqui-ai/tts/issues\n[github discussions]: https://github.com/coqui-ai/TTS/discussions\n[gitter room]: https://gitter.im/coqui-ai/TTS?utm_source=share-link&utm_medium=link&utm_campaign=share-link\n[Tutorials and Examples]: https://github.com/coqui-ai/TTS/wiki/TTS-Notebooks-and-Tutorials\n\n\n## \ud83d\udd17 Links and Resources\n| Type                            | Links                               |\n| ------------------------------- | --------------------------------------- |\n| \ud83d\udcbc **Documentation**              | [ReadTheDocs](https://tts.readthedocs.io/en/latest/)\n| \ud83d\udcbe **Installation**               | [TTS/README.md](https://github.com/coqui-ai/TTS/tree/dev#install-tts)|\n| \ud83d\udc69\u200d\ud83d\udcbb **Contributing**               | [CONTRIBUTING.md](https://github.com/coqui-ai/TTS/blob/main/CONTRIBUTING.md)|\n| \ud83d\udccc **Road Map**                   | [Main Development Plans](https://github.com/coqui-ai/TTS/issues/378)\n| \ud83d\ude80 **Released Models**            | [TTS Releases](https://github.com/coqui-ai/TTS/releases) and [Experimental Models](https://github.com/coqui-ai/TTS/wiki/Experimental-Released-Models)|\n\n## \ud83e\udd47 TTS Performance\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/coqui-ai/TTS/main/images/TTS-performance.png\" width=\"800\" /></p>\n\nUnderlined \"TTS*\" and \"Judy*\" are \ud83d\udc38TTS models\n<!-- [Details...](https://github.com/coqui-ai/TTS/wiki/Mean-Opinion-Score-Results) -->\n\n## Features\n- High-performance Deep Learning models for Text2Speech tasks.\n    - Text2Spec models (Tacotron, Tacotron2, Glow-TTS, SpeedySpeech).\n    - Speaker Encoder to compute speaker embeddings efficiently.\n    - Vocoder models (MelGAN, Multiband-MelGAN, GAN-TTS, ParallelWaveGAN, WaveGrad, WaveRNN)\n- Fast and efficient model training.\n- Detailed training logs on the terminal and Tensorboard.\n- Support for Multi-speaker TTS.\n- Efficient, flexible, lightweight but feature complete `Trainer API`.\n- Released and ready-to-use models.\n- Tools to curate Text2Speech datasets under```dataset_analysis```.\n- Utilities to use and test your models.\n- Modular (but not too much) code base enabling easy implementation of new ideas.\n\n## Implemented Models\n### Text-to-Spectrogram\n- Tacotron: [paper](https://arxiv.org/abs/1703.10135)\n- Tacotron2: [paper](https://arxiv.org/abs/1712.05884)\n- Glow-TTS: [paper](https://arxiv.org/abs/2005.11129)\n- Speedy-Speech: [paper](https://arxiv.org/abs/2008.03802)\n- Align-TTS: [paper](https://arxiv.org/abs/2003.01950)\n- FastPitch: [paper](https://arxiv.org/pdf/2006.06873.pdf)\n- FastSpeech: [paper](https://arxiv.org/abs/1905.09263)\n\n### End-to-End Models\n- VITS: [paper](https://arxiv.org/pdf/2106.06103)\n\n### Attention Methods\n- Guided Attention: [paper](https://arxiv.org/abs/1710.08969)\n- Forward Backward Decoding: [paper](https://arxiv.org/abs/1907.09006)\n- Graves Attention: [paper](https://arxiv.org/abs/1910.10288)\n- Double Decoder Consistency: [blog](https://erogol.com/solving-attention-problems-of-tts-models-with-double-decoder-consistency/)\n- Dynamic Convolutional Attention: [paper](https://arxiv.org/pdf/1910.10288.pdf)\n- Alignment Network: [paper](https://arxiv.org/abs/2108.10447)\n\n### Speaker Encoder\n- GE2E: [paper](https://arxiv.org/abs/1710.10467)\n- Angular Loss: [paper](https://arxiv.org/pdf/2003.11982.pdf)\n\n### Vocoders\n- MelGAN: [paper](https://arxiv.org/abs/1910.06711)\n- MultiBandMelGAN: [paper](https://arxiv.org/abs/2005.05106)\n- ParallelWaveGAN: [paper](https://arxiv.org/abs/1910.11480)\n- GAN-TTS discriminators: [paper](https://arxiv.org/abs/1909.11646)\n- WaveRNN: [origin](https://github.com/fatchord/WaveRNN/)\n- WaveGrad: [paper](https://arxiv.org/abs/2009.00713)\n- HiFiGAN: [paper](https://arxiv.org/abs/2010.05646)\n- UnivNet: [paper](https://arxiv.org/abs/2106.07889)\n\nYou can also help us implement more models.\n\n## Install TTS\n\ud83d\udc38TTS is tested on Ubuntu 18.04 with **python >= 3.7, < 3.11.**.\n\nIf you are only interested in [synthesizing speech](https://tts.readthedocs.io/en/latest/inference.html) with the released \ud83d\udc38TTS models, installing from PyPI is the easiest option.\n\n```bash\npip install TTS\n```\n\nIf you plan to code or train models, clone \ud83d\udc38TTS and install it locally.\n\n```bash\ngit clone https://github.com/coqui-ai/TTS\npip install -e .[all,dev,notebooks]  # Select the relevant extras\n```\n\nIf you are on Ubuntu (Debian), you can also run following commands for installation.\n\n```bash\n$ make system-deps  # intended to be used on Ubuntu (Debian). Let us know if you have a diffent OS.\n$ make install\n```\n\nIf you are on Windows, \ud83d\udc51@GuyPaddock wrote installation instructions [here](https://stackoverflow.com/questions/66726331/how-can-i-run-mozilla-tts-coqui-tts-training-with-cuda-on-a-windows-system).\n\n## Use TTS\n\n### Single Speaker Models\n\n- List provided models:\n\n    ```\n    $ tts --list_models\n    ```\n\n- Run TTS with default models:\n\n    ```\n    $ tts --text \"Text for TTS\"\n    ```\n\n- Run a TTS model with its default vocoder model:\n\n    ```\n    $ tts --text \"Text for TTS\" --model_name \"<language>/<dataset>/<model_name>\n    ```\n\n- Run with specific TTS and vocoder models from the list:\n\n    ```\n    $ tts --text \"Text for TTS\" --model_name \"<language>/<dataset>/<model_name>\" --vocoder_name \"<language>/<dataset>/<model_name>\" --output_path\n    ```\n\n- Run your own TTS model (Using Griffin-Lim Vocoder):\n\n    ```\n    $ tts --text \"Text for TTS\" --model_path path/to/model.pth --config_path path/to/config.json --out_path output/path/speech.wav\n    ```\n\n- Run your own TTS and Vocoder models:\n    ```\n    $ tts --text \"Text for TTS\" --model_path path/to/config.json --config_path path/to/model.pth --out_path output/path/speech.wav\n        --vocoder_path path/to/vocoder.pth --vocoder_config_path path/to/vocoder_config.json\n    ```\n\n### Multi-speaker Models\n\n- List the available speakers and choose as <speaker_id> among them:\n\n    ```\n    $ tts --model_name \"<language>/<dataset>/<model_name>\"  --list_speaker_idxs\n    ```\n\n- Run the multi-speaker TTS model with the target speaker ID:\n\n    ```\n    $ tts --text \"Text for TTS.\" --out_path output/path/speech.wav --model_name \"<language>/<dataset>/<model_name>\"  --speaker_idx <speaker_id>\n    ```\n\n- Run your own multi-speaker TTS model:\n\n    ```\n    $ tts --text \"Text for TTS\" --out_path output/path/speech.wav --model_path path/to/config.json --config_path path/to/model.pth --speakers_file_path path/to/speaker.json --speaker_idx <speaker_id>\n    ```\n\n## Directory Structure\n```\n|- notebooks/       (Jupyter Notebooks for model evaluation, parameter selection and data analysis.)\n|- utils/           (common utilities.)\n|- TTS\n    |- bin/             (folder for all the executables.)\n      |- train*.py                  (train your target model.)\n      |- distribute.py              (train your TTS model using Multiple GPUs.)\n      |- compute_statistics.py      (compute dataset statistics for normalization.)\n      |- ...\n    |- tts/             (text to speech models)\n        |- layers/          (model layer definitions)\n        |- models/          (model definitions)\n        |- utils/           (model specific utilities.)\n    |- speaker_encoder/ (Speaker Encoder models.)\n        |- (same)\n    |- vocoder/         (Vocoder models.)\n        |- (same)\n```\n\n\n",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/coqui-ai/TTS",
            "keywords": "",
            "license": "MPL-2.0",
            "maintainer": "",
            "maintainer_email": "",
            "name": "TTS2",
            "package_url": "https://pypi.org/project/TTS2/",
            "platform": null,
            "project_url": "https://pypi.org/project/TTS2/",
            "project_urls": {
                "Discussions": "https://github.com/coqui-ai/TTS/discussions",
                "Documentation": "https://github.com/coqui-ai/TTS/wiki",
                "Homepage": "https://github.com/coqui-ai/TTS",
                "Repository": "https://github.com/coqui-ai/TTS",
                "Tracker": "https://github.com/coqui-ai/TTS/issues"
            },
            "release_url": "https://pypi.org/project/TTS2/0.7.0/",
            "requires_dist": [
                "bokeh (==1.4.0) ; extra == 'notebooks'",
                "pylint (==2.10.2) ; extra == 'dev'",
                "nose2 ; extra == 'dev'",
                "isort ; extra == 'dev'",
                "coverage ; extra == 'dev'",
                "black ; extra == 'dev'",
                "bokeh (==1.4.0) ; extra == 'all'",
                "pylint (==2.10.2) ; extra == 'all'",
                "nose2 ; extra == 'all'",
                "isort ; extra == 'all'",
                "coverage ; extra == 'all'",
                "black ; extra == 'all'",
                "gruut[cs,de,es,fr,it,nl,pt,ru,sv] (==2.2.3)",
                "unidic-lite (==1.0.8)",
                "mecab-python3 (==1.0.5)",
                "pypinyin",
                "jieba",
                "coqpit (>=0.0.16)",
                "trainer",
                "pyworld (==0.2.10)",
                "matplotlib",
                "pandas",
                "umap-learn (==0.5.1)",
                "pysbd",
                "flask",
                "fsspec (>=2021.04.0)",
                "pyyaml",
                "anyascii",
                "tqdm",
                "inflect",
                "numba (==0.53.1)",
                "librosa (==0.8.0)",
                "soundfile",
                "torchaudio",
                "torch (>=1.7)",
                "scipy (>=1.4.0)",
                "cython (==0.29.28)",
                "numpy"
            ],
            "requires_python": ">=3.7.0, <3.11",
            "summary": "Deep learning for Text to Speech by Coqui.",
            "version": "0.7.0",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 14525232,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "02c910f0630dea22777ad8fb4d412eb8",
                    "sha256": "9ffe461b7e91b90cb108bd41e3d0fd66928b271624c179447930d3c56df84ad3"
                },
                "downloads": -1,
                "filename": "TTS2-0.7.0-cp39-cp39-manylinux1_x86_64.whl",
                "has_sig": false,
                "md5_digest": "02c910f0630dea22777ad8fb4d412eb8",
                "packagetype": "bdist_wheel",
                "python_version": "cp39",
                "requires_python": ">=3.7.0, <3.11",
                "size": 546454,
                "upload_time": "2022-07-23T16:06:05",
                "upload_time_iso_8601": "2022-07-23T16:06:05.142273Z",
                "url": "https://files.pythonhosted.org/packages/7c/ff/2fca83e1034cb44b0e6d05f6205408750b51c25787f88723a5764477db6f/TTS2-0.7.0-cp39-cp39-manylinux1_x86_64.whl",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}