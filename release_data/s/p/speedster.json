{
    "0.0.1": {
        "info": {
            "author": "",
            "author_email": "",
            "bugtrack_url": null,
            "classifiers": [],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "",
            "keywords": "",
            "license": "",
            "maintainer": "",
            "maintainer_email": "",
            "name": "speedster",
            "package_url": "https://pypi.org/project/speedster/",
            "platform": null,
            "project_url": "https://pypi.org/project/speedster/",
            "project_urls": null,
            "release_url": "https://pypi.org/project/speedster/0.0.1/",
            "requires_dist": [
                "nebullvm (>=0.6)"
            ],
            "requires_python": "",
            "summary": "",
            "version": "0.0.1",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16274619,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "cd4a46c574f73684c6825c5bd3d7c8f2",
                    "sha256": "1459385cac32ad35b17e993d4396d85f6042b9574ad9a6632628708a079b66cd"
                },
                "downloads": -1,
                "filename": "speedster-0.0.1-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "cd4a46c574f73684c6825c5bd3d7c8f2",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": null,
                "size": 16905,
                "upload_time": "2022-12-16T18:52:33",
                "upload_time_iso_8601": "2022-12-16T18:52:33.133412Z",
                "url": "https://files.pythonhosted.org/packages/8f/c4/2ef970d86d7a32b114158c63bdbf97bb29b36bcca50f704eb66033165102/speedster-0.0.1-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "4a04caf9aea66f169474a6fa32c73f7f",
                    "sha256": "ac29cd6cff891c397b6fd3e0c0dcbd4ad433e5ef892cf54fd36735ca45cea33c"
                },
                "downloads": -1,
                "filename": "speedster-0.0.1.tar.gz",
                "has_sig": false,
                "md5_digest": "4a04caf9aea66f169474a6fa32c73f7f",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 16571,
                "upload_time": "2022-12-16T18:52:34",
                "upload_time_iso_8601": "2022-12-16T18:52:34.768474Z",
                "url": "https://files.pythonhosted.org/packages/26/8b/391dea54bd77df107a65a8ddfe1a00f7a31866a9e3284e7ce4f2ac3c1c33/speedster-0.0.1.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "0.0.2": {
        "info": {
            "author": "",
            "author_email": "",
            "bugtrack_url": null,
            "classifiers": [],
            "description": "# \ud83d\udca5 Speedster App\n\nAutomatically apply SOTA optimization techniques to achieve the maximum inference speed-up on your hardware.\n\n\n## \ud83d\udcd6 Description \n`Speedster`\u00a0is an open-source App designed to speed up AI inference in just a few lines of code. The library boosts your model to achieve the maximum acceleration that is physically possible on your hardware.\n\nWe are building a new AI inference acceleration product leveraging state-of-the-art open-source optimization tools enabling the optimization of the whole software to hardware stack. If you like the idea, give us a star to support the project\u00a0\u2b50\n\n\n![nebullvm benchmarks](https://user-images.githubusercontent.com/28647171/201156822-00a307d7-e3b9-4121-aa83-c7d2a167f791.png)\n\n\n\nThe core\u00a0`Speedster`\u00a0workflow consists of 3 steps:\n\n- [x]  **Select**: input your model in your preferred DL framework and express your preferences regarding:\n    - Accuracy loss: do you want to trade off a little accuracy for much higher performance?\n    - Optimization time: stellar accelerations can be time-consuming. Can you wait, or do you need an instant answer?\n- [x]  **Search**: the App automatically tests every combination of optimization techniques across the software-to-hardware stack (sparsity, quantization, compilers, etc.) that is compatible with your needs and local hardware.\n- [x]  **Serve**: finally, `Speedster` chooses the best configuration of optimization techniques and returns an accelerated version of your model in the DL framework of your choice (just on steroids\u00a0\ud83d\ude80).\n\n\n# Installation\n\n> :warning: For **MacOS** with **ARM processors**, please use a conda environment.\n\nInstall Speedster and its base requirements:\n```\npip install speedster\n```\n\n> :warning: If you want to optimize a **PyTorch model**, PyTorch must be pre-installed \n> on your environment before proceeding to the next step, please install it from this \n> [link](https://pytorch.org/get-started/locally/).\n\n\nInstall the deep learning compilers:\n```\npython -m nebullvm.installers.auto_installer \\\n    --frameworks torch onnx tensorflow huggingface \\\n    --compilers all\n```\n\nFor more details on the installation step, please visit [Installation](https://docs.nebuly.com/speedster/installation).\n\n\n# API quick view\n\nOnly a single line of code is needed to get your accelerated model:\n\n```python\nfrom speedster import optimize_model\n\noptimized_model = optimize_model(model, input_data=input_data)\n```\nCheckout how to define the `model` and `input_data` parameters depending on which framework you want to use and how to use the optimized model: \n[PyTorch](https://github.com/nebuly-ai/nebullvm/tree/main/notebooks/speedster/pytorch#pytorch-api-quick-view), \n[HuggingFace](https://github.com/nebuly-ai/nebullvm/tree/main/notebooks/speedster/huggingface#huggingface-api-quick-view), \n[TensorFlow](https://github.com/nebuly-ai/nebullvm/tree/main/notebooks/speedster/tensorflow#tensorflow-api-quick-view), \n[ONNX](https://github.com/nebuly-ai/nebullvm/tree/main/notebooks/speedster/onnx#onnx-api-quick-view).\n\nFor more details, please visit also the documentation sections [Get Started](https://docs.nebuly.com/speedster/get-started),  [Speedster API](https://docs.nebuly.com/speedster/get-started/speedster-api) and [Examples of API options](https://docs.nebuly.com/speedster/get-started/examples-of-api-options).\n\n# **How it works**\n\nWe are not here to reinvent the wheel, but to build an all-in-one open-source product to master all the available AI acceleration techniques and deliver the **fastest AI ever.** As a result, `Speedster` leverages available enterprise-grade open-source optimization tools. If these tools and  communities already exist, and are distributed under a permissive license (Apache, MIT, etc), we integrate them and happily contribute to their communities. However, many tools do not exist yet, in which case we implement them and open-source the code so that the community can benefit from it.\n\n### **Product design**\n\n`Speedster`\u00a0is shaped around **4 building blocks** and leverages a modular design to foster scalability and integration of new acceleration components across the stack.\n\n- [x]  **Converter:** converts the input model from its original framework to the framework backends supported by `Speedster`, namely PyTorch, TensorFlow, and ONNX. This allows the Compressor and Optimizer modules to apply any optimization technique to the model.\n- [x]  **Compressor:**\u00a0applies various compression techniques to the model, such as pruning, knowledge distillation, or quantization-aware training.\n- [x]  **Optimizer:**\u00a0converts the compressed models to the intermediate representation (IR) of the supported deep learning compilers. The compilers apply both post-training quantization techniques and graph optimizations, to produce compiled binary files.\n- [x]  **Inference Learner:**\u00a0takes the best performing compiled model and converts it to the same interface as the original input model.\n\n![nebullvm nebuly ai](https://user-images.githubusercontent.com/100476561/180975206-3a3a1f80-afc6-42b0-9953-4b8426c09b62.png)\n\nThe\u00a0**compressor**\u00a0stage leverages the following open-source projects:\n\n- [Intel/neural-compressor](https://github.com/intel/neural-compressor): targeting to provide unified APIs for network compression technologies, such as low precision quantization, sparsity, pruning, knowledge distillation, across different deep learning frameworks to pursue optimal inference performance.\n- [SparseML](https://github.com/neuralmagic/sparseml): libraries for applying sparsification recipes to neural networks with a few lines of code, enabling faster and smaller models.\n\nThe\u00a0**optimizer stage**\u00a0leverages the following open-source projects:\n\n- [Apache TVM](https://github.com/apache/tvm): open deep learning compiler stack for cpu, gpu and specialized accelerators.\n- [BladeDISC](https://github.com/alibaba/BladeDISC): end-to-end Dynamic Shape Compiler project for machine learning workloads.\n- [DeepSparse](https://github.com/neuralmagic/deepsparse): neural network inference engine that delivers GPU-class performance for sparsified models on CPUs.\n- [OpenVINO](https://github.com/openvinotoolkit/openvino): open-source toolkit for optimizing and deploying AI inference.\n- [ONNX Runtime](https://github.com/microsoft/onnxruntime): cross-platform, high performance ML inferencing and training accelerator\n- [TensorRT](https://github.com/NVIDIA/TensorRT): C++ library for high performance inference on NVIDIA GPUs and deep learning accelerators.\n- [TFlite](https://github.com/tensorflow/tflite-micro)\u00a0and\u00a0[XLA](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/compiler/xla): open-source libraries to accelerate TensorFlow models.\n\n# **Documentation**\n\n- [Installation](https://docs.nebuly.com/speedster/installation)\n- [Get started](https://docs.nebuly.com/speedster/get-started)\n- [Notebooks](https://github.com/nebuly-ai/nebullvm/tree/main/notebooks)\n- [Benchmarks](https://docs.nebuly.com/speedster/benchmarks)\n- [Supported features and roadmap](https://docs.nebuly.com/speedster/how-speedster-works/supported-features-and-roadmap)\n\n# **Community**\n\n- **[Discord](https://discord.gg/RbeQMu886J)**: best for sharing your projects, hanging out with the community and learning about AI acceleration.\n- **[GitHub issues](https://github.com/nebuly-ai/nebullvm/issues)**: ideal for suggesting new acceleration components, requesting new features, and reporting bugs and improvements.\n\nWe\u2019re developing `Speedster` together with our community so the best way to get started is to pick a `good-first issue`. Please read our [contribution guidelines](https://docs.nebuly.com/welcome/questions-and-contributions) for a deep dive on how to best contribute to our project!\n\nDon't forget to leave a star \u2b50 to support the project and happy acceleration \ud83d\ude80\n\n# **Status**\n\n- **Model converter backends**\n    - [x]  ONNX, PyTorch, TensorFlow\n    - [ ]  Jax\n- **Compressor**\n    - [x]  Pruning and sparsity\n    - [ ]  Quantized-aware training, distillation, layer replacement and low rank compression\n- **Optimizer**\n    - [x]  TensorRT, OpenVINO, ONNX Runtime, TVM, PyTorch, DeepSparse, BladeDisc, TFlite, XLA\n- **Inference learners**\n    - [x]  PyTorch, ONNX, Hugging Face, TensorFlow\n    - [ ]  Jax\n\n---\n\n<p align=\"center\">\n  <a href=\"https://discord.gg/RbeQMu886J\">Join the community</a> |\n  <a href=\"https://docs.nebuly.com/welcome/questions-and-contributions\">Contribute to the library</a>\n</p>\n\n\n<p align=\"center\">\n<a href=\"https://docs.nebuly.com/speedster/installation\">Installation</a> \u2022\n<a href=\"https://docs.nebuly.com/speedster/get-started\">Get started</a> \u2022\n<a href=\"https://github.com/nebuly-ai/nebullvm/tree/main/notebooks\">Notebooks</a> \u2022\n<a href=\"https://docs.nebuly.com/speedster/benchmarks\">Benchmarks</a>\n</p>\n",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "",
            "keywords": "",
            "license": "",
            "maintainer": "",
            "maintainer_email": "",
            "name": "speedster",
            "package_url": "https://pypi.org/project/speedster/",
            "platform": null,
            "project_url": "https://pypi.org/project/speedster/",
            "project_urls": null,
            "release_url": "https://pypi.org/project/speedster/0.0.2/",
            "requires_dist": [
                "nebullvm (>=0.6.1)"
            ],
            "requires_python": "",
            "summary": "",
            "version": "0.0.2",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16274619,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "35fafaf068500b3fd6f51bbe6979e862",
                    "sha256": "48fc33c4e909e9f128db37b093c72fbfee1451807bb9edd6b09f12aa4d047c3e"
                },
                "downloads": -1,
                "filename": "speedster-0.0.2-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "35fafaf068500b3fd6f51bbe6979e862",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": null,
                "size": 22233,
                "upload_time": "2023-01-01T22:33:49",
                "upload_time_iso_8601": "2023-01-01T22:33:49.505524Z",
                "url": "https://files.pythonhosted.org/packages/d7/14/3c44585a6adc98a9db0dfdb69157657f2925fa93648accaa2f75ee31d3a3/speedster-0.0.2-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "39af1090ebc1a55e1a51be669dd79f13",
                    "sha256": "d4ed4e5ae1718d1416e29d8bc2b4957fcf5d920da4c2e8af552a0c998a5d9f64"
                },
                "downloads": -1,
                "filename": "speedster-0.0.2.tar.gz",
                "has_sig": false,
                "md5_digest": "39af1090ebc1a55e1a51be669dd79f13",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": null,
                "size": 21303,
                "upload_time": "2023-01-01T22:33:50",
                "upload_time_iso_8601": "2023-01-01T22:33:50.824146Z",
                "url": "https://files.pythonhosted.org/packages/6e/88/ee72d3fccfe8233187c4d517ab2451597a7964831c7a027566d76b2f419e/speedster-0.0.2.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}