{
    "0.0.1": {
        "info": {
            "author": "Petals devs",
            "author_email": "hivemind-team@hotmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 1 - Planning",
                "Intended Audience :: Developers",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: MIT License",
                "Programming Language :: Python :: 3",
                "Programming Language :: Python :: 3.10",
                "Programming Language :: Python :: 3.7",
                "Programming Language :: Python :: 3.8",
                "Programming Language :: Python :: 3.9",
                "Topic :: Scientific/Engineering",
                "Topic :: Scientific/Engineering :: Artificial Intelligence",
                "Topic :: Scientific/Engineering :: Mathematics",
                "Topic :: Software Development",
                "Topic :: Software Development :: Libraries",
                "Topic :: Software Development :: Libraries :: Python Modules"
            ],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/bigscience-workshop/petals",
            "keywords": "",
            "license": "",
            "maintainer": "",
            "maintainer_email": "",
            "name": "petals",
            "package_url": "https://pypi.org/project/petals/",
            "platform": null,
            "project_url": "https://pypi.org/project/petals/",
            "project_urls": {
                "Bug Tracker": "https://github.com/bigscience-workshop/petals/issues",
                "Homepage": "https://github.com/bigscience-workshop/petals"
            },
            "release_url": "https://pypi.org/project/petals/0.0.1/",
            "requires_dist": null,
            "requires_python": ">=3.7",
            "summary": "To install, follow instructions from https://github.com/bigscience-workshop/petals",
            "version": "0.0.1",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16262374,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "51c375187de95023194f461ba3df6056",
                    "sha256": "be1daea9bc20fcf7cf45ce1da1cc75f901dc1a1a8ed53a5bddb37bc2e80b3bb8"
                },
                "downloads": -1,
                "filename": "petals-0.0.1-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "51c375187de95023194f461ba3df6056",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.7",
                "size": 1330,
                "upload_time": "2022-12-27T14:33:56",
                "upload_time_iso_8601": "2022-12-27T14:33:56.836511Z",
                "url": "https://files.pythonhosted.org/packages/e0/ba/20f77f8fe199467604e3b82f817b3523e198da860f22fae0d2190e7e864a/petals-0.0.1-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "466238344a1844cd025a4acb1321501a",
                    "sha256": "acc00a4e6e00bbe339c3f25b874c52e07f48cac8bcdc9f7a8b2e6909f361b09d"
                },
                "downloads": -1,
                "filename": "petals-0.0.1.tar.gz",
                "has_sig": false,
                "md5_digest": "466238344a1844cd025a4acb1321501a",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.7",
                "size": 1370,
                "upload_time": "2022-12-27T14:33:58",
                "upload_time_iso_8601": "2022-12-27T14:33:58.416168Z",
                "url": "https://files.pythonhosted.org/packages/c4/f8/41540e88c5fd15834c8ac3872029b56fe580774a774ab59c5f018e7ce5b7/petals-0.0.1.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "1.0.0": {
        "info": {
            "author": "Petals Developers",
            "author_email": "petals-dev@googlegroups.com",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 4 - Beta",
                "Intended Audience :: Developers",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: MIT License",
                "Programming Language :: Python :: 3",
                "Programming Language :: Python :: 3.7",
                "Programming Language :: Python :: 3.8",
                "Programming Language :: Python :: 3.9",
                "Topic :: Scientific/Engineering",
                "Topic :: Scientific/Engineering :: Artificial Intelligence",
                "Topic :: Scientific/Engineering :: Mathematics",
                "Topic :: Software Development",
                "Topic :: Software Development :: Libraries",
                "Topic :: Software Development :: Libraries :: Python Modules"
            ],
            "description": "<p align=\"center\">\n    <img src=\"https://i.imgur.com/7eR7Pan.png\" width=\"400\"><br>\n    Run 100B+ language models at home, BitTorrent-style.<br>\n    Fine-tuning and inference up to 10x faster than offloading<br><br>\n</p>\n\nGenerate text using distributed BLOOM and fine-tune it for your own tasks:\n\n```python\nfrom petals import DistributedBloomForCausalLM\n\nmodel = DistributedBloomForCausalLM.from_pretrained(\"bigscience/bloom-petals\", tuning_mode=\"ptune\", pre_seq_len=16)\n# Embeddings & prompts are on your device, BLOOM blocks are distributed across the Internet\n\ninputs = tokenizer(\"A cat sat\", return_tensors=\"pt\")[\"input_ids\"]\noutputs = model.generate(inputs, max_new_tokens=5)\nprint(tokenizer.decode(outputs[0]))  # A cat sat on a mat...\n\n# Fine-tuning (updates only prompts or adapters hosted locally)\noptimizer = torch.optim.AdamW(model.parameters())\nfor input_ids, labels in data_loader:\n    outputs = model.forward(input_ids)\n    loss = cross_entropy(outputs.logits, labels)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n```\n\n<p align=\"center\">\n    \ud83d\ude80 &nbsp;<b><a href=\"https://colab.research.google.com/drive/1Ervk6HPNS6AYVr3xVdQnY5a-TjjmLCdQ?usp=sharing\">Try now in Colab</a></b>\n</p>\n\nConnect your own GPU and increase Petals capacity:\n\n```bash\n# In an Anaconda env\nconda install pytorch cudatoolkit=11.3 -c pytorch\npip install git+https://github.com/bigscience-workshop/petals\npython -m petals.cli.run_server bigscience/bloom-petals\n\n# Or using our GPU-enabled Docker image\nsudo docker run --net host --ipc host --gpus all --volume petals-cache:/cache --rm \\\n    learningathome/petals:main python -m petals.cli.run_server bigscience/bloom-petals\n```\n\n\ud83d\udcac If you have any issues or feedback, please join [our Discord server](https://discord.gg/D9MwApKgWa)!\n\nCheck out more examples and tutorials:\n\n- Chatbot web app: [link](http://chat.petals.ml), [source code](https://github.com/borzunov/petals-chat)\n- Training a personified chatbot: [notebook](./examples/prompt-tuning-personachat.ipynb)\n- Fine-tuning BLOOM for text semantic classification: [notebook](./examples/prompt-tuning-sst2.ipynb)\n- Launching your own swarm: [tutorial](https://github.com/bigscience-workshop/petals/wiki/Launch-your-own-swarm)\n- Running a custom foundation model: [tutorial](https://github.com/bigscience-workshop/petals/wiki/Run-a-custom-model-with-Petals)\n\n## How does it work?\n\n- Petals runs large language models like BLOOM-176B **collaboratively** \u2014 you load a small part of the model, then team up with people serving the other parts to run inference or fine-tuning.\n- Inference runs at \u2248 1 sec per step (token) \u2014 10x faster than possible with offloading, enough for chatbots and other interactive apps. Parallel inference reaches hundreds of tokens/sec.\n- Beyond classic language model APIs \u2014 you can employ any fine-tuning and sampling methods by executing custom paths through the model or accessing its hidden states. You get the comforts of an API with the flexibility of PyTorch.\n\n<p align=\"center\">\n    <img src=\"https://i.imgur.com/RTYF3yW.png\" width=\"800\">\n</p>\n\n<p align=\"center\">\n    \ud83d\udcdc &nbsp;<b><a href=\"https://arxiv.org/pdf/2209.01188.pdf\">Read paper</a></b>\n</p>\n\n### \ud83d\udd12 Privacy and security\n\nThe Petals public swarm is designed for research and academic use. **Please do not use the public swarm to process sensitive data.** We ask for that because it is an open network, and it is technically possible for peers serving model layers to recover input data and model outputs or modify them in a malicious way. Instead, you can [set up a private Petals swarm](https://github.com/bigscience-workshop/petals/wiki/Launch-your-own-swarm) hosted by people and organization you trust, who are authorized to process your data. We discuss privacy and security in more detail [here](https://github.com/bigscience-workshop/petals/wiki/Security,-privacy,-and-AI-safety).\n\n### \ud83d\udccb Model's terms of use\n\nBefore building your own application that runs a language model with Petals, please check out the model's **terms of use, risks, and limitations**. In case of BLOOM, they are described in its [model card](https://huggingface.co/bigscience/bloom) and [license](https://huggingface.co/spaces/bigscience/license).\n\n## FAQ\n\n1. **What's the motivation for people to host model layers in the public swarm?**\n\n    People who run inference and fine-tuning themselves get a certain speedup if they host a part of the model locally. Some may be also motivated to \"give back\" to the community helping them to run the model (similarly to how [BitTorrent](https://en.wikipedia.org/wiki/BitTorrent) users help others by sharing data they have already downloaded).\n\n    Since it may be not enough for everyone, we are also working on introducing explicit __incentives__ (\"bloom points\") for people donating their GPU time to the public swarm. Once this system is ready, people who earned these points will be able to spend them on inference/fine-tuning with higher priority or increased security guarantees, or (maybe) exchange them for other rewards.\n\n2. **Why is the platform named \"Petals\"?**\n\n    \"Petals\" is a metaphor for people serving different parts of the model. Together, they host the entire language model &mdash; [BLOOM](https://huggingface.co/bigscience/bloom).\n\n    While our platform focuses on BLOOM now, we aim to support more [foundation models](https://arxiv.org/abs/2108.07258) in future.\n\n## Installation\n\nHere's how to install Petals with conda:\n```\nconda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch\npip install git+https://github.com/bigscience-workshop/petals\n```\n\nThis script uses Anaconda to install cuda-enabled PyTorch.\nIf you don't have anaconda, you can get it from [here](https://www.anaconda.com/products/distribution).\nIf you don't want anaconda, you can install PyTorch [any other way](https://pytorch.org/get-started/locally/).\nIf you want to run models with 8-bit weights, please install **PyTorch with CUDA 11** or newer for compatility with [bitsandbytes](https://github.com/timDettmers/bitsandbytes).\n\n__System requirements:__ Petals only supports Linux for now. If you don't have a Linux machine, consider running Petals in Docker (see our [image](https://hub.docker.com/r/learningathome/petals)) or, in case of Windows, in WSL2 ([read more](https://learn.microsoft.com/en-us/windows/ai/directml/gpu-cuda-in-wsl)). CPU is enough to run a client, but you probably need a GPU to run a server efficiently.\n\n## \ud83d\udee0\ufe0f Development\n\nPetals uses pytest with a few plugins. To install them, run:\n\n```python\nconda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch\ngit clone https://github.com/bigscience-workshop/petals.git && cd petals\npip install -e .[dev]\n```\n\nTo run minimalistic tests, you need to make a local swarm with a small model and some servers. You may find more information about how local swarms work and how to run them in [this tutorial](https://github.com/bigscience-workshop/petals/wiki/Launch-your-own-swarm).\n\n```bash\nexport MODEL_NAME=bloom-testing/test-bloomd-560m-main\n\npython -m petals.cli.run_server $MODEL_NAME --block_indices 0:12 \\\n  --identity tests/test.id --host_maddrs /ip4/127.0.0.1/tcp/31337 --new_swarm  &> server1.log &\nsleep 5  # wait for the first server to initialize DHT\n\npython -m petals.cli.run_server $MODEL_NAME --block_indices 12:24 \\\n  --initial_peers SEE_THE_OUTPUT_OF_THE_1ST_PEER &> server2.log &\n\ntail -f server1.log server2.log  # view logs for both servers\n```\n\nThen launch pytest:\n\n```\nexport MODEL_NAME=bloom-testing/test-bloomd-560m-main REF_NAME=bigscience/bloom-560m\nexport INITIAL_PEERS=/ip4/127.0.0.1/tcp/31337/p2p/QmS9KwZptnVdB9FFV7uGgaTq4sEKBwcYeKZDfSpyKDUd1g\nPYTHONPATH=. pytest tests --durations=0 --durations-min=1.0 -v\n```\n\nAfter you're done, you can terminate the servers and ensure that no zombie processes are left with `pkill -f petals.cli.run_server && pkill -f p2p`.\n\nThe automated tests use a more complex server configuration that can be found [here](https://github.com/bigscience-workshop/petals/blob/main/.github/workflows/run-tests.yaml).\n\n### Code style\n\nWe use [black](https://black.readthedocs.io/en/stable/the_black_code_style/current_style.html) and [isort](https://pycqa.github.io/isort/) for all pull requests.\nBefore committing your code, simply run `black . && isort .` and you will be fine.\n\n--------------------------------------------------------------------------------\n\n<p align=\"center\">\n    This project is a part of the <a href=\"https://bigscience.huggingface.co/\">BigScience</a> research workshop.\n</p>\n<p align=\"center\">\n    <img src=\"https://petals.ml/bigscience.png\" width=\"150\">\n</p>\n",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/bigscience-workshop/petals",
            "keywords": "",
            "license": "",
            "maintainer": "",
            "maintainer_email": "",
            "name": "petals",
            "package_url": "https://pypi.org/project/petals/",
            "platform": null,
            "project_url": "https://pypi.org/project/petals/",
            "project_urls": {
                "Bug Tracker": "https://github.com/bigscience-workshop/petals/issues",
                "Homepage": "https://github.com/bigscience-workshop/petals"
            },
            "release_url": "https://pypi.org/project/petals/1.0.0/",
            "requires_dist": [
                "torch (>=1.12)",
                "bitsandbytes (==0.34.0)",
                "accelerate (==0.15.0)",
                "huggingface-hub (==0.11.1)",
                "transformers (==4.25.1)",
                "protobuf (<4.0dev,>=3.20.3)",
                "speedtest-cli (==2.1.3)",
                "hivemind (==1.1.3)",
                "humanfriendly",
                "async-timeout (>=4.0.2)",
                "pytest (==6.2.5) ; extra == 'dev'",
                "pytest-forked ; extra == 'dev'",
                "pytest-asyncio (==0.16.0) ; extra == 'dev'",
                "black (==22.3.0) ; extra == 'dev'",
                "isort (==5.10.1) ; extra == 'dev'",
                "psutil ; extra == 'dev'"
            ],
            "requires_python": ">=3.7",
            "summary": "Easy way to efficiently run 100B+ language models without high-end GPUs",
            "version": "1.0.0",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16262374,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "934607d5b63070e87f426c671d17ad31",
                    "sha256": "75b17dcf36a2c7800efffb983baccbaa9d5122f7f7d6d023f4d73a73c38fe311"
                },
                "downloads": -1,
                "filename": "petals-1.0.0-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "934607d5b63070e87f426c671d17ad31",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.7",
                "size": 81375,
                "upload_time": "2022-12-30T21:39:08",
                "upload_time_iso_8601": "2022-12-30T21:39:08.185925Z",
                "url": "https://files.pythonhosted.org/packages/36/d3/5829fbb5ad119fa79dbf4ac7b7f25a48af49608a49cb46ab390767aefa78/petals-1.0.0-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "64abbfeef190ea2dfabd8f6f8da23975",
                    "sha256": "1b609a221c22a32c97fbd0fa5ff0bb7f6e2f2d05ee605bcc96be2c216ddddd4e"
                },
                "downloads": -1,
                "filename": "petals-1.0.0.tar.gz",
                "has_sig": false,
                "md5_digest": "64abbfeef190ea2dfabd8f6f8da23975",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.7",
                "size": 70508,
                "upload_time": "2022-12-30T21:39:10",
                "upload_time_iso_8601": "2022-12-30T21:39:10.091229Z",
                "url": "https://files.pythonhosted.org/packages/43/63/a53f47e52ed6ec09fe7c98f28a3877a21d47f3ca50e318d96c7e336e0589/petals-1.0.0.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}