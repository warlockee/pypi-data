{
    "0.0.1": {
        "info": {
            "author": "Vaibhav Singh",
            "author_email": "vaibhav.singh.3001@gmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 3 - Alpha",
                "Intended Audience :: Developers",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: Apache Software License",
                "Programming Language :: Python :: 3.10",
                "Programming Language :: Python :: 3.9",
                "Topic :: Scientific/Engineering",
                "Topic :: Scientific/Engineering :: Artificial Intelligence",
                "Topic :: Software Development",
                "Topic :: Software Development :: Libraries",
                "Topic :: Software Development :: Libraries :: Python Modules"
            ],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/veb-101/Attention-and-Transformers",
            "keywords": "tensorflow keras attention transformers",
            "license": "Apache 2.0",
            "maintainer": "",
            "maintainer_email": "",
            "name": "Attention-and-Transformers",
            "package_url": "https://pypi.org/project/Attention-and-Transformers/",
            "platform": null,
            "project_url": "https://pypi.org/project/Attention-and-Transformers/",
            "project_urls": {
                "Homepage": "https://github.com/veb-101/Attention-and-Transformers"
            },
            "release_url": "https://pypi.org/project/Attention-and-Transformers/0.0.1/",
            "requires_dist": [
                "tensorflow-datasets",
                "livelossplot",
                "Pillow",
                "opencv-contrib-python",
                "pandas",
                "scikit-learn",
                "matplotlib",
                "scikit-image",
                "tensorflow-addons ; platform_machine != \"aarch64\" and platform_machine != \"aarch32\"",
                "tensorflow (==2.10.0) ; platform_system != \"Darwin\"",
                "tensorflow-macos ; platform_system == \"Darwin\""
            ],
            "requires_python": ">=3.9,<3.11.*",
            "summary": "Building attention mechanisms and Transformer models from scratch. Alias ATF. https://github.com/veb-101/Attention-and-Transformers",
            "version": "0.0.1",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16129021,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "c3613d59a4186f4f13fa17dfbb08999c",
                    "sha256": "3035de547414eb7fc292a4e4e3dcb4bc4b57aa8bb544a1cd4e625eb647f23069"
                },
                "downloads": -1,
                "filename": "Attention_and_Transformers-0.0.1-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "c3613d59a4186f4f13fa17dfbb08999c",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.9,<3.11.*",
                "size": 8379,
                "upload_time": "2022-11-18T21:02:35",
                "upload_time_iso_8601": "2022-11-18T21:02:35.618551Z",
                "url": "https://files.pythonhosted.org/packages/8f/f0/8046683eb8b6738c8fafa41c8f6ab317d2331000fa5303fd713134b87ee2/Attention_and_Transformers-0.0.1-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "c8511ddbbafc8f9bd4819debf82c1bcf",
                    "sha256": "34559a79d900db5e40f3056e34a1d733d95b249bbf4f2c3d9df8c4db3e259a56"
                },
                "downloads": -1,
                "filename": "Attention_and_Transformers-0.0.1.tar.gz",
                "has_sig": false,
                "md5_digest": "c8511ddbbafc8f9bd4819debf82c1bcf",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.9,<3.11.*",
                "size": 7613,
                "upload_time": "2022-11-18T21:02:37",
                "upload_time_iso_8601": "2022-11-18T21:02:37.262685Z",
                "url": "https://files.pythonhosted.org/packages/5f/11/abb921146cdd95edd31e8b51a35354a246b9f3e141ea614c8c71ffdc2a7d/Attention_and_Transformers-0.0.1.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "0.0.10": {
        "info": {
            "author": "Vaibhav Singh",
            "author_email": "vaibhav.singh.3001@gmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 3 - Alpha",
                "Intended Audience :: Developers",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: Apache Software License",
                "Programming Language :: Python :: 3.10",
                "Programming Language :: Python :: 3.9",
                "Topic :: Scientific/Engineering",
                "Topic :: Scientific/Engineering :: Artificial Intelligence",
                "Topic :: Software Development",
                "Topic :: Software Development :: Libraries",
                "Topic :: Software Development :: Libraries :: Python Modules"
            ],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/veb-101/Attention-and-Transformers",
            "keywords": "tensorflow keras attention transformers",
            "license": "Apache 2.0",
            "maintainer": "",
            "maintainer_email": "",
            "name": "Attention-and-Transformers",
            "package_url": "https://pypi.org/project/Attention-and-Transformers/",
            "platform": null,
            "project_url": "https://pypi.org/project/Attention-and-Transformers/",
            "project_urls": {
                "Homepage": "https://github.com/veb-101/Attention-and-Transformers"
            },
            "release_url": "https://pypi.org/project/Attention-and-Transformers/0.0.10/",
            "requires_dist": [
                "tensorflow-datasets",
                "livelossplot",
                "Pillow",
                "opencv-contrib-python",
                "pandas",
                "scikit-learn",
                "matplotlib",
                "scikit-image",
                "tensorflow-addons ; platform_machine != \"aarch64\" and platform_machine != \"aarch32\"",
                "tensorflow ; platform_system != \"Darwin\"",
                "tensorflow-macos ; platform_system == \"Darwin\""
            ],
            "requires_python": ">=3.7,<3.11.*",
            "summary": "Building attention mechanisms and Transformer models from scratch. Alias ATF.",
            "version": "0.0.10",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16129021,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "56e1c802adee0d45897a6dd26dbbebc3",
                    "sha256": "d2b07a813a567cf913654ac1bd6d3d5c542a5ce3d262bfbfdec1129a51139fa3"
                },
                "downloads": -1,
                "filename": "Attention_and_Transformers-0.0.10-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "56e1c802adee0d45897a6dd26dbbebc3",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.7,<3.11.*",
                "size": 21294,
                "upload_time": "2022-12-16T17:58:18",
                "upload_time_iso_8601": "2022-12-16T17:58:18.699674Z",
                "url": "https://files.pythonhosted.org/packages/8f/05/b1c5c4a9192fa46062e2ba9ba28ed018366d106914ba6fafcace033128d4/Attention_and_Transformers-0.0.10-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "5613f36a8f3b8cacec42db9fd5b1b5c7",
                    "sha256": "7b673233332d09e1de052409e4db5f574bdcbacc279b6c07505d9abb5e96244b"
                },
                "downloads": -1,
                "filename": "Attention_and_Transformers-0.0.10.tar.gz",
                "has_sig": false,
                "md5_digest": "5613f36a8f3b8cacec42db9fd5b1b5c7",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.7,<3.11.*",
                "size": 13839,
                "upload_time": "2022-12-16T17:58:20",
                "upload_time_iso_8601": "2022-12-16T17:58:20.019680Z",
                "url": "https://files.pythonhosted.org/packages/9f/7a/1abf3baa59e239e973debf05df235b3835c012046fef0c1c473cb6cb16bf/Attention_and_Transformers-0.0.10.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "0.0.11": {
        "info": {
            "author": "Vaibhav Singh",
            "author_email": "vaibhav.singh.3001@gmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 3 - Alpha",
                "Intended Audience :: Developers",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: Apache Software License",
                "Programming Language :: Python :: 3.10",
                "Programming Language :: Python :: 3.9",
                "Topic :: Scientific/Engineering",
                "Topic :: Scientific/Engineering :: Artificial Intelligence",
                "Topic :: Software Development",
                "Topic :: Software Development :: Libraries",
                "Topic :: Software Development :: Libraries :: Python Modules"
            ],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/veb-101/Attention-and-Transformers",
            "keywords": "tensorflow keras attention transformers",
            "license": "Apache 2.0",
            "maintainer": "",
            "maintainer_email": "",
            "name": "Attention-and-Transformers",
            "package_url": "https://pypi.org/project/Attention-and-Transformers/",
            "platform": null,
            "project_url": "https://pypi.org/project/Attention-and-Transformers/",
            "project_urls": {
                "Homepage": "https://github.com/veb-101/Attention-and-Transformers"
            },
            "release_url": "https://pypi.org/project/Attention-and-Transformers/0.0.11/",
            "requires_dist": [
                "tensorflow-datasets",
                "livelossplot",
                "Pillow",
                "opencv-contrib-python",
                "pandas",
                "scikit-learn",
                "matplotlib",
                "scikit-image",
                "tensorflow-addons ; platform_machine != \"aarch64\" and platform_machine != \"aarch32\"",
                "tensorflow ; platform_system != \"Darwin\"",
                "tensorflow-macos ; platform_system == \"Darwin\""
            ],
            "requires_python": ">=3.7,<3.11.*",
            "summary": "Building attention mechanisms and Transformer models from scratch. Alias ATF.",
            "version": "0.0.11",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16129021,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "9cb610c56dfae9eab23f6e6592bcaf99",
                    "sha256": "82e988670beb8c8645d322857f2fc30a64f5f54beeea1fba269e6ca773083a31"
                },
                "downloads": -1,
                "filename": "Attention_and_Transformers-0.0.11-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "9cb610c56dfae9eab23f6e6592bcaf99",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.7,<3.11.*",
                "size": 21283,
                "upload_time": "2022-12-16T18:00:43",
                "upload_time_iso_8601": "2022-12-16T18:00:43.414742Z",
                "url": "https://files.pythonhosted.org/packages/30/1f/dc2cc3597f3e3b4014c79ec1a3e20cefd6dc0b47174da01333378be7132c/Attention_and_Transformers-0.0.11-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "d4a1f8f6e425bd55e10753545c59f7e3",
                    "sha256": "ceb3d7abe776eaed46ab66d0eab96c29f83483d5a60c8fbe8cbf8dd6ef30ba2d"
                },
                "downloads": -1,
                "filename": "Attention_and_Transformers-0.0.11.tar.gz",
                "has_sig": false,
                "md5_digest": "d4a1f8f6e425bd55e10753545c59f7e3",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.7,<3.11.*",
                "size": 13864,
                "upload_time": "2022-12-16T18:00:44",
                "upload_time_iso_8601": "2022-12-16T18:00:44.667675Z",
                "url": "https://files.pythonhosted.org/packages/7c/10/3360d9076daca7e49e4107b622cf24d0c3f3397352c4dcf916ebb8a4e72d/Attention_and_Transformers-0.0.11.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "0.0.13": {
        "info": {
            "author": "Vaibhav Singh",
            "author_email": "vaibhav.singh.3001@gmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 3 - Alpha",
                "Intended Audience :: Developers",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: Apache Software License",
                "Programming Language :: Python :: 3.10",
                "Programming Language :: Python :: 3.7",
                "Programming Language :: Python :: 3.8",
                "Programming Language :: Python :: 3.9",
                "Topic :: Scientific/Engineering",
                "Topic :: Scientific/Engineering :: Artificial Intelligence",
                "Topic :: Software Development",
                "Topic :: Software Development :: Libraries",
                "Topic :: Software Development :: Libraries :: Python Modules"
            ],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/veb-101/Attention-and-Transformers",
            "keywords": "tensorflow keras attention transformers",
            "license": "Apache 2.0",
            "maintainer": "",
            "maintainer_email": "",
            "name": "Attention-and-Transformers",
            "package_url": "https://pypi.org/project/Attention-and-Transformers/",
            "platform": null,
            "project_url": "https://pypi.org/project/Attention-and-Transformers/",
            "project_urls": {
                "Homepage": "https://github.com/veb-101/Attention-and-Transformers"
            },
            "release_url": "https://pypi.org/project/Attention-and-Transformers/0.0.13/",
            "requires_dist": [
                "tensorflow-datasets",
                "livelossplot",
                "Pillow",
                "opencv-contrib-python",
                "pandas",
                "scikit-learn",
                "matplotlib",
                "scikit-image",
                "tensorflow-addons ; platform_machine != \"aarch64\" and platform_machine != \"aarch32\"",
                "tensorflow ; platform_system != \"Darwin\"",
                "tensorflow-macos ; platform_system == \"Darwin\""
            ],
            "requires_python": ">=3.7,<3.11.*",
            "summary": "Building attention mechanisms and Transformer models from scratch. Alias ATF.",
            "version": "0.0.13",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16129021,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "cea6d116e8a4f9e46ae110c42ec04a5e",
                    "sha256": "f48381fee0b416b331498405213b27d9b76b7b5d8739fa4bb7f666b5d5a1b47e"
                },
                "downloads": -1,
                "filename": "Attention_and_Transformers-0.0.13-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "cea6d116e8a4f9e46ae110c42ec04a5e",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.7,<3.11.*",
                "size": 21287,
                "upload_time": "2022-12-16T18:05:58",
                "upload_time_iso_8601": "2022-12-16T18:05:58.881231Z",
                "url": "https://files.pythonhosted.org/packages/5b/e9/9bf0c2a047aa23ac273b7475757d392edcb467f4a5990a624ccfb27bd063/Attention_and_Transformers-0.0.13-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "97c440174e5f6e1e9c8264a5b6205498",
                    "sha256": "159d38036aa1d465efcf01567ee068409b3b715caeabc3f1bfc232ccbb1d5536"
                },
                "downloads": -1,
                "filename": "Attention_and_Transformers-0.0.13.tar.gz",
                "has_sig": false,
                "md5_digest": "97c440174e5f6e1e9c8264a5b6205498",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.7,<3.11.*",
                "size": 13860,
                "upload_time": "2022-12-16T18:06:01",
                "upload_time_iso_8601": "2022-12-16T18:06:01.214724Z",
                "url": "https://files.pythonhosted.org/packages/82/98/be8e4a64717d300409d12bb75c6768db3851960ac1b72732149dccfae29e/Attention_and_Transformers-0.0.13.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "0.0.14": {
        "info": {
            "author": "Vaibhav Singh",
            "author_email": "vaibhav.singh.3001@gmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 3 - Alpha",
                "Intended Audience :: Developers",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: Apache Software License",
                "Programming Language :: Python :: 3.10",
                "Programming Language :: Python :: 3.7",
                "Programming Language :: Python :: 3.8",
                "Programming Language :: Python :: 3.9",
                "Topic :: Scientific/Engineering",
                "Topic :: Scientific/Engineering :: Artificial Intelligence",
                "Topic :: Software Development",
                "Topic :: Software Development :: Libraries",
                "Topic :: Software Development :: Libraries :: Python Modules"
            ],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/veb-101/Attention-and-Transformers",
            "keywords": "tensorflow keras attention transformers",
            "license": "Apache 2.0",
            "maintainer": "",
            "maintainer_email": "",
            "name": "Attention-and-Transformers",
            "package_url": "https://pypi.org/project/Attention-and-Transformers/",
            "platform": null,
            "project_url": "https://pypi.org/project/Attention-and-Transformers/",
            "project_urls": {
                "Homepage": "https://github.com/veb-101/Attention-and-Transformers"
            },
            "release_url": "https://pypi.org/project/Attention-and-Transformers/0.0.14/",
            "requires_dist": [
                "tensorflow-datasets",
                "livelossplot",
                "Pillow",
                "opencv-contrib-python",
                "pandas",
                "scikit-learn",
                "matplotlib",
                "scikit-image",
                "tensorflow-addons ; platform_machine != \"aarch64\" and platform_machine != \"aarch32\"",
                "tensorflow (>=2.10.0) ; platform_system != \"Darwin\"",
                "tensorflow-macos ; platform_system == \"Darwin\""
            ],
            "requires_python": ">=3.7,<3.11.*",
            "summary": "Building attention mechanisms and Transformer models from scratch. Alias ATF.",
            "version": "0.0.14",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16129021,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "86ae8b919eff9d6268637e9b93bb2001",
                    "sha256": "dddce00a839ac0ba029af7aa3d6f761b31880ade165a6c37de7e26a9069111b4"
                },
                "downloads": -1,
                "filename": "Attention_and_Transformers-0.0.14-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "86ae8b919eff9d6268637e9b93bb2001",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.7,<3.11.*",
                "size": 21290,
                "upload_time": "2022-12-16T18:11:57",
                "upload_time_iso_8601": "2022-12-16T18:11:57.887688Z",
                "url": "https://files.pythonhosted.org/packages/13/da/1c650d093d45772a2883d0465ea6a50bdb16d4142a85520517e1a52c0b6b/Attention_and_Transformers-0.0.14-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "4a6d9d7feb0cbed48d3e00cc1c8be2aa",
                    "sha256": "1c0d495d9576a727cca5b2abc2137a80138de2e10430a566e79cc65dfd96bad5"
                },
                "downloads": -1,
                "filename": "Attention_and_Transformers-0.0.14.tar.gz",
                "has_sig": false,
                "md5_digest": "4a6d9d7feb0cbed48d3e00cc1c8be2aa",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.7,<3.11.*",
                "size": 13858,
                "upload_time": "2022-12-16T18:11:59",
                "upload_time_iso_8601": "2022-12-16T18:11:59.422573Z",
                "url": "https://files.pythonhosted.org/packages/57/04/b11bd785472a5ca2f7a1133c04fd8c9421cd7d1207f331c2d2f47598699a/Attention_and_Transformers-0.0.14.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "0.0.8": {
        "info": {
            "author": "Vaibhav Singh",
            "author_email": "vaibhav.singh.3001@gmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 3 - Alpha",
                "Intended Audience :: Developers",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: Apache Software License",
                "Programming Language :: Python :: 3.10",
                "Programming Language :: Python :: 3.9",
                "Topic :: Scientific/Engineering",
                "Topic :: Scientific/Engineering :: Artificial Intelligence",
                "Topic :: Software Development",
                "Topic :: Software Development :: Libraries",
                "Topic :: Software Development :: Libraries :: Python Modules"
            ],
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/veb-101/Attention-and-Transformers",
            "keywords": "tensorflow keras attention transformers",
            "license": "Apache 2.0",
            "maintainer": "",
            "maintainer_email": "",
            "name": "Attention-and-Transformers",
            "package_url": "https://pypi.org/project/Attention-and-Transformers/",
            "platform": null,
            "project_url": "https://pypi.org/project/Attention-and-Transformers/",
            "project_urls": {
                "Homepage": "https://github.com/veb-101/Attention-and-Transformers"
            },
            "release_url": "https://pypi.org/project/Attention-and-Transformers/0.0.8/",
            "requires_dist": [
                "tensorflow-datasets",
                "livelossplot",
                "Pillow",
                "opencv-contrib-python",
                "pandas",
                "scikit-learn",
                "matplotlib",
                "scikit-image",
                "tensorflow-addons ; platform_machine != \"aarch64\" and platform_machine != \"aarch32\"",
                "tensorflow ; platform_system != \"Darwin\"",
                "tensorflow-macos ; platform_system == \"Darwin\""
            ],
            "requires_python": ">=3.9,<3.11.*",
            "summary": "Building attention mechanisms and Transformer models from scratch. Alias ATF.",
            "version": "0.0.8",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16129021,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "5955565e6c75118f26377d5a60f13f70",
                    "sha256": "1898fb1e687bd72eea610fed3735cca6a2f4c9227a414dfb2932cc26dcdb8555"
                },
                "downloads": -1,
                "filename": "Attention_and_Transformers-0.0.8-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "5955565e6c75118f26377d5a60f13f70",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.9,<3.11.*",
                "size": 21190,
                "upload_time": "2022-12-16T16:37:13",
                "upload_time_iso_8601": "2022-12-16T16:37:13.009138Z",
                "url": "https://files.pythonhosted.org/packages/ad/a8/1aa838210bf384b8aeaf4009420e6c0a8f195decd8f59a411dee5ad26d67/Attention_and_Transformers-0.0.8-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "221c1e0eb86cf19d518f497f6475f45f",
                    "sha256": "ef774d1590623d449d452d79bf6b5a148b3608ad1647578cf227f97b353d6616"
                },
                "downloads": -1,
                "filename": "Attention_and_Transformers-0.0.8.tar.gz",
                "has_sig": false,
                "md5_digest": "221c1e0eb86cf19d518f497f6475f45f",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.9,<3.11.*",
                "size": 13789,
                "upload_time": "2022-12-16T16:37:14",
                "upload_time_iso_8601": "2022-12-16T16:37:14.514837Z",
                "url": "https://files.pythonhosted.org/packages/a8/98/8406d632ac2a38e399fb04aeb4a01f332252b4a4f04e634beea3fa52be28/Attention_and_Transformers-0.0.8.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "0.0.9": {
        "info": {
            "author": "Vaibhav Singh",
            "author_email": "vaibhav.singh.3001@gmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 3 - Alpha",
                "Intended Audience :: Developers",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: Apache Software License",
                "Programming Language :: Python :: 3.10",
                "Programming Language :: Python :: 3.9",
                "Topic :: Scientific/Engineering",
                "Topic :: Scientific/Engineering :: Artificial Intelligence",
                "Topic :: Software Development",
                "Topic :: Software Development :: Libraries",
                "Topic :: Software Development :: Libraries :: Python Modules"
            ],
            "description": "## Attention mechanisms and Transformers\r\n\r\n![PyPI - Python Version](https://img.shields.io/pypi/pyversions/Attention_and_Transformers) [![PyPI version](https://badge.fury.io/py/Attention-and-Transformers.svg)](https://badge.fury.io/py/Attention-and-Transformers) [![TensorFlow 2.10.0](https://img.shields.io/badge/TensorFlow-2.10\\|2.11-FF6F00?logo=tensorflow)](https://github.com/tensorflow/tensorflow/releases/tag/v2.10.0) [![TensorFlow](https://img.shields.io/badge/TensorFlow-%23FF6F00.svg?style=for-the-badge&logo=TensorFlow&logoColor=white)](https://www.tensorflow.org/)\r\n\r\n* This goal of this repository is to host basic architecture and model traning code associated with the different attention mechanisms and transformer architecture.\r\n* At the moment, I more interested in learning and recreating these new architectures from scratch than full-fledged training. For now, I'll just be training these models on small datasets.\r\n\r\n#### Installation\r\n\r\n* Using pip to install from [pypi](https://pypi.org/project/Attention-and-Transformers/)\r\n\r\n```bash\r\npip install Attention-and-Transformers\r\n```\r\n\r\n* Using pip to install latest version from github\r\n\r\n```bash\r\npip install git+https://github.com/veb-101/Attention-and-Transformers.git\r\n```\r\n\r\n* Local clone and install\r\n\r\n```bash\r\ngit clone https://github.com/veb-101/Attention-and-Transformers.git atf\r\ncd atf\r\npython setup.py install\r\n```\r\n\r\n**Test Installation**\r\n\r\n```bash\r\npython load_test.py\r\n```\r\n\r\n**Attention Mechanisms**\r\n\r\n<table>\r\n<thead>\r\n<tr>\r\n<th style=\"text-align:center\">\r\n<strong># No.</strong>\r\n</th>\r\n<th style=\"text-align:center\">\r\n<strong>Mechanism</strong>\r\n</th>\r\n<th style=\"text-align:center\">\r\n<strong>Paper</strong>\r\n</th>\r\n</tr>\r\n</thead>\r\n<tbody>\r\n\r\n<tr>\r\n<td style=\"text-align:center\">1</td>\r\n<td style=\"text-align:center\">\r\n<a href=\"https://github.com/veb-101/Attention-and-Transformers/blob/main/Attention_and_Transformers/ViT/multihead_self_attention.py\">Multi-head Self Attention</a>\r\n</td>\r\n<td style=\"text-align:center\">\r\n<a href=\"https://arxiv.org/abs/1706.03762\">Attention is all you need</a>\r\n</td>\r\n</tr>\r\n<tr>\r\n<td style=\"text-align:center\">2</td>\r\n<td style=\"text-align:center\">\r\n<a href=\"https://github.com/veb-101/Attention-and-Transformers/blob/main/Attention_and_Transformers/MobileViT_v1/multihead_self_attention_2D.py\">Multi-head Self Attention 2D</a>\r\n</td>\r\n<td style=\"text-align:center\">\r\n<a href=\"https://arxiv.org/abs/2110.02178\">MobileViT V1</a>\r\n</td>\r\n</tr>\r\n<tr>\r\n<td style=\"text-align:center\">2</td>\r\n<td style=\"text-align:center\">\r\n<a href=\"https://github.com/veb-101/Attention-and-Transformers/blob/main/Attention_and_Transformers/MobileViT_v2/linear_attention.py\">Separable Self Attention</a>\r\n</td>\r\n<td style=\"text-align:center\">\r\n<a href=\"https://arxiv.org/abs/2206.02680\">MobileViT V2</a>\r\n</td>\r\n</tr>\r\n</tbody>\r\n</table>\r\n\r\n**Transformer Models**\r\n\r\n<table>\r\n<thead>\r\n<tr>\r\n<th style=\"text-align:center\">\r\n<strong># No.</strong>\r\n</th>\r\n<th style=\"text-align:center\">\r\n<strong>Models</strong>\r\n</th>\r\n<th style=\"text-align:center\">\r\n<strong>Paper</strong>\r\n</th>\r\n</tr>\r\n</thead>\r\n<tbody>\r\n<tr>\r\n<td style=\"text-align:center\">1</td>\r\n<td style=\"text-align:center\">\r\n<a href=\"https://github.com/veb-101/Attention-and-Transformers/blob/main/Attention_and_Transformers/ViT/vision_transformer.py\">Vision Transformer</a>\r\n</td>\r\n<td style=\"text-align:center\">\r\n<a href=\"https://arxiv.org/abs/2010.11929\">An Image is Worth 16x16 Words:</a>\r\n</td>\r\n</tr>\r\n<tr>\r\n<td style=\"text-align:center\">2</td>\r\n<td style=\"text-align:center\">\r\n<a href=\"https://github.com/veb-101/Attention-and-Transformers/blob/main/Attention_and_Transformers/MobileViT_v1/mobile_vit_v1.py\">MobileViT-V1</a>\r\n</td>\r\n<td style=\"text-align:center\">\r\n<a href=\"https://arxiv.org/abs/2110.02178\">MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer</a>\r\n</td>\r\n</tr>\r\n<tr>\r\n<td style=\"text-align:center\">3</td>\r\n<td style=\"text-align:center\">MobileViT-V2 (under development)</td>\r\n<td style=\"text-align:center\">\r\n<a href=\"https://arxiv.org/abs/2206.02680\">Separable Self-attention for Mobile Vision Transformers</a>\r\n</td>\r\n</tr>\r\n</tbody>\r\n</table>\r\n",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/veb-101/Attention-and-Transformers",
            "keywords": "tensorflow keras attention transformers",
            "license": "Apache 2.0",
            "maintainer": "",
            "maintainer_email": "",
            "name": "Attention-and-Transformers",
            "package_url": "https://pypi.org/project/Attention-and-Transformers/",
            "platform": null,
            "project_url": "https://pypi.org/project/Attention-and-Transformers/",
            "project_urls": {
                "Homepage": "https://github.com/veb-101/Attention-and-Transformers"
            },
            "release_url": "https://pypi.org/project/Attention-and-Transformers/0.0.9/",
            "requires_dist": [
                "livelossplot",
                "Pillow",
                "opencv-contrib-python",
                "pandas",
                "scikit-learn",
                "matplotlib",
                "scikit-image",
                "tensorflow-addons ; platform_machine != \"aarch64\" and platform_machine != \"aarch32\"",
                "tensorflow ; platform_system != \"Darwin\"",
                "tensorflow-macos ; platform_system == \"Darwin\""
            ],
            "requires_python": ">=3.9,<3.11.*",
            "summary": "Building attention mechanisms and Transformer models from scratch. Alias ATF.",
            "version": "0.0.9",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16129021,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "6ff9cc62a6053719749f0e54fd6f5f13",
                    "sha256": "4ab6a68901d8e4a4a2ea1d94f7d1159b16c16433e391fc9ed832cfb5bf8f1357"
                },
                "downloads": -1,
                "filename": "Attention_and_Transformers-0.0.9-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "6ff9cc62a6053719749f0e54fd6f5f13",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.9,<3.11.*",
                "size": 21188,
                "upload_time": "2022-12-16T16:59:16",
                "upload_time_iso_8601": "2022-12-16T16:59:16.098612Z",
                "url": "https://files.pythonhosted.org/packages/39/80/b4b9dfa15bca93f4a1053021758960593096a7a1dcc93c4606dcf4da7565/Attention_and_Transformers-0.0.9-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "7ad14ab184681b576b393a3af6c8bb65",
                    "sha256": "1a3ed456ad6be647a299f0a6aacf618a64acdaaa8e445511b175137277287fb0"
                },
                "downloads": -1,
                "filename": "Attention_and_Transformers-0.0.9.tar.gz",
                "has_sig": false,
                "md5_digest": "7ad14ab184681b576b393a3af6c8bb65",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.9,<3.11.*",
                "size": 13802,
                "upload_time": "2022-12-16T16:59:18",
                "upload_time_iso_8601": "2022-12-16T16:59:18.215511Z",
                "url": "https://files.pythonhosted.org/packages/f7/32/9dae3f53230f3460af436b3ddabbdef2665a3bd399fc067537dbe49fd904/Attention_and_Transformers-0.0.9.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}