{
    "0.9.1": {
        "info": {
            "author": "",
            "author_email": "Pietro d Alessandro <pietrodalessandro@gmail.com>",
            "bugtrack_url": null,
            "classifiers": [
                "License :: OSI Approved :: GNU General Public License v3 or later (GPLv3+)",
                "Operating System :: OS Independent",
                "Programming Language :: Python :: 3",
                "Topic :: Scientific/Engineering :: Mathematics"
            ],
            "description": "# AlgoMeter: Tool for developing, testing and measuring optimizers algorithms\nPython benchmark suite environment for optimizer algorithms.    \nHere is a repository where you can find a python implementation of a benchmark environment for optimizer algorithms. \nProduce comparative measures between algorithms  in csv format with effective test function call count. It has a problem function call optimization, so more calls at the same point X count only for one call. Contains a standard library implementation of convex functions and difference of convex functions.  \nThe hope is to have a useful, common tool for the scientific optimization community to experiment and share results.\n\n## problems + algorithms = experiments\n- A problem is a function f where f: R(n) -> R with n called dimension.  \n- f = f1() - f2() difference of convex function where f1, f2: R(n) -> R. \n- 'problems' is a list of problem\n- 'algorithm' is a code that try to find problem's minimum\n- 'experiment' is an algorMeter run with a list of problems and a list of algorithms that produce a result report\n\n## How to use...\n### Implement an algorithm...\nCopy and customize algorithm examples like the following *(there are many included example?.py)*\n\n```python\ndef gradient(p, **kwargs):\n    '''Simple gradient'''\n    for k in p.loop():\n        p.Xkp1 = p.Xk - 1/(k+1) * p.gfXk / np.linalg.norm(p.gfXk) \n```\n\nand refer to the available following system properties\n\n| algorMeter properties | Description\n|-----|-----------|\n|k, p.K | current iteration |\n| p.Xk | current point |\n| p.Xkp1 | next point. **to be set for next iteration** |\n| p.fXk | p.f(p.Xk) = p.f1(p.Xk) - p.f2(p.Xk)  |\n|p.fXkPrev| previous iteration f(x)|\n| p.f1Xk | p.f1(p.Xk) |\n| p.f2Xk | p.f1(p.Xk) |\n| p.gfXk | p.gf(p.Xk) = p.gf1(p.Xk) - p.gf2(p.Xk)  |\n| p.gf1Xk | p.gf1(p.Xk) |\n| p.gf2Xk | p.gf2(p.Xk) |\n| p.optimumPoint | Optimum X |\n| p.optimumValue | p.f(p.optimumPoint) |\n| p.XStart | Start Point |\n\nto determine the p.Xkp1 for the next iteration.  \n...and run it:\n\n```python\ndf, pv = algorMeter(algorithms = [gradient], problems = probList_coax, iterations = 500, absTol=1E-2)\nprint('\\n', pv,'\\n', df)\n```\n\npv and df are pandas dataframe with run result. A .csv file with result is also created in csv folder. \n\n*(see example1.py)*\n\n## AlgoMeter interface\n\n```python\ndef algorMeter(algorithms, problems, tuneParameters = None, iterations = 500, \n    runs = 1, trace = False, dbprint= False, csv = True, savedata = False,\n     absTol =1.E-4, relTol = 1.E-5,  **kwargs):\n```\n- algorithms: algorithms list. *(algoList_simple is available )* \n- problems: problem list. See problems list in example4.py for syntax.   *(probList_base, probList_coax, probList_DCJBKM are available)*\n- tuneParameters = None: see tuneParameters section \n- iterations = 500: max iterations number \n- runs = 1: see random section \n- trace = False: see trace section \n- dbprint= False: see dbprint section \n- csv = True: write a report in csv format in csv folder\n- savedata = False: save data in data folder\n- absTol =1.E-4, relTol = 1.E-5: tolerance used in numpy allClose and isClose\n- **kwargs: python kwargs propagated to algorithms\n\ncall to algorMeter returns two pandas dataframe p1, p2. p2 is a success and fail summary count.\np1 is a detailed report with the following columns.\n- Problem\t\n- Dim\t\n- Algorithm\t\n- Status\t: Success, Fail or Error\n- Iterations\t\n- f(XStar)\t\n- f(BKXStar)\t\n- Delta\t: absolute difference between  f(XStar) and f(BKXStar)\t\n- Seconds\t\n- Start point\t\n- XStar\t: minimum\n- BKXStar : best known minum\n- \\# f1\t# f2 # gf1\t# gf2: effective calls count\n- ... : other columns with count to counter.up utility (see below)\n\n\n###  Stop and success condition\n```python\n    def isSuccess(self):\n        '''return True if experiment success. Reassign it if needed'''\n        return self.Success and bool(np.isclose(self.f(self.XStar), self.optimumValue,atol=self.absTol, rtol= self.relTol)) \n\n    def isHalt(self):\n        '''return True if experiment must stop. Reassign it if needed'''\n        return np.isclose(self.fXk,self.fXkPrev,rtol=self.relTol,atol=self.absTol)  or \\\n                np.allclose (self.gfXk,np.zeros(self.dimension),rtol=self.relTol,atol=self.absTol) \n```\n\ncan be overriden like in\n```python\n    def stop():\n        return bool(np.isclose(p.f(p.Xk), p.optimumValue,atol=p.absTol, rtol= p.relTol)) or \\\n                bool(np.allclose (p.Xk, p.optimumPoint,rtol=p.relTol,atol=p.absTol))\n    \n    p.isHalt = stop\n    p.isSuccess = stop\n\n```\n\n## Problems function call optimization\nAlgometer uses a problems function call optimization system so more calls at the same point X count only for one call. So in algorithm implementation is not necessary to store the previous result in variables to reduce f1, f2, gf1, gf2 function calls. Algometer cache 128 previous calls to obtain such automatic optimization.\n## Problems ready to use\nImporting 'algormeter.libs' probList_base, probList_coax, probList_DCJBKM problems list are available.    \n **probList_DCJBKM** contains ten frequently used unconstrained DC optimization problems, where objective functions are presented as DC (Difference of Convex) functions:\n\ud835\udc53(\ud835\udc65)=\ud835\udc531(\ud835\udc65)\u2212\ud835\udc532(\ud835\udc65).\n [Joki, Bagirov](https://link.springer.com/article/10.1007/s10898-016-0488-3)\n\n **probList_coax**  contain DemMol,Mifflin,LQ,MAXQ,QL,CB2,CB3 convex functions\n\n **probList_base** contains ParAbs, Acad simple functions for algorithms early test.  \n\n See 'ProblemsLib.pdf'\n\n### Counters\nInstruction like \n> counter.up('lb<0', cls='qp')  \n\nis used in order to count events in code, summerized in statistics at the end of experiment as a column, available in dataframe returned by call to algorMeter and in final csv.\nFor the code above a column with count of counter.up calls and head 'qp.lb>0' is produced.  \nAlso are automatically available columns '# f1', '# f2', '# gf1', '# gf1' with effective calls to f1, f2, gf1, gf2\n\n### dbprint = True\nInstruction dbx.print produce print out only if algorMeter call ha option dbptint == True\n> dbx.print('t:',t, 'Xprev:',Xprev, 'f(Xprev):',p.f(Xprev) ).  \n\nNB: If dbprint = True python exceptions are not handled and raised.\n\n### Trace == True\nIf Default.TRACE = True a line with function values are shown as follows in the console for each iteration for algorithms analysis purpose.\n>  Acad-2 k:0,f:-0.420,x:[ 0.7 -1.3],gf:[ 1.4 -0.6],f1:2.670,gf1:[ 3.1 -2.9],f2:3.090,gf2:[ 1.7 -2.3]   \n > Acad-2 k:1,f:-1.816,x:[-1.0004 -0.5712],gf:[-8.3661e-04  8.5750e-01],f1:0.419,gf1:[-2.0013 -0.7137],f2:2.235,gf2:[-2.0004 -1.5712]  \n> Acad-2 k:2,f:-1.754,x:[-0.9995 -1.4962],gf:[ 9.6832e-04 -9.9250e-01],f1:2.361,gf1:[-1.9985 -3.4887],f2:4.115,gf2:[-1.9995 -2.4962]\n\nThese lines represent the path followed by the algorithm for the specific problem.  \nNB: If trace = True python exceptions are not handled and raised.\n\n### tuneParameters\nSome time is necessary tune some parameter combinations.  Procede as follow (See example4.py):\n- Use numeric parameters with TunePar as domain name, like TunePar.alpha in your algo code.\n- Define a list of lists with possible values of tuning parameters as follows:\n```python\ntpar = [ # [name, [values list]]\n    ('TunePar.alpha', [1. + i for i in np.arange(.05,.9,.05)]),\n    # ('TunePar.beta', [1. + i for i in np.arange(.05,.9,.05)]),\n]\n```\n- call algorMeter with csv = True and tuneParameters=<list of parameters values> like tuneParameters=tpar.\n- open csv file produced and analyze the performance of parameters combinations by looking column '# tunePar'. Useful is a pivot table on such column.\n\n## Random start point \nIf algorMeter parameter run is set with a number greater than 1, each algorithm is repeated on the same problem with random start point in range -1 to 1 for all dimensions.\nBy the method setRandom(center, size) random X can be set in [center-size, center+size] interval.  \nSee example5.py\n## Record data \nwith option data == True store in 'npy' folder one file in numpy format, for each experiment with X and Y=f(X) for all iterations.\nIt is a numpy array with:\n> X = data[:,:-1]  \nY = data[:,-1] \n\nFile name is like 'gradient,JB05-50.npy'.  \nThese files are read by viewer.py data visualizer.\n## Minimize\nIn case you need to find the minimum of a problem/function by applying an algorithm developed with algormeter, the minimize method is available. (See example6.py):\n```python\n    p = MyProb(K) \n    status, x, y = p.minimize(myAlgo)\n```\n## Visualizer.py\nRunning visualizer.py produce or updates contour image in folder 'pics' for each experiment with dimension = 2 with data in folder 'npy'.\n\n# Acknowledgment\nAlgometer was inspired and suggested by prof. Manlio Gaudioso of the University of Calabria and made with him.\n# Contributing\nYou can download or fork the repository freely. If you see a mistake you can send me a mail at pietrodalessandro@gmail.com \nIf you open up a ticket, please make sure it describes the problem or feature request fully.\nAny suggestion are welcome.\n# WARNING\nAlgoMeter is still in the early stages of development. \n\n# License\n**If you use AlgoMeter for the preparation of a scientific paper, the citation with a link to this repository would be appreciated.**\n\nThis program is free software: you can redistribute it and/or modify it under\nthe terms of the GNU General Public License as published by the Free Software\nFoundation, either version 3 of the License, or (at your option) any later\nversion.\n\nThis program is distributed in the hope that it will be useful, but WITHOUT ANY\nWARRANTY. \n\n# Dependencies\nPython version at least\n- Python 3.10.6\n\nPackage installable with pip3\n- numpy\n- pandas\n- matplotlib\n\nAlgometer plays well with [Visual Studio Code](https://code.visualstudio.com) and in jupyter\n",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "",
            "keywords": "convex-optimization,difference-convex-function,optimization-algorithms",
            "license": "",
            "maintainer": "",
            "maintainer_email": "",
            "name": "algormeter",
            "package_url": "https://pypi.org/project/algormeter/",
            "platform": null,
            "project_url": "https://pypi.org/project/algormeter/",
            "project_urls": {
                "Bug Tracker": "https://github.com/xedla/algormeter/issues",
                "Homepage": "https://github.com/xedla/algormeter.git"
            },
            "release_url": "https://pypi.org/project/algormeter/0.9.1/",
            "requires_dist": [
                "matplotlib",
                "numpy",
                "pandas"
            ],
            "requires_python": ">=3.10",
            "summary": "Tool for developing, testing and measuring optimizers algorithms",
            "version": "0.9.1",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16239982,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "5cfac3a3c5547d5e8a4d8f06e2417af9",
                    "sha256": "652b9331045219aad5c4a663da86741ba2d2e171cd671576fcfa39f2e2a98224"
                },
                "downloads": -1,
                "filename": "algormeter-0.9.1-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "5cfac3a3c5547d5e8a4d8f06e2417af9",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.10",
                "size": 19234,
                "upload_time": "2022-12-28T16:06:19",
                "upload_time_iso_8601": "2022-12-28T16:06:19.657885Z",
                "url": "https://files.pythonhosted.org/packages/bf/81/220ab98cc81f701e9609e552e0352fdb0b606df0ae5edd5f2e72b9d700a4/algormeter-0.9.1-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "be4a7db74c085aff313fee2edee96958",
                    "sha256": "3088c68642d37894e16550b5e9295356711e746e620cc3e5621babdbecd7af3b"
                },
                "downloads": -1,
                "filename": "algormeter-0.9.1.tar.gz",
                "has_sig": false,
                "md5_digest": "be4a7db74c085aff313fee2edee96958",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.10",
                "size": 704229,
                "upload_time": "2022-12-28T16:06:22",
                "upload_time_iso_8601": "2022-12-28T16:06:22.195768Z",
                "url": "https://files.pythonhosted.org/packages/57/ee/6239e9d30958c599ce1b47fcae341ff61d51cdb5243aa1cc4b45d0955704/algormeter-0.9.1.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}