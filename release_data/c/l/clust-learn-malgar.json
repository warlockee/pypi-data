{
    "0.0.1": {
        "info": {
            "author": "Miguel Alvarez-Garcia, Raquel Ibar-Alonso, Mar Arenas-Parra",
            "author_email": "Miguel Alvarez-Garcia <malvarez.statistics@gmail.com>",
            "bugtrack_url": null,
            "classifiers": [
                "License :: OSI Approved :: GNU General Public License v3 (GPLv3)",
                "Programming Language :: Python :: 3",
                "Programming Language :: Python :: 3.9",
                "Topic :: Scientific/Engineering"
            ],
            "description": "<!-- title -->\n<div align=\"center\">\n  <h3>Clust-learn</h3>\n</div>\n\n<!-- Short description -->\n<p align=\"center\">\n   A Python package for extracting information from large and high-dimensional mixed-type data through explainable cluster analysis.\n</p>\n\n<br/>\n\n<div align=\"center\">\n  <img src=\"images/visualizations.png\" alt=\"clust-learn visualizations\"/>\n</div>\n\n<hr>\n\n## Table of contents\n1. [Introduction](#user-content-introduction)\n2. [Overall architecture](#user-content-architecture)\n3. [Implementation](#user-content-implementation)\n4. [Installation](#user-content-install)\n5. [Version and license information](#user-content-license)\n6. [Bug reports and future work](#user-content-future)\n7. [User guide & API](#user-content-api)\n\t1. [Data processing](#user-content-module-preprocessing)\n\t\t1. [Data imputation](#user-content-module-preprocessing-imputation)\n\t\t2. [Outliers](#user-content-module-preprocessing-outliers)\n\t2. [Dimensionality reduction](#user-content-module-dimensionality)\n\t\t- [DimensionalityReduction class](#DimensionalityReduction_class)\n\t\t- [Class methods](#DimensionalityReduction_class_methods)\n\t3. [Clustering](#user-content-module-clustering)\n\t\t- [Clustering class](#Clustering_class)\n\t\t- [Class methods](#Clustering_class_methods)\n\t4. [Classifier](#user-content-module-classifier)\n\t\t- [Classifier class](#Classifier_class)\n\t\t- [Class methods](#Classifier_class_methods)\n8. [Citing](#user-content-citing)\n\n<h2 id=\"user-content-introduction\">\n1. Introduction\n</h2>\n\n`clust-learn` enables users to run end-to-end explainable cluster analysis to extract information from large and high-dimensional\nmixed-type data, and it does so by providing a framework that guides the user through data preprocessing, dimensionality reduction, \nclustering, and classification of the obtained clusters. It is designed to require very few lines of code, and with a strong\nfocus on explainability.\n\n<h2 id=\"user-content-architecture\">\n2. Overall architecture\n</h2>\n\n`clust-learn` is organized into four modules, one for each component of the methodological framework presented [here](#user-content-citing): \n* [data_preprocessing](https://github.com/malgar/clust-learn/tree/master/clearn/data_preprocessing)\n* [dimensionality_reduction](https://github.com/malgar/clust-learn/tree/master/clearn/dimensionality_reduction)\n* [clustering](https://github.com/malgar/clust-learn/tree/master/clearn/clustering)\n* [classifier](https://github.com/malgar/clust-learn/tree/master/clearn/classifier)\n\n**Figue 1** shows the package layout with the functionalities covered by each module along with the techniques used, the\nexplainability strategies available, and the main functions and class methods encapsulating these techniques and\nexplainability strategies.\n\n<br/>\n\n<div align=\"center\">\n  <img src=\"images/package_structure.png\" alt=\"clust-learn package structure\"/>\n</div>\n\n<br/>\n\n<h2 id=\"user-content-implementation\">\n3. Implementation\n</h2>\n\nThe package is implemented with Python 3.9 using open source libraries. It relies heavily on [pandas](https://pandas.pydata.org/) and\n[scikit-learn](https://scikit-learn.org/stable/). Read the complete list of requirements [here](https://github.com/malgar/clust-learn/blob/master/requirements.txt).\n\nIt can be installed manually or from pip/PyPI (see Section [4. Installation](#user-content-install)).\n\n<h2 id=\"user-content-install\">\n4. Installation\n</h2>\n\nThe package is on [PyPI]([!!link]). Simply run:\n\n```\npip install clust-learn\n```\n\n<h2 id=\"user-content-license\">\n5. Version and license information\n</h2>\n\n* Version: 1.0\n* Author: Miguel Alvarez (@gmail.com)\n* License: GPLv3 \n\n<h2 id=\"user-content-future\">\n6. Bug reports and future work\n</h2>\n\nPlease report bugs and feature requests through creating a new issue [here](https://github.com/malgar/clust-learn/issues).\n\n<h2 id=\"user-content-api\">\n7. User guide & API\n</h2>\n\n`clust-learn` is organized into four modules:\n\n1. Data preprocessing\n2. Dimensionality reduction\n3. Clustering\n4. Classifier\n\n**Figue 1** shows the package layout with the functionalities covered by each module along with the techniques used, the explainability strategies available, and the main functions and class methods encapsulating these techniques and explainability strategies.\n\nThe four modules are designed to be used sequentially to ensure robust and explainable results. However, each of them is independent and can be used separately to suit different use cases.\n\n\n<h3 id=\"user-content-module-preprocessing\">\n7.i. Data preprocessing\n</h3>\n\nData preprocessing consists of a set of manipulation and transformation tasks performed on the raw data before it is used for its analysis. Although data quality is essential for obtaining robust and reliable results, real-world data is often incomplete, noisy, or inconsistent. Therefore, data preprocessing is a crucial step in any analytical study.\n\n<h4 id=\"user-content-module-preprocessing-imputation\">\n7.i.a. Data imputation\n</h4>\n\n<h4 id=\"compute_missing\">\ncompute_missing()\n</h4>\n\n```\ncompute_missing(df, normalize=True)\n```\n\nCalculates the pct/count of missing values per column.\n\n**Parameters**\n\n- `df` : `pandas.DataFrame`\n- `normalize` : `boolean`, default=`True`\n\n**Returns**\n\n- `missing_df` : `pandas.DataFrame`\n\t- DataFrame with the pct/counts of missing values per column.\n\t\n<h4 id=\"missing_values_heatmap\">\nmissing_values_heatmap()\n</h4>\n\n```\nmissing_values_heatmap(df, output_path=None, savefig_kws=None)\n```\n\nPlots a heatmap to visualize missing values (light color).\n\n**Parameters**\n\n- `df` : `pandas.DataFrame`\n   - DataFrame containing the data.\n- `output_path` : `str`, default=`None`\n   - Path to save figure as image.\n- `savefig_kws` : `dict`, default=`None`\n   - Save figure options.\n\n<h4 id=\"impute_missing_values\">\nimpute_missing_values()\n</h4>\n\n```\nimpute_missing_values(df, num_vars, cat_vars, num_pair_kws=None, mixed_pair_kws=None, cat_pair_kws=None, graph_thres=0.05, k=8, max_missing_thres=0.33)\n```\n\nThis function imputes missing values following this steps:\n1. One-to-one model based imputation for strongly related variables.\n2. Cluster based hot deck imputation where clusters are obtained as the connected components of an undirected graph *G=(V,E)*, where *V* is the set of variables and *E* the pairs of variables with mutual information above a predefined threshold.\n3. Records with a proportion of missing values above a predefined threshold are discarded to ensure the quality of the hot deck imputation.\n4. Hot deck imputation for the remaining missing values considering all variables together.\n\n**Parameters**\n\n- `df` : `pandas.DataFrame`\n\t- Data frame containing the data with potential missing values.\n- `num_vars` : `str`, `list`, `pandas.Series`, or `numpy.array`\n\t- Numerical variable name(s).\n- `cat_vars` : `str`, `list`, `pandas.Series`, or `numpy.array`\n\t- Categorical variable name(s).\n- `{num,mixed,cat}_pair_kws` : `dict`, default=`None`\n\t- Additional keyword arguments to pass to compute imputation pairs for one-to-one model based imputation, namely:\n\t\t- For numerical pairs, `corr_thres` and `method` for setting the correlation coefficient threshold and method. By default, `corr_thres=0.7` and `method='pearson'`.\n\t\t- For mixed-type pairs, `np2_thres` for setting the a threshold on partial *eta* square with 0.14 as default value.\n\t\t- For categorical pairs, `mi_thres` for setting a threshold on mutual information score. By default, `mi_thres=0.6`.\n- `graph_thres` : `float`, default=0.05\n\t- Threshold to determine if two variables are similar based on mutual information score, and therefore are an edge of the graph from which variable clusters are derived.\n- `k` : `int`, default=8\n\t- Number of neighbors to consider in hot deck imputation.\n- `max_missing_thres`: `float`, default=0.33\n\t- Maximum proportion of missing values per observation allowed before final general hot deck imputation - see step 3 of the missing value imputation methodology in section 2.1.\n\n**Returns**\n\n- `final_pairs` : `pandas.DataFrame`\n\t- DataFrame with pairs of highly correlated variables (`var1`: variable with values to impute; `var2`: variable to be used as independent variable for model-based imputation), together proportion of missing values of variables `var1` and `var2`.\n   \n<h4 id=\"plot_imputation_distribution_assessment\">\nplot_imputation_distribution_assessment()\n</h4>\n\n```\nplot_imputation_distribution_assessment(df_prior, df_posterior, imputed_vars, sample_frac=1.0, prior_kws=None, posterior_kws=None, output_path=None, savefig_kws=None)\n```\n\nPlots a distribution comparison of each variable with imputed variables, before and after imputation.\n\n**Parameters**\n\n- `df_prior` : `pandas.DataFrame`\n\t- DataFrame containing the data before imputation.\n- `df_posterior` : `pandas.DataFrame`\n\t- DataFrame containing the data after imputation.\n- `imputed_vars` : `list`\n\t- List of variables with imputed variables.\n- `sample_frac` : float, default=1.0\n\t- If < 1 a random sample of every pair of variables will be plotted.\n- `{prior,posterior}_kws` : `dict`, default=`None`\n\t- Additional keyword arguments to pass to the [kdeplot](https://seaborn.pydata.org/generated/seaborn.kdeplot.html).\n- `output_path` : `str`, default=`None`\n\t- Path to save figure as image.\n- `savefig_kws` : `dict`, default=`None`\n\t- Save figure options.\n\n<h4 id=\"user-content-module-preprocessing-outliers\">\n7.i.b. Outliers\n</h4>\n\n<h4 id=\"remove_outliers\">\nremove_outliers()\n</h4>\n\n```\nremove_outliers(df, variables, iforest_kws=None)\n```\n\nRemoves outliers using the [Isolation Forest algorithm](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html).\n\n**Parameters**\n\n- `df` : `pandas.DataFrame`\n\t- DataFrame containing the data.\n- `variables` : `list`\n\t- Variables with potential outliers.\n- `iforest_kws` : `dict`, default=`None`\n\t- IsolationForest algorithm hyperparameters.\n\n**Returns**\n\n- df_inliers : `pandas.DataFrame`\n\t- DataFrame with inliers (i.e. observations that are not outliers).\n- df_outliers : `pandas.DataFrame`\n\t- DataFrame with outliers.\n\n\n<h3 id=\"user-content-module-dimensionality\">\n7.ii. Dimensionality reduction\n</h3>\n\nAll the functionality of this module is encapsulated in the `DimensionalityReduction` class so that the original data, the instances of the models used, and any other relevant information is self-maintained and always accessible.\n\n<h4 id=\"DimensionalityReduction_class\">\nDimensionalityReduction class\n</h4>\n\n```\ndr = DimensionalityReduction(df, num_vars=None, cat_vars=None, num_algorithm='pca', cat_algorithm='mca', num_kwargs=None, cat_kwargs=None)\n```\n\n| Parameter | Type | Description |\n|:-|:-|:-|\n| `df` | `pandas.DataFrame` | Data table containing the data with the original variables |\n| `num_vars` | `string`, `list`, `pandas.Series`, or `numpy.array` | Numerical variable name(s) |\n| `cat_vars` | `string`, `list`, `pandas.Series`, or `numpy.array` | Categorical variable name(s) |\n| `num_algorithm` | `string` | Algorithm to be used for dimensionality reduction of numerical variables. By default, PCA is used. The current version also supports SPCA |\n| `cat_algorithm` | `string` | Algorithm to be used for dimensionality reduction of categorical variables. By default, MCA is used. The current version doesn\u2019t support other algorithms |\n| `num_kwargs` | `dictionary` | Additional keyword arguments to pass to the model used for numerical variables |\n| `cat_kwargs` | `dictionary` | Additional keyword arguments to pass to the model used for categorical variables |\n| **Attribute** | **Type** | **Description** |\n| `n_components_` | `int` | Final number of extracted components |\n| `min_explained_variance_ratio_` | `float` | Minimum explained variance ratio. By default, 0.5 |\n| `num_trans_` | `pandas.DataFrame` | Extracted components from numerical variables |\n| `cat_trans_` | `pandas.DataFrame` | Extracted components from categorical variables |\n| `num_components_` | `list` | List of names assigned to the extracted components from numerical variables |\n| `cat_components_` | `list` | List of names assigned to the extracted components from categorical variables |\n| `pca_` | `sklearn.decomposition.PCA` | PCA instance used to speed up some computations and for comparison purposes |\n\n<h4 id=\"DimensionalityReduction_class_methods\">\nMethods\n</h4>\n\n<h4>transform()</h4>\n\n[Source](https://github.com/malgar/clust-learn/blob/f0744a15823c2b7c6b49c278d08d708d05df952a/clearn/dimensionality_reduction/dimensionality_reduction.py#L79)\n\n```\ntransform(self, n_components=None, min_explained_variance_ratio=0.5)\n```\n\nTransforms a DataFrame df to a lower dimensional space.\n\n<h4>num_main_contributors(()</h4>\n\n[Source](https://github.com/malgar/clust-learn/blob/f0744a15823c2b7c6b49c278d08d708d05df952a/clearn/dimensionality_reduction/dimensionality_reduction.py#L191)\n\n```\nnum_main_contributors(self, thres=0.5, n_contributors=None, dim_idx=None, component_description=None, col_description=None, output_path=None)\n```\n\nComputes the original numerical variables with the strongest relation to the derived variable(s) (measured as Pearson correlation coefficient).\n\n<h4>cat_main_contributors(()</h4>\n\n[Source](https://github.com/malgar/clust-learn/blob/f0744a15823c2b7c6b49c278d08d708d05df952a/clearn/dimensionality_reduction/dimensionality_reduction.py#L225)\n\n```\ncat_main_contributors(self, thres=0.14, n_contributors=None, dim_idx=None, component_description=None, col_description=None, output_path=None)\n```\n\nComputes the original categorical variables with the strongest relation to the derived variable(s)(measured as correlation ratio).\n\n<h4>cat_main_contributors_stats()</h4>\n\n[Source](https://github.com/malgar/clust-learn/blob/f0744a15823c2b7c6b49c278d08d708d05df952a/clearn/dimensionality_reduction/dimensionality_reduction.py#L259)\n\n```\ncat_main_contributors_stats(self, thres=0.14, n_contributors=None, dim_idx=None, output_path=None)\n```\n\nComputes for every categorical variable's value, the mean and std of the derived variables that are strongly related to the categorical variable (based on the correlation ratio)).\n\n<h4>plot_num_explained_variance()</h4>\n\n[Source](https://github.com/malgar/clust-learn/blob/f0744a15823c2b7c6b49c278d08d708d05df952a/clearn/dimensionality_reduction/dimensionality_reduction.py#L286)\n\n```\nplot_num_explained_variance(self, thres=0.5, plots='all', output_path=None, savefig_kws=None)\n```\n\nPlot the explained variance (ratio, cumulative, and/or normalized) for numerical variables.\n\n<h4>plot_cat_explained_variance()</h4>\n\n[Source](https://github.com/malgar/clust-learn/blob/f0744a15823c2b7c6b49c278d08d708d05df952a/clearn/dimensionality_reduction/dimensionality_reduction.py#L303)\n\n```\nplot_cat_explained_variance(self, thres=0.5, plots='all', output_path=None, savefig_kws=None)\n```\n\nPlot the explained variance (ratio, cumulative, and/or normalized) for categorical variables.\n\n<h4>plot_num_main_contributors()</h4>\n\n[Source](https://github.com/malgar/clust-learn/blob/f0744a15823c2b7c6b49c278d08d708d05df952a/clearn/dimensionality_reduction/dimensionality_reduction.py#L321)\n\n```\nplot_num_main_contributors(self, thres=0.5, n_contributors=5, dim_idx=None, output_path=None, savefig_kws=None)\n```\n\nPlot main contributors (original variables with the strongest relation with derived variables) for every derived variable.\n\n<h4>plot_cat_main_contributor_distribution()</h4>\n\n[Source](https://github.com/malgar/clust-learn/blob/f0744a15823c2b7c6b49c278d08d708d05df952a/clearn/dimensionality_reduction/dimensionality_reduction.py#L344)\n\n```\nplot_cat_main_contributor_distribution(self, thres=0.14, n_contributors=None, dim_idx=None, output_path=None, savefig_kws=None)\n```\n\nPlot main contributors (original variables with the strongest relation with derived variables) for every derived variable.\n\n<h3 id=\"user-content-module-clustering\">\n7.iii. Clustering\n</h3>\n\nThe `Clustering` class encapsulates all the functionality of this module and stores the data, the instances of the algorithms used, and other relevant information so it is always accessible.\n\n<h4 id=\"Clustering_class\">\nClustering class\n</h4>\n\n```\ncl = Clustering(df, algorithms='kmeans', normalize=False)\n```\n\n| Parameter | Type | Description |\n|:-|:-|:-|\n| `df` | `pandas.DataFrame` | Data frame containing the data to be clustered |\n| `algorithms` | `string` or `list` | Algorithms to be used for clustering. The current version supports k-means and agglomerative clustering |\n| `normalize` | `bool` | Whether to apply data normalization for fair comparisons between variables. In case dimensionality reduction is applied beforehand, normalization should not be applied |\n| **Attribute** | **Type** | **Description** |\n| `dimensions_` | `list` | List of columns of they input data frame |\n| `instances_` | `dict` | Pairs of algorithm name and its instance |\n| `metric_` | `string` | The cluster validation metric used. Four metrics available: ['inertia', 'davies_bouldin_score', 'silhouette_score',  'calinski_harabasz_score'] |\n| `optimal_config_` | `tuple` | Tuple with the optimal configuration for clustering containing the algorithm name, number of clusters, and value of the chosen validation metric |\n| `scores_` | `dict` | Pairs of algorithm name and a list of values of the chosen validation metric for a cluster range |\n\n<h4 id=\"Clustering_class_methods\">\nMethods\n</h4>\n\n<h4>compute_clusters()</h4>\n\n[Source](https://github.com/malgar/clust-learn/blob/be4a2238670af01023bd419a0f8adaa7f9cee9f6/clearn/clustering/clustering.py#L117)\n\n```\ncompute_clusters(self, n_clusters=None, metric='inertia', max_clusters=10, prefix=None, weights=None)\n```\n\nCalculates clusters.\nIf more than one algorithm is passed in the class constructor, first, the optimal number of clusters\nis computed for each algorithm based on the metric passed to the method. Secondly, the algorithm that\nprovides the best performance for the corresponding optimal number of clusters is selected.\nTherefore, the result shows the clusters calculated with the best performing algorithm based on the\ncriteria explained above.\n\n<h4 id=\"describe_clusters\">\ndescribe_clusters()\n</h4>\n\n[Source](https://github.com/malgar/clust-learn/blob/be4a2238670af01023bd419a0f8adaa7f9cee9f6/clearn/clustering/clustering.py#L182)\n\n```\ndescribe_clusters(self, df_ext=None, variables=None, cluster_filter=None, statistics=['mean', 'median', 'std'], output_path=None)\n```\n\nDescribes clusters based on internal or external *continuous* variables.\nFor categorical variables use [`describe_clusters_cat()`](#describe_clusters_cat).\n\n<h4 id=\"describe_clusters_cat\">\ndescribe_clusters_cat()\n</h4>\n\n[Source](https://github.com/malgar/clust-learn/blob/be4a2238670af01023bd419a0f8adaa7f9cee9f6/clearn/clustering/clustering.py#L237)\n\n```\ndescribe_clusters_cat(self, cat_array, cat_name=None, order=None, normalize=False, output_path=None)\n```\n\nDescribes clusters based on  external *categorical* variables. The result is a contingency table.\nFor continuous variables use [`describe_clusters()`](#describe_clusters).\n\n<h4>compare_cluster_means_to_global_means()</h4>\n\n[Source](https://github.com/malgar/clust-learn/blob/be4a2238670af01023bd419a0f8adaa7f9cee9f6/clearn/clustering/clustering.py#L276)\n\n```\ncompare_cluster_means_to_global_means(self, df_original=None, output_path=None)\n```\n\nFor every cluster and every internal variable, the relative difference between the intra-cluster mean\nand the global mean.\n\n<h4>anova_tests()</h4>\n\n[Source](https://github.com/malgar/clust-learn/blob/be4a2238670af01023bd419a0f8adaa7f9cee9f6/clearn/clustering/clustering.py#L303)\n\n```\nanova_tests(self, df_test=None, vars_test=None, cluster_filter=None, output_path=None)\n```\n\nRuns ANOVA tests for a given set of continuous variables (internal or external) to test dependency with clusters.\n\n<h4>chi2_test()</h4>\n\n[Source](https://github.com/malgar/clust-learn/blob/be4a2238670af01023bd419a0f8adaa7f9cee9f6/clearn/clustering/clustering.py#L360)\n\n```\nchi2_test(self, cat_array)\n```\n\nRuns Chi-squared tests for a given categorical variable to test dependency with clusters.\n\n<h4>plot_score_comparison()</h4>\n\n[Source](https://github.com/malgar/clust-learn/blob/be4a2238670af01023bd419a0f8adaa7f9cee9f6/clearn/clustering/clustering.py#L379)\n\n```\nplot_score_comparison(self, output_path=None, savefig_kws=None)\n```\n\nPlots the comparison in performance between the different clustering algorithms.\n\n<h4>plot_optimal_components_normalized()</h4>\n\n[Source](https://github.com/malgar/clust-learn/blob/be4a2238670af01023bd419a0f8adaa7f9cee9f6/clearn/clustering/clustering.py#L400)\n\n```\nplot_optimal_components_normalized(self, output_path=None, savefig_kws=None)\n```\n\nPlots the normalized curve used for computing the optimal number of clusters.\n\n<h4>plot_clustercount()</h4>\n\n[Source](https://github.com/malgar/clust-learn/blob/be4a2238670af01023bd419a0f8adaa7f9cee9f6/clearn/clustering/clustering.py#L419)\n\n```\nplot_clustercount(self, output_path=None, savefig_kws=None)\n```\n\nPlots a bar plot with cluster counts.\n\n<h4>plot_cluster_means_to_global_means_comparison()</h4>\n\n[Source](https://github.com/malgar/clust-learn/blob/be4a2238670af01023bd419a0f8adaa7f9cee9f6/clearn/clustering/clustering.py#L432)\n\n```\nplot_cluster_means_to_global_means_comparison(self, df_original=None, xlabel=None, ylabel=None,\n                                              levels=[-0.50, -0.32, -0.17, -0.05, 0.05, 0.17, 0.32, 0.50],\n                                              output_path=None, savefig_kws=None)\n```\n\nPlots the normalized curve used for computing the optimal number of clusters.\n\n<h4>plot_distribution_comparison_by_cluster()</h4>\n\n[Source](https://github.com/malgar/clust-learn/blob/be4a2238670af01023bd419a0f8adaa7f9cee9f6/clearn/clustering/clustering.py#L469)\n\n```\nplot_distribution_comparison_by_cluster(self, df_ext=None, xlabel=None, ylabel=None, output_path=None, savefig_kws=None)\n```\n\nPlots the violin plots per cluster and *continuous* variables of interest to understand differences in their distributions by cluster.\n\n<h4>plot_clusters_2D()</h4>\n\n[Source](https://github.com/malgar/clust-learn/blob/be4a2238670af01023bd419a0f8adaa7f9cee9f6/clearn/clustering/clustering.py#L498)\n\n```\nplot_clusters_2D(self, coor1, coor2, style_kwargs=dict(), output_path=None, savefig_kws=None)\n```\n\nPlots two 2D plots:\n\t - A scatter plot styled by the categorical variable `hue`.\n\t - A 2D plot comparing cluster centroids and optionally the density area.\n\t \n<h4>plot_cat_distribution_by_cluster()</h4>\n\n[Source](https://github.com/malgar/clust-learn/blob/be4a2238670af01023bd419a0f8adaa7f9cee9f6/clearn/clustering/clustering.py#L545)\n\n```\nplot_cat_distribution_by_cluster(self, cat_array, cat_label=None, cluster_label=None, output_path=None, savefig_kws=None)\n```\n\nPlots the relative contingency table of the clusters with a categorical variable as a stacked bar plot.\n\n<h3 id=\"user-content-module-classifier\">\n7.iv. Classifier\n</h3>\n\nThe functionality of this module is encapsulated in the `Classifier` class, which is also responsible for storing the original data, the instances of the models used, and any other relevant information.\n\n<h4 id=\"Classifier_class\">\nClassifier class\n</h4>\n\n```\nclassifier = Classifier(df, predictor_cols, target, num_cols=None, cat_cols=None)\n```\n\n| Parameter | Type | Description |\n|:-|:-|:-|\n| `df` | `pandas.DataFrame` | Data frame containing the data |\n| `predictor_cols` | `list` of `string` | List of columns to use as predictors |\n| `target` | `numpy.array` or `list` | Values of the target variable |\n| `num_cols` | `list` | List of numerical columns from predictor_cols |\n| `cat_cols` | `list` | List of categorical columns from predictor_cols |\n| **Attribute** | **Type** | **Description** |\n| `filtered_features_` | `list` | List of columns of the input data frame |\n| `model_` | Instance of `TransformerMixin` and `BaseEstimator` from `sklearn.base` | Trained classifier |\n| `X_train_` | `numpy.array` | Train split of predictors |\n| `X_test_` | `numpy.array` | Test split of predictors |\n| `y_train_` | `numpy.array` | Train split of target |\n| `y_test_` | `numpy.array` | Test split of target |\n| `grid_result_` | `sklearn.model_selection.GridSearchCV` | Instance of fitted estimator for hyperparameter tuning |\n\n<h4 id=\"Classifier_class_methods\">\nMethods\n</h4>\n\n<h4>train_model()</h4>\n\n[Source](https://github.com/malgar/clust-learn/blob/5826ef273eb876c961eab7fa4eacb31caff25ef0/clearn/classifier/classifier.py#L52)\n\n```\ntrain_model(self, model=None, feature_selection=True, features_to_keep=[],\n\t\t\tfeature_selection_model=None, hyperparameter_tuning=False, param_grid=None,\n\t\t\ttrain_size=0.8)\n```\n\nThis method trains a classification model.\n\nBy default, it uses XGBoost, but any other estimator (instance of `scikit-learn.Estimator`) can be used.\n\nThe building process consists of three main steps:\n - Feature Selection (optional)\n \nFeature removing highly correlated variables using a classification model and SHAP values\nto determine which to keep, and Recursive Feature Elimination with Cross-Validation (RFECV)\non the remaining features.\n\n - Hyperparameter tuning (optional)\n \nRuns grid search with cross-validation for hyperparameter tuning. **Note** the parameter grid\nmust be passed.\n\n - Model training\n \nTrains a classification model with the selected features and hyperparameters. By default, an XGBoost\nclassifier will be trained.\n   \n**Note** both hyperparameter tuning and model training are run on a train set. Train-test split is performed\nusing `sklearn.model_selection.train_test_split`.\n\n<h4>hyperparameter_tuning_metrics()</h4>\n\n[Source](https://github.com/malgar/clust-learn/blob/5826ef273eb876c961eab7fa4eacb31caff25ef0/clearn/classifier/classifier.py#L134)\n\n```\nhyperparameter_tuning_metrics(self, output_path=None)\n```\n\nThis method returns the average and standard deviation of the cross-validation runs for every hyperparameter\ncombination in hyperparameter tuning.\n\n<h4>confusion_matrix()</h4>\n\n[Source](https://github.com/malgar/clust-learn/blob/5826ef273eb876c961eab7fa4eacb31caff25ef0/clearn/classifier/classifier.py#L154)\n\n```\nconfusion_matrix(self, test=True, sum_stats=True, output_path=None)\n```\n\nThis method returns the confusion matrix of the classification model.\n\n<h4>classification_report()</h4>\n\n[Source](https://github.com/malgar/clust-learn/blob/5826ef273eb876c961eab7fa4eacb31caff25ef0/clearn/classifier/classifier.py#L195)\n\n```\nclassification_report(self, test=True, output_path=None)\n```\n\nThis method returns the `sklearn.metrics.classification_report` in `pandas.DataFrame` format.\n\nThis report contains the intra-class metrics precision, recall and F1-score, together with the global accuracy,\nand macro average and weighted average of the three intra-class metrics.\n\n<h4>plot_shap_importances()</h4>\n\n[Source](https://github.com/malgar/clust-learn/blob/5826ef273eb876c961eab7fa4eacb31caff25ef0/clearn/classifier/classifier.py#L225)\n\n```\nplot_shap_importances(self, n_top=7, output_path=None, savefig_kws=None)\n```\n\nPlots shap importance values, calculated as the combined average of the absolute values of the shap values\nfor all classes.\n\n<h4>plot_shap_importances_beeswarm()</h4>\n\n[Source](https://github.com/malgar/clust-learn/blob/5826ef273eb876c961eab7fa4eacb31caff25ef0/clearn/classifier/classifier.py#L241)\n\n```\nplot_shap_importances_beeswarm(self, class_id, n_top=10, output_path=None, savefig_kws=None)\n```\n\nPlots a summary of shap values for a specific class of the target variable. This uses [shap beeswarm plot](https://shap.readthedocs.io/en/latest/example_notebooks/api_examples/plots/beeswarm.html).\n\n<h4>plot_confusion_matrix()</h4>\n\n[Source](https://github.com/malgar/clust-learn/blob/5826ef273eb876c961eab7fa4eacb31caff25ef0/clearn/classifier/classifier.py#L260)\n\n```\nplot_confusion_matrix(self, test=True, sum_stats=True, output_path=None, savefig_kws=None)\n```\n\nThis function makes a pretty plot of an sklearn Confusion Matrix cf using a Seaborn heatmap visualization.\n\n<h4>plot_roc_curves()</h4>\n\n[Source](https://github.com/malgar/clust-learn/blob/5826ef273eb876c961eab7fa4eacb31caff25ef0/clearn/classifier/classifier.py#L280)\n\n```\n plot_roc_curves(self, test=True, output_path=None, savefig_kws=None)\n```\n\nPlots ROC curve for every class.\n\n<h2 id=\"user-content-citing\">\n8. Citing\n</h2>\n\n<<TO-DO>>\n\n\n",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/malgar/clust-learn",
            "keywords": "",
            "license": "GPLv3",
            "maintainer": "",
            "maintainer_email": "",
            "name": "clust-learn-malgar",
            "package_url": "https://pypi.org/project/clust-learn-malgar/",
            "platform": null,
            "project_url": "https://pypi.org/project/clust-learn-malgar/",
            "project_urls": {
                "Bug Tracker": "https://github.com/malgar/clust-learn/issues",
                "Homepage": "https://github.com/malgar/clust-learn"
            },
            "release_url": "https://pypi.org/project/clust-learn-malgar/0.0.1/",
            "requires_dist": [
                "kneed (==0.7.0)",
                "matplotlib (==3.4.3)",
                "networkx (==2.6.3)",
                "numpy (==1.20.3)",
                "pandas (==1.3.4)",
                "pingouin (==0.5.1)",
                "prince (==0.7.0)",
                "scikit-learn (==1.0.2)",
                "scipy (==1.7.1)",
                "seaborn (==0.11.2)",
                "shap (==0.40.0)",
                "statsmodels (==0.13.2)",
                "xgboost (==1.5.2)"
            ],
            "requires_python": ">=3.9",
            "summary": "A Python package for explainable cluster analysis",
            "version": "0.0.1",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16037787,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "8268883c19e323babbdb589b7055b6a4",
                    "sha256": "1b223036281d21d685a5ae0b54f8db029ce207b474e5c63c11ee9e4be41a0f4a"
                },
                "downloads": -1,
                "filename": "clust_learn_malgar-0.0.1-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "8268883c19e323babbdb589b7055b6a4",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.9",
                "size": 62521,
                "upload_time": "2022-12-08T17:25:57",
                "upload_time_iso_8601": "2022-12-08T17:25:57.796678Z",
                "url": "https://files.pythonhosted.org/packages/61/b0/9dad6ddc2e9170952eac6500ac4a6326da6e02467a20b72f2c869e51dc57/clust_learn_malgar-0.0.1-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "d3333351470f93fc83c47ebb7b6e5906",
                    "sha256": "06c83b798e69101ba38d4ec2c2584b97478d9091077967a670b0d39c5556aa45"
                },
                "downloads": -1,
                "filename": "clust-learn_malgar-0.0.1.tar.gz",
                "has_sig": false,
                "md5_digest": "d3333351470f93fc83c47ebb7b6e5906",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.9",
                "size": 59246,
                "upload_time": "2022-12-08T17:25:59",
                "upload_time_iso_8601": "2022-12-08T17:25:59.446246Z",
                "url": "https://files.pythonhosted.org/packages/43/79/4d4f534c5055bda7f2f40ae7f29f90b6f3bf7495bcccf27b988826d1f043/clust-learn_malgar-0.0.1.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}