{
    "1.0.1": {
        "info": {
            "author": "Mehdi Cherti",
            "author_email": "mehdicherti@gmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 2 - Pre-Alpha",
                "Intended Audience :: Developers",
                "License :: OSI Approved :: MIT License",
                "Natural Language :: English",
                "Programming Language :: Python :: 3",
                "Programming Language :: Python :: 3.6",
                "Programming Language :: Python :: 3.7",
                "Programming Language :: Python :: 3.8"
            ],
            "description": "# CLIP Benchmark\n\n\nThe goal of this repo is to evaluate CLIP-like models on a standard set\nof datasets on different tasks such as zero-shot classification and zero-shot\nretrieval.\n\nBelow we show the average rank (1 is the best, lower is better) of different CLIP models, evaluated\non different datasets.\n\n![benchmark.png](benchmark.png)\n\nThe current detailed results of the benchmark can be seen [here](benchmark/README.md)\nor directly in the [notebook](benchmark/results.ipynb).\n\n## Features\n\n* Support for zero-shot classification and zero-shot retrieval\n* Support for [OpenCLIP](https://github.com/mlfoundations/open_clip) pre-trained models\n* Support various datasets from [torchvision](https://pytorch.org/vision/stable/datasets.html), [tensorflow datasets](https://www.tensorflow.org/datasets), and [VTAB](https://github.com/google-research/task_adaptation).\n\n\n## How to install?\n\n`pip install clip-benchmark`\n\nFor development, you can also do this:\n\n```bash\ngit clone https://github.com/LAION-AI/CLIP_benchmark\ncd CLIP_benchmark\npython setup.py install\n```\n\n## How to use?\n\n### Command line interface (CLI)\n\nThe easiest way to benchmark the models is using the CLI, `clip_benchmark`.\nYou can specify the model to use, the dataset and the task to evaluate on. Once it is done, evaluation is performed and\nthe results are written into a JSON file.\n\n### CIFAR-10 example\n\n Here is an example for CIFAR-10 zero-shot classification using OpenCLIP's pre-trained model on LAION-400m:\n\n `clip_benchmark --dataset=cifar10 --task=zeroshot_classification --pretrained=laion400m_e32 --model=ViT-B-32-quickgelu --output=result.json --batch_size=64`\n\nHere is the content of `result.json` after the evaluation is done:\n\n```json\n{\n    \"dataset\": \"cifar10\", \"model\": \"ViT-B-32-quickgelu\", \n    \"pretrained\": \"laion400m_e32\", \"task\": \"zeroshot_classification\",\n    \"metrics\": {\"acc1\": 0.9074, \"acc5\": 0.998}\n}\n```\n\n### VOC2007 example\n\nHere is another example with VOC2007, which is a multi-label classification dataset.\n\n `clip_benchmark --dataset=voc2007_multilabel --task=zeroshot_classification --pretrained=laion400m_e32 --model=ViT-B-32-quickgelu --output=result.json --batch_size=64`\n\nHere is the content of `result.json` after the evaluation is done:\n\n```json\n{\"dataset\": \"voc2007_multilabel\", \"model\": \"ViT-B-32-quickgelu\", \"pretrained\": \"laion400m_e32\", \"task\": \"zeroshot_classification\", \"metrics\": {\"mean_average_precision\": 0.7627869844436646}}\n```\n\nHere, we compute the mean average precision or mAP, more details about that metric [here](https://fangdahan.medium.com/calculate-mean-average-precision-map-for-multi-label-classification-b082679d31be) in the context of multi-label classification.\n\n### VTAB example\n\nHere is an example on how to run it on [VTAB](https://github.com/google-research/task_adaptation) classification tasks.\nFirst, you need to install VTAB's dedicated package.\n\n`pip install task_adaptation==0.1`\n\nThe name of the dataset follows the template `vtab/<TASK_NAME>`.\nTo have the list of the 19 classification tasks using in VTAB, you can use:\n\n`python -c 'from clip_benchmark.datasets.builder import VTAB_19TASKS;print(\"\\n\".join(VTAB_19TASKS))'`\n\n\nThen, you can run it by providing the full dataset name.\nExample with `eurosat`:\n\n `clip_benchmark --dataset=vtab/eurosat --task=zeroshot_classification --pretrained=laion400m_e32 --model=ViT-B-32-quickgelu --output=result.json --batch_size=64`\n\n\n### TensorFlow dataset example\n\n\n\nHere is an example on how to run it on [Tensorflow datasets](https://www.tensorflow.org/datasets).\nFirst, you need to install `tfds-nightly` and `timm`.\n\n`pip install timm tfds-nightly`\n\n\nThe name of the dataset follows the template `tfds/<DATASET_NAME>`.\n\nExample with `cifar10`:\n\n `clip_benchmark --dataset=tfds/cifar10 --task=zeroshot_classification --pretrained=laion400m_e32 --model=ViT-B-32-quickgelu --output=result.json --batch_size=64`\n\n\n### COCO captions example\n\n Here is an example for COCO captions zero-shot retrieval:\n\n `clip_benchmark --dataset=mscoco_captions --task=zeroshot_retrieval --pretrained=laion400m_e32 --model=ViT-B-32-quickgelu --output=result.json --dataset_root=<PATH_TO_IMAGE_FOLDER> --annotation_file=<PATH_TO_ANNOTATION_FILE> --batch_size=64` \n\n (see <https://cocodataset.org/#home> for instructions on how to download)\n\n Note that for using COCO, you also need to install `pycocotools`, using:\n\n `pip install pycocotools`\n\n### API\n\nYou can also use the API directly. This is especially useful if your model\ndoes not belong to currently supported models.\n(TODO)\n\n## Credits\n\n- Thanks to [OpenCLIP](https://github.com/mlfoundations/open_clip) authors, zero-shot accuracy code is adapted from there and pre-trained models are used in the command line interface.\n- Thanks to [SLIP](https://github.com/facebookresearch/SLIP) authors, some zero-shot templates and classnames are from there.\n- Thanks to [Wise-ft](https://github.com/mlfoundations/wise-ft) authors, Imagenet robustness datasets code is adapted from there\n- Thanks to [LiT](https://arxiv.org/abs/2111.07991.pdf) authors, some zero-shot templates and classnames of VTAB datasets are from there.\n- This package was created with [Cookiecutter]( https://github.com/audreyr/cookiecutter) and the [audreyr/cookiecutter-pypackage](https://github.com/audreyr/cookiecutter-pypackage) project template. Thanks to the author.\n\n\n## History\n\n### 1.0.1\n\n* pypi description as markdown\n\n### 1.0.0\n\n* Actual first release on PyPI.\n\n\n### 0.1.0\n\n* First release on PyPI.\n\n\n",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/mehdidc/clip_benchmark",
            "keywords": "clip_benchmark",
            "license": "MIT license",
            "maintainer": "",
            "maintainer_email": "",
            "name": "clip-benchmark",
            "package_url": "https://pypi.org/project/clip-benchmark/",
            "platform": null,
            "project_url": "https://pypi.org/project/clip-benchmark/",
            "project_urls": {
                "Homepage": "https://github.com/mehdidc/clip_benchmark"
            },
            "release_url": "https://pypi.org/project/clip-benchmark/1.0.1/",
            "requires_dist": [
                "torch (>=1.8.1<2)",
                "torchvision (>=0.8.9<2)",
                "tqdm (>=4.62.3<2)",
                "scikit-learn (>=1.0<2)",
                "open-clip-torch (>=0.2.1)"
            ],
            "requires_python": ">=3.6",
            "summary": "CLIP-like models benchmarks on various datasets",
            "version": "1.0.1",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 15806692,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "9f21c83f403a4102a0473d12a9678162",
                    "sha256": "34c263699ab3e9a2c57cd961eb3f4cf6de264860e86186076ba4cca6c20a29d3"
                },
                "downloads": -1,
                "filename": "clip_benchmark-1.0.1-py2.py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "9f21c83f403a4102a0473d12a9678162",
                "packagetype": "bdist_wheel",
                "python_version": "py2.py3",
                "requires_python": ">=3.6",
                "size": 67031,
                "upload_time": "2022-11-17T21:19:49",
                "upload_time_iso_8601": "2022-11-17T21:19:49.685969Z",
                "url": "https://files.pythonhosted.org/packages/56/57/8325bbd0b139f4e5aab54f61c555a837c87cd81c82c917c2f6590572b092/clip_benchmark-1.0.1-py2.py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "19b1cb6b92ae7c902d95b3b437e00b27",
                    "sha256": "cbe00bb6d4ccc36b777b56bbfd04edf2ae7f0494d9a33be8ba99d2d1d021d796"
                },
                "downloads": -1,
                "filename": "clip_benchmark-1.0.1.tar.gz",
                "has_sig": false,
                "md5_digest": "19b1cb6b92ae7c902d95b3b437e00b27",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.6",
                "size": 66282,
                "upload_time": "2022-11-17T21:19:51",
                "upload_time_iso_8601": "2022-11-17T21:19:51.379320Z",
                "url": "https://files.pythonhosted.org/packages/52/64/618506de24693974c32e7532e14a49946c51da32a8a953ecf28912df4cfe/clip_benchmark-1.0.1.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}