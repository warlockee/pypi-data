{
    "1.1.0": {
        "info": {
            "author": "EMalagoli92",
            "author_email": "emala.892@gmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 5 - Production/Stable",
                "Intended Audience :: Developers",
                "Intended Audience :: Education",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: MIT License",
                "Programming Language :: Python :: 3",
                "Topic :: Scientific/Engineering",
                "Topic :: Software Development",
                "Topic :: Software Development :: Libraries",
                "Topic :: Software Development :: Libraries :: Python Modules"
            ],
            "description": "<div align=\"center\">\n\n  <a href=\"https://www.tensorflow.org\">![TensorFLow](https://img.shields.io/badge/TensorFlow-2.X-orange?style=for-the-badge) \n  <a href=\"https://github.com/EMalagoli92/CvT-TensorFlow/blob/main/LICENSE\">![License](https://img.shields.io/github/license/EMalagoli92/CvT-TensorFlow?style=for-the-badge) \n  <a href=\"https://www.python.org\">![Python](https://img.shields.io/badge/python-%3E%3D%203.9-blue?style=for-the-badge)</a>  \n  \n</div>\n\n# CvT-TensorFlow\nTensorFlow 2.X reimplementation of [CvT: Introducing Convolutions to Vision Transformers](https://arxiv.org/abs/2103.15808), Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, Lei Zhang.\n- Exact TensorFlow reimplementation of official PyTorch repo, including `timm` modules used by authors, preserving models and layers structure.\n- ImageNet pretrained weights ported from PyTorch official implementation.\n\n## Table of contents\n- [Abstract](#abstract)\n- [Results](#results)\n- [Installation](#installation)\n- [Usage](#usage)\n- [Acknowledgement](#acknowledgement)\n- [Citations](#citations)\n- [License](#license)\n\n<div id=\"abstract\"/>\n\n## Abstract\nConvolutional vision Transformers (CvT), improves Vision Transformers (ViT) in \nperformance and efficienty by introducing convolutions into ViT to yield the \nbest of both designs. This is accomplished through two primary modifications: \na hierarchy of Transformers containing a new convolutional token embedding, \nand a convolutional Transformer block leveraging a convolutional projection. \nThese changes introduce desirable properties of convolutional neural networks \n(CNNs) to the ViT architecture (e.g. shift, scale, and distortion invariance) \nwhile maintaining the merits of Transformers (e.g. dynamic attention, \nglobal context, and better generalization). \nMoreover the achieved results show that the positional encoding, \na crucial component in existing Vision Transformers, can be safely removed \nin the model, simplifying the design for higher resolution vision tasks.\n\n\n![Alt text](https://raw.githubusercontent.com/EMalagoli92/CvT-TensorFlow/266afd1057827d10f0dfb842f8ef73f5b19e471d/assets/images/pipeline.svg)\n<p align = \"center\"><sub>The pipeline of the CvT architecture. (a) Overall architecture, showing the hierarchical multi-stage\nstructure facilitated by the Convolutional Token Embedding layer. (b) Details of the Convolutional Transformer Block,\nwhich contains the convolution projection as the first layer.</sub></p>\n\n<div id=\"results\"/>\n\n## Results\nTensorFlow implementation and ImageNet ported weights have been compared to the official PyTorch implementation on [ImageNet-V2](https://www.tensorflow.org/datasets/catalog/imagenet_v2) test set.\n\n### Models pre-trained on ImageNet-1K\n| Configuration  | Resolution | Top-1 (Original) | Top-1 (Ported) | Top-5 (Original) | Top-5 (Ported) | #Params\n| ------------- | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- |\n| CvT-13 | 224x224 | 69.81 | 69.81 | 89.13 | 89.13 | 20M |\n| CvT-13 | 384x384 | 71.31 | 71.31 | 89.97 | 89.97 | 20M |\n| CvT-21 | 224x224 | 71.18 | 71.17 | 89.31 | 89.31 | 32M |\n| CvT-21 | 384x384 | 71.61 | 71.61 | 89.71 | 89.71 | 32M |\n\n\n### Models pre-trained on ImageNet-22K\n| Configuration  | Resoluton | Top-1 (Original) | Top-1 (Ported) | Top-5 (Original) | Top-5 (Ported) | #Params\n| ------------- | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- |\n| CvT-13 | 384x284 | 71.76 | 71.76 | 91.39 | 91.39 | 20M |\n| CvT-21 | 384x384 | 74.97 | 74.97 | 92.63 | 92.63 | 32M |\n| CvT-W24 | 384x384 | 78.15 | 78.15 | 94.48 | 94.48 | 277M | \n\nMax metrics difference: `9e-5`.\n\n<div id=\"installation\"/>\n\n## Installation\n- Install from PyPI\n```\npip install cvt-tensorflow\n```\n- Install from Github\n```\npip install git+https://github.com/EMalagoli92/CvT-TensorFlow\n```\n- Clone the repo and install necessary packages \n```\ngit clone https://github.com/EMalagoli92/CvT-TensorFlow.git\npip install -r requirements.txt\n```\n\nTested on *Ubuntu 20.04.4 LTS x86_64*, *python 3.9.7*.\n\n<div id=\"usage\"/>\n\n## Usage\n- Define a custom CvT configuration.\n```python\nfrom cvt_tensorflow import CvT\n\n# Define a custom CvT configuration\nmodel = CvT(\n    in_chans=3,\n    num_classes=1000,\n    classifier_activation=\"softmax\",\n    data_format=\"channels_last\",\n    spec={\n        \"INIT\": \"trunc_norm\",\n        \"NUM_STAGES\": 3,\n        \"PATCH_SIZE\": [7, 3, 3],\n        \"PATCH_STRIDE\": [4, 2, 2],\n        \"PATCH_PADDING\": [2, 1, 1],\n        \"DIM_EMBED\": [64, 192, 384],\n        \"NUM_HEADS\": [1, 3, 6],\n        \"DEPTH\": [1, 2, 10],\n        \"MLP_RATIO\": [4.0, 4.0, 4.0],\n        \"ATTN_DROP_RATE\": [0.0, 0.0, 0.0],\n        \"DROP_RATE\": [0.0, 0.0, 0.0],\n        \"DROP_PATH_RATE\": [0.0, 0.0, 0.1],\n        \"QKV_BIAS\": [True, True, True],\n        \"CLS_TOKEN\": [False, False, True],\n        \"QKV_PROJ_METHOD\": [\"dw_bn\", \"dw_bn\", \"dw_bn\"],\n        \"KERNEL_QKV\": [3, 3, 3],\n        \"PADDING_KV\": [1, 1, 1],\n        \"STRIDE_KV\": [2, 2, 2],\n        \"PADDING_Q\": [1, 1, 1],\n        \"STRIDE_Q\": [1, 1, 1],\n    },\n)\n```\n- Use a predefined CvT configuration.\n```python\nfrom cvt_tensorflow import CvT\n\nmodel = CvT(\n    configuration=\"cvt-21\", data_format=\"channels_last\", classifier_activation=\"softmax\"\n)\nmodel.build((None, 224, 224, 3))\nprint(model.summary())\n```\n```\nModel: \"cvt-21\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n stage0 (VisionTransformer)  multiple                  62080     \n                                                                 \n stage1 (VisionTransformer)  multiple                  1920576   \n                                                                 \n stage2 (VisionTransformer)  ((None, 384, 14, 14),     29296128  \n                              (None, 1, 384))                    \n                                                                 \n norm (LayerNorm_)           (None, 1, 384)            768       \n                                                                 \n head (Linear_)              (None, 1000)              385000    \n                                                                 \n pred (Activation)           (None, 1000)              0         \n                                                                 \n=================================================================\nTotal params: 31,664,552\nTrainable params: 31,622,696\nNon-trainable params: 41,856\n_________________________________________________________________\n```\n- Train from scratch the model.\n```python    \n# Example\nmodel.compile(\n    optimizer=\"sgd\",\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"accuracy\", \"sparse_top_k_categorical_accuracy\"],\n)\nmodel.fit(x, y)\n```\n- Use ported ImageNet pretrained weights\n```python\n# Example\nfrom cvt_tensorflow import CvT\n\n# Use cvt-13-384x384_22k ImageNet pretrained weights\nmodel = CvT(\n    configuration=\"cvt-13\",\n    pretrained=True,\n    pretrained_resolution=384,\n    pretrained_version=\"22k\",\n    classifier_activation=\"softmax\",\n)\ny_pred = model(image)\n```\n\n<div id=\"acknowledgement\"/>\n\n## Acknowledgement\n[CvT](https://github.com/microsoft/CvT) (Official PyTorch implementation)\n\n\n<div id=\"citations\"/>\n\n## Citations\n```bibtex\n@article{wu2021cvt,\n  title={Cvt: Introducing convolutions to vision transformers},\n  author={Wu, Haiping and Xiao, Bin and Codella, Noel and Liu, Mengchen and Dai, Xiyang and Yuan, Lu and Zhang, Lei},\n  journal={arXiv preprint arXiv:2103.15808},\n  year={2021}\n}\n```\n\n<div id=\"license\"/>\n\n## License\nThis work is made available under the [MIT License](https://github.com/EMalagoli92/CvT-TensorFlow/blob/main/LICENSE)\n",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/EMalagoli92/CvT-TensorFlow",
            "keywords": "",
            "license": "MIT",
            "maintainer": "",
            "maintainer_email": "",
            "name": "cvt-tensorflow",
            "package_url": "https://pypi.org/project/cvt-tensorflow/",
            "platform": null,
            "project_url": "https://pypi.org/project/cvt-tensorflow/",
            "project_urls": {
                "Homepage": "https://github.com/EMalagoli92/CvT-TensorFlow"
            },
            "release_url": "https://pypi.org/project/cvt-tensorflow/1.1.0/",
            "requires_dist": [
                "absl-py (==1.3.0)",
                "astunparse (==1.6.3)",
                "cachetools (==5.2.0)",
                "certifi (==2022.9.24)",
                "charset-normalizer (==2.1.1)",
                "cloudpickle (==2.2.0)",
                "decorator (==5.1.1)",
                "dm-tree (==0.1.7)",
                "einops (==0.4.1)",
                "flatbuffers (==1.12)",
                "gast (==0.4.0)",
                "google-auth (==2.12.0)",
                "google-auth-oauthlib (==0.4.6)",
                "google-pasta (==0.2.0)",
                "grpcio (==1.49.1)",
                "h5py (==3.7.0)",
                "idna (==3.4)",
                "importlib-metadata (==5.0.0)",
                "keras (==2.9.0)",
                "Keras-Preprocessing (==1.1.2)",
                "libclang (==14.0.6)",
                "Markdown (==3.4.1)",
                "MarkupSafe (==2.1.1)",
                "numpy (==1.23.1)",
                "oauthlib (==3.2.1)",
                "opt-einsum (==3.3.0)",
                "packaging (==21.3)",
                "protobuf (==3.19.6)",
                "pyasn1 (==0.4.8)",
                "pyasn1-modules (==0.2.8)",
                "pyparsing (==3.0.9)",
                "requests (==2.28.1)",
                "requests-oauthlib (==1.3.1)",
                "rsa (==4.9)",
                "six (==1.16.0)",
                "tensorboard (==2.9.1)",
                "tensorboard-data-server (==0.6.1)",
                "tensorboard-plugin-wit (==1.8.1)",
                "tensorflow (==2.9.0)",
                "tensorflow-addons (==0.17.1)",
                "tensorflow-estimator (==2.9.0)",
                "tensorflow-io-gcs-filesystem (==0.27.0)",
                "tensorflow-probability (==0.17.0)",
                "termcolor (==2.0.1)",
                "typeguard (==2.13.3)",
                "typing-extensions (==4.4.0)",
                "urllib3 (==1.26.12)",
                "Werkzeug (==2.2.2)",
                "wrapt (==1.14.1)",
                "zipp (==3.9.0)"
            ],
            "requires_python": ">=3.9",
            "summary": "TensorFlow 2.X reimplementation of CvT: Introducing Convolutions to Vision Transformers, Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, Lei Zhang.",
            "version": "1.1.0",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16238999,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "md5": "31356b318af5eab99b57a6600579df87",
                    "sha256": "b4de7b4c6bdad197c2b766a3b46906bcf20a377ed19d6a61a05f10fb295c92fe"
                },
                "downloads": -1,
                "filename": "cvt_tensorflow-1.1.0-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "31356b318af5eab99b57a6600579df87",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.9",
                "size": 24885,
                "upload_time": "2022-12-28T14:00:31",
                "upload_time_iso_8601": "2022-12-28T14:00:31.080679Z",
                "url": "https://files.pythonhosted.org/packages/69/06/ce71acc3fa2fe19ed6f65664634090f6c7a8f8a785fc826cf943e9554c87/cvt_tensorflow-1.1.0-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "md5": "434478d1adabf3610c7430bc2b6d386a",
                    "sha256": "99601d3ad8e83b1578ea9f0d8d8d8a541e24eee64f04734f310c20d68abb59fb"
                },
                "downloads": -1,
                "filename": "cvt_tensorflow-1.1.0.tar.gz",
                "has_sig": false,
                "md5_digest": "434478d1adabf3610c7430bc2b6d386a",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.9",
                "size": 21805,
                "upload_time": "2022-12-28T14:00:32",
                "upload_time_iso_8601": "2022-12-28T14:00:32.546751Z",
                "url": "https://files.pythonhosted.org/packages/c5/53/91bd922746a4d74d1f1a4511a6f3042cb485a35e3738f26d64111a64616e/cvt_tensorflow-1.1.0.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    },
    "1.1.1": {
        "info": {
            "author": "EMalagoli92",
            "author_email": "emala.892@gmail.com",
            "bugtrack_url": null,
            "classifiers": [
                "Development Status :: 5 - Production/Stable",
                "Intended Audience :: Developers",
                "Intended Audience :: Education",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: MIT License",
                "Programming Language :: Python :: 3",
                "Topic :: Scientific/Engineering",
                "Topic :: Software Development",
                "Topic :: Software Development :: Libraries",
                "Topic :: Software Development :: Libraries :: Python Modules"
            ],
            "description": "<div align=\"center\">\n\n  <a href=\"https://www.tensorflow.org\">![TensorFLow](https://img.shields.io/badge/TensorFlow-2.X-orange?style=for-the-badge) \n  <a href=\"https://github.com/EMalagoli92/CvT-TensorFlow/blob/main/LICENSE\">![License](https://img.shields.io/github/license/EMalagoli92/CvT-TensorFlow?style=for-the-badge) \n  <a href=\"https://www.python.org\">![Python](https://img.shields.io/badge/python-%3E%3D%203.9-blue?style=for-the-badge)</a>  \n  \n</div>\n\n# CvT-TensorFlow\nTensorFlow 2.X reimplementation of [CvT: Introducing Convolutions to Vision Transformers](https://arxiv.org/abs/2103.15808), Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, Lei Zhang.\n- Exact TensorFlow reimplementation of official PyTorch repo, including `timm` modules used by authors, preserving models and layers structure.\n- ImageNet pretrained weights ported from PyTorch official implementation.\n\n## Table of contents\n- [Abstract](#abstract)\n- [Results](#results)\n- [Installation](#installation)\n- [Usage](#usage)\n- [Acknowledgement](#acknowledgement)\n- [Citations](#citations)\n- [License](#license)\n\n<div id=\"abstract\"/>\n\n## Abstract\nConvolutional vision Transformers (CvT), improves Vision Transformers (ViT) in \nperformance and efficienty by introducing convolutions into ViT to yield the \nbest of both designs. This is accomplished through two primary modifications: \na hierarchy of Transformers containing a new convolutional token embedding, \nand a convolutional Transformer block leveraging a convolutional projection. \nThese changes introduce desirable properties of convolutional neural networks \n(CNNs) to the ViT architecture (e.g. shift, scale, and distortion invariance) \nwhile maintaining the merits of Transformers (e.g. dynamic attention, \nglobal context, and better generalization). \nMoreover the achieved results show that the positional encoding, \na crucial component in existing Vision Transformers, can be safely removed \nin the model, simplifying the design for higher resolution vision tasks.\n\n\n![Alt text](https://raw.githubusercontent.com/EMalagoli92/CvT-TensorFlow/266afd1057827d10f0dfb842f8ef73f5b19e471d/assets/images/pipeline.svg)\n<p align = \"center\"><sub>The pipeline of the CvT architecture. (a) Overall architecture, showing the hierarchical multi-stage\nstructure facilitated by the Convolutional Token Embedding layer. (b) Details of the Convolutional Transformer Block,\nwhich contains the convolution projection as the first layer.</sub></p>\n\n<div id=\"results\"/>\n\n## Results\nTensorFlow implementation and ImageNet ported weights have been compared to the official PyTorch implementation on [ImageNet-V2](https://www.tensorflow.org/datasets/catalog/imagenet_v2) test set.\n\n### Models pre-trained on ImageNet-1K\n| Configuration  | Resolution | Top-1 (Original) | Top-1 (Ported) | Top-5 (Original) | Top-5 (Ported) | #Params\n| ------------- | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- |\n| CvT-13 | 224x224 | 69.81 | 69.81 | 89.13 | 89.13 | 20M |\n| CvT-13 | 384x384 | 71.31 | 71.31 | 89.97 | 89.97 | 20M |\n| CvT-21 | 224x224 | 71.18 | 71.17 | 89.31 | 89.31 | 32M |\n| CvT-21 | 384x384 | 71.61 | 71.61 | 89.71 | 89.71 | 32M |\n\n\n### Models pre-trained on ImageNet-22K\n| Configuration  | Resoluton | Top-1 (Original) | Top-1 (Ported) | Top-5 (Original) | Top-5 (Ported) | #Params\n| ------------- | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- |\n| CvT-13 | 384x284 | 71.76 | 71.76 | 91.39 | 91.39 | 20M |\n| CvT-21 | 384x384 | 74.97 | 74.97 | 92.63 | 92.63 | 32M |\n| CvT-W24 | 384x384 | 78.15 | 78.15 | 94.48 | 94.48 | 277M | \n\nMax metrics difference: `9e-5`.\n\n<div id=\"installation\"/>\n\n## Installation\n- Install from PyPI\n```\npip install cvt-tensorflow\n```\n- Install from Github\n```\npip install git+https://github.com/EMalagoli92/CvT-TensorFlow\n```\n- Clone the repo and install necessary packages \n```\ngit clone https://github.com/EMalagoli92/CvT-TensorFlow.git\npip install -r requirements.txt\n```\n\nTested on *Ubuntu 20.04.4 LTS x86_64*, *python 3.9.7*.\n\n<div id=\"usage\"/>\n\n## Usage\n- Define a custom CvT configuration.\n```python\nfrom cvt_tensorflow import CvT\n\n# Define a custom CvT configuration\nmodel = CvT(\n    in_chans=3,\n    num_classes=1000,\n    classifier_activation=\"softmax\",\n    data_format=\"channels_last\",\n    spec={\n        \"INIT\": \"trunc_norm\",\n        \"NUM_STAGES\": 3,\n        \"PATCH_SIZE\": [7, 3, 3],\n        \"PATCH_STRIDE\": [4, 2, 2],\n        \"PATCH_PADDING\": [2, 1, 1],\n        \"DIM_EMBED\": [64, 192, 384],\n        \"NUM_HEADS\": [1, 3, 6],\n        \"DEPTH\": [1, 2, 10],\n        \"MLP_RATIO\": [4.0, 4.0, 4.0],\n        \"ATTN_DROP_RATE\": [0.0, 0.0, 0.0],\n        \"DROP_RATE\": [0.0, 0.0, 0.0],\n        \"DROP_PATH_RATE\": [0.0, 0.0, 0.1],\n        \"QKV_BIAS\": [True, True, True],\n        \"CLS_TOKEN\": [False, False, True],\n        \"QKV_PROJ_METHOD\": [\"dw_bn\", \"dw_bn\", \"dw_bn\"],\n        \"KERNEL_QKV\": [3, 3, 3],\n        \"PADDING_KV\": [1, 1, 1],\n        \"STRIDE_KV\": [2, 2, 2],\n        \"PADDING_Q\": [1, 1, 1],\n        \"STRIDE_Q\": [1, 1, 1],\n    },\n)\n```\n- Use a predefined CvT configuration.\n```python\nfrom cvt_tensorflow import CvT\n\nmodel = CvT(\n    configuration=\"cvt-21\", data_format=\"channels_last\", classifier_activation=\"softmax\"\n)\nmodel.build((None, 224, 224, 3))\nprint(model.summary())\n```\n```\nModel: \"cvt-21\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n stage0 (VisionTransformer)  multiple                  62080     \n                                                                 \n stage1 (VisionTransformer)  multiple                  1920576   \n                                                                 \n stage2 (VisionTransformer)  ((None, 384, 14, 14),     29296128  \n                              (None, 1, 384))                    \n                                                                 \n norm (LayerNorm_)           (None, 1, 384)            768       \n                                                                 \n head (Linear_)              (None, 1000)              385000    \n                                                                 \n pred (Activation)           (None, 1000)              0         \n                                                                 \n=================================================================\nTotal params: 31,664,552\nTrainable params: 31,622,696\nNon-trainable params: 41,856\n_________________________________________________________________\n```\n- Train from scratch the model.\n```python    \n# Example\nmodel.compile(\n    optimizer=\"sgd\",\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"accuracy\", \"sparse_top_k_categorical_accuracy\"],\n)\nmodel.fit(x, y)\n```\n- Use ported ImageNet pretrained weights\n```python\n# Example\nfrom cvt_tensorflow import CvT\n\n# Use cvt-13-384x384_22k ImageNet pretrained weights\nmodel = CvT(\n    configuration=\"cvt-13\",\n    pretrained=True,\n    pretrained_resolution=384,\n    pretrained_version=\"22k\",\n    classifier_activation=\"softmax\",\n)\ny_pred = model(image)\n```\n\n<div id=\"acknowledgement\"/>\n\n## Acknowledgement\n[CvT](https://github.com/microsoft/CvT) (Official PyTorch implementation)\n\n\n<div id=\"citations\"/>\n\n## Citations\n```bibtex\n@article{wu2021cvt,\n  title={Cvt: Introducing convolutions to vision transformers},\n  author={Wu, Haiping and Xiao, Bin and Codella, Noel and Liu, Mengchen and Dai, Xiyang and Yuan, Lu and Zhang, Lei},\n  journal={arXiv preprint arXiv:2103.15808},\n  year={2021}\n}\n```\n\n<div id=\"license\"/>\n\n## License\nThis work is made available under the [MIT License](https://github.com/EMalagoli92/CvT-TensorFlow/blob/main/LICENSE)\n",
            "description_content_type": "text/markdown",
            "docs_url": null,
            "download_url": "",
            "downloads": {
                "last_day": -1,
                "last_month": -1,
                "last_week": -1
            },
            "home_page": "https://github.com/EMalagoli92/CvT-TensorFlow",
            "keywords": "",
            "license": "MIT",
            "maintainer": "",
            "maintainer_email": "",
            "name": "cvt-tensorflow",
            "package_url": "https://pypi.org/project/cvt-tensorflow/",
            "platform": null,
            "project_url": "https://pypi.org/project/cvt-tensorflow/",
            "project_urls": {
                "Homepage": "https://github.com/EMalagoli92/CvT-TensorFlow"
            },
            "release_url": "https://pypi.org/project/cvt-tensorflow/1.1.1/",
            "requires_dist": [
                "absl-py (==1.3.0)",
                "astunparse (==1.6.3)",
                "cachetools (==5.2.0)",
                "certifi (==2022.9.24)",
                "charset-normalizer (==2.1.1)",
                "cloudpickle (==2.2.0)",
                "decorator (==5.1.1)",
                "dm-tree (==0.1.7)",
                "einops (==0.4.1)",
                "flatbuffers (==1.12)",
                "gast (==0.4.0)",
                "google-auth (==2.12.0)",
                "google-auth-oauthlib (==0.4.6)",
                "google-pasta (==0.2.0)",
                "grpcio (==1.49.1)",
                "h5py (==3.7.0)",
                "idna (==3.4)",
                "importlib-metadata (==5.0.0)",
                "keras (==2.9.0)",
                "Keras-Preprocessing (==1.1.2)",
                "libclang (==14.0.6)",
                "Markdown (==3.4.1)",
                "MarkupSafe (==2.1.1)",
                "numpy (==1.23.1)",
                "oauthlib (==3.2.1)",
                "opt-einsum (==3.3.0)",
                "packaging (==21.3)",
                "protobuf (==3.19.6)",
                "pyasn1 (==0.4.8)",
                "pyasn1-modules (==0.2.8)",
                "pyparsing (==3.0.9)",
                "requests (==2.28.1)",
                "requests-oauthlib (==1.3.1)",
                "rsa (==4.9)",
                "six (==1.16.0)",
                "tensorboard (==2.9.1)",
                "tensorboard-data-server (==0.6.1)",
                "tensorboard-plugin-wit (==1.8.1)",
                "tensorflow (==2.9.0)",
                "tensorflow-addons (==0.17.1)",
                "tensorflow-estimator (==2.9.0)",
                "tensorflow-io-gcs-filesystem (==0.27.0)",
                "tensorflow-probability (==0.17.0)",
                "termcolor (==2.0.1)",
                "typeguard (==2.13.3)",
                "typing-extensions (==4.4.0)",
                "urllib3 (==1.26.12)",
                "Werkzeug (==2.2.2)",
                "wrapt (==1.14.1)",
                "zipp (==3.9.0)"
            ],
            "requires_python": ">=3.9",
            "summary": "TensorFlow 2.X reimplementation of CvT: Introducing Convolutions to Vision Transformers, Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, Lei Zhang.",
            "version": "1.1.1",
            "yanked": false,
            "yanked_reason": null
        },
        "last_serial": 16303684,
        "urls": [
            {
                "comment_text": "",
                "digests": {
                    "blake2b_256": "7ed47562de6894cfb53865fcf360e33fc095b58964b1a3c1f91159f4518d65da",
                    "md5": "445c54f415de7de09141ae9d05221c62",
                    "sha256": "8f6a53b7909244b267d0405057304e16dd85bb5333e00c3af0ddd8548d01982b"
                },
                "downloads": -1,
                "filename": "cvt_tensorflow-1.1.1-py3-none-any.whl",
                "has_sig": false,
                "md5_digest": "445c54f415de7de09141ae9d05221c62",
                "packagetype": "bdist_wheel",
                "python_version": "py3",
                "requires_python": ">=3.9",
                "size": 24751,
                "upload_time": "2023-01-04T15:44:09",
                "upload_time_iso_8601": "2023-01-04T15:44:09.771574Z",
                "url": "https://files.pythonhosted.org/packages/7e/d4/7562de6894cfb53865fcf360e33fc095b58964b1a3c1f91159f4518d65da/cvt_tensorflow-1.1.1-py3-none-any.whl",
                "yanked": false,
                "yanked_reason": null
            },
            {
                "comment_text": "",
                "digests": {
                    "blake2b_256": "03392f7f7554a6c65ab1516d9dadb7572bd338f6f5147524c09d2e9ed25ec005",
                    "md5": "24fab9c88e20a897697f2b8828fcef3c",
                    "sha256": "fbd61e4e354b1a69a22896d53ac50680b40af80fd8c4f25ea35f395b8dd8ee92"
                },
                "downloads": -1,
                "filename": "cvt_tensorflow-1.1.1.tar.gz",
                "has_sig": false,
                "md5_digest": "24fab9c88e20a897697f2b8828fcef3c",
                "packagetype": "sdist",
                "python_version": "source",
                "requires_python": ">=3.9",
                "size": 21634,
                "upload_time": "2023-01-04T15:44:11",
                "upload_time_iso_8601": "2023-01-04T15:44:11.144710Z",
                "url": "https://files.pythonhosted.org/packages/03/39/2f7f7554a6c65ab1516d9dadb7572bd338f6f5147524c09d2e9ed25ec005/cvt_tensorflow-1.1.1.tar.gz",
                "yanked": false,
                "yanked_reason": null
            }
        ],
        "vulnerabilities": []
    }
}